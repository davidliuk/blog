<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.18" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.59" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://davidliuk.github.io/blog/blog/education/ms/6600/siggraph/pre/transcript%20(copy).html"><meta property="og:site_name" content="David's Blog"><meta property="og:title" content="Transcript"><meta property="og:description" content="Transcript Background Motivation Challenge the task of automatic rigging comes with 00:02:09.520 many challenges 00:02:11.360 predicting an animation skeleton and 00:02:13.200 s..."><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><meta property="og:updated_time" content="2025-05-28T16:32:52.000Z"><meta property="article:modified_time" content="2025-05-28T16:32:52.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Transcript","image":[""],"dateModified":"2025-05-28T16:32:52.000Z","author":[{"@type":"Person","name":"David Liu","url":"https://github.com/davidliuk"}]}</script><link rel="icon" href="/blog/favicon.ico"><link rel="icon" href="/blog/assets/icon/512.png" type="image/png" sizes="512x512"><link rel="icon" href="/blog/assets/icon/196.png" type="image/png" sizes="192x192"><link rel="icon" href="/blog/assets/icon/196.png" type="image/png" sizes="196x196"><link rel="manifest" href="/blog/manifest.webmanifest" crossorigin="use-credentials"><meta name="theme-color" content="#46bd87"><link rel="apple-touch-icon" href="/blog/assets/icon/152.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="msapplication-TileImage" content="/blog/assets/icon/144.png"><meta name="msapplication-TileColor" content="#ffffff"><title>Transcript | David's Blog</title><meta name="description" content="Transcript Background Motivation Challenge the task of automatic rigging comes with 00:02:09.520 many challenges 00:02:11.360 predicting an animation skeleton and 00:02:13.200 s...">
    <link rel="preload" href="/blog/assets/style-CBOU5JPC.css" as="style"><link rel="stylesheet" href="/blog/assets/style-CBOU5JPC.css">
    <link rel="modulepreload" href="/blog/assets/app-isOblzBz.js"><link rel="modulepreload" href="/blog/assets/transcript (copy).html-CY6CaZUQ.js"><link rel="modulepreload" href="/blog/assets/plugin-vue_export-helper-DlAUqK2U.js">
    
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><div class="theme-container no-sidebar external-link-icon has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><!--[--><a class="route-link vp-brand" href="/blog/" aria-label="Take me home"><img class="vp-nav-logo" src="/blog/favicon.ico" alt><!----><span class="vp-site-name hide-in-pad">David&#39;s Blog</span></a><!--]--><!----></div><div class="vp-navbar-center"><!----><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/blog/" aria-label="Home"><!--[--><span class="font-icon icon home" style=""></span><!--]-->Home<!----></a></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="Algorithm"><!--[--><span class="font-icon icon edit" style=""></span>Algorithm<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/blog/algo/framework/" aria-label="Framework"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->Framework<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/blog/algo/summary/" aria-label="Summary"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->Summary<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/blog/algo/faq/" aria-label="FAQ"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->FAQ<!----></a></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="Computer Science"><!--[--><span class="font-icon icon edit" style=""></span>Computer Science<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Basic</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/cs/basic/network/" aria-label="Network"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->Network<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/cs/basic/os/" aria-label="Operating System"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->Operating System<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Database</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/cs/database/mysql/" aria-label="MySQL"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->MySQL<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/cs/database/redis/" aria-label="Redis"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->Redis<!----></a></li></ul></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="Software Engineer"><!--[--><span class="font-icon icon edit" style=""></span>Software Engineer<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Lang</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/se/lang/java/" aria-label="Java"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->Java<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">DevOps</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/se/devops/unix/" aria-label="Unix"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->Unix<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/se/devops/docker/" aria-label="Docker"><!---->Docker<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/se/devops/k8s/" aria-label="Kubernetes"><!---->Kubernetes<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Design</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/se/design/pattern/" aria-label="Design Pattern"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->Design Pattern<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/se/design/ood/" aria-label="Object-Oriented Design"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->Object-Oriented Design<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/se/design/system/" aria-label="System Design"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->System Design<!----></a></li></ul></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/blog/se/tools/" aria-label="Tools"><!---->Tools<!----></a></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="Tests"><!--[--><span class="font-icon icon edit" style=""></span>Tests<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">GRE</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/test/GRE/QUANTITATIVE/" aria-label="QUANTITATIVE"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->QUANTITATIVE<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/test/GRE/VERBAL/" aria-label="VERBAL"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->VERBAL<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">TOEFL</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/test/TOEFL/LISTENING/" aria-label="LISTENING"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->LISTENING<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/test/TOEFL/READING/" aria-label="READING"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->READING<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/test/TOEFL/SPEAKING/" aria-label="SPEAKING"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->SPEAKING<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/test/TOEFL/WRITING/" aria-label="WRITING"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->WRITING<!----></a></li></ul></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/blog/me.html" aria-label="About Me"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->About Me<!----></a></div></nav><!--]--><!----></div><div class="vp-navbar-end"><!----><!--[--><div class="vp-nav-item"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="Select language"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon i18n-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="i18n icon" name="i18n" style="width:1rem;height:1rem;vertical-align:middle;"><path d="M379.392 460.8 494.08 575.488l-42.496 102.4L307.2 532.48 138.24 701.44l-71.68-72.704L234.496 460.8l-45.056-45.056c-27.136-27.136-51.2-66.56-66.56-108.544h112.64c7.68 14.336 16.896 27.136 26.112 35.84l45.568 46.08 45.056-45.056C382.976 312.32 409.6 247.808 409.6 204.8H0V102.4h256V0h102.4v102.4h256v102.4H512c0 70.144-37.888 161.28-87.04 210.944L378.88 460.8zM576 870.4 512 1024H409.6l256-614.4H768l256 614.4H921.6l-64-153.6H576zM618.496 768h196.608L716.8 532.48 618.496 768z"></path></svg><!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link route-link-active auto-link" href="/blog/education/ms/6600/siggraph/pre/transcript%20(copy).html" aria-label="English"><!---->English<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/blog/zh/" aria-label="简体中文"><!---->简体中文<!----></a></li></ul></button></div></div><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/davidliuk/blog" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-color-mode-switch" id="color-mode-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" name="auto" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" name="dark" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" name="light" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!----><!--]--><!----><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->Transcript</h1><div class="page-info"><span class="page-author-info" aria-label="Author🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://github.com/davidliuk" target="_blank" rel="noopener noreferrer">David Liu</a></span><span property="author" content="David Liu"></span></span><!----><span class="page-date-info" aria-label="Writing Date📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span data-allow-mismatch="text">May 28, 2025</span><meta property="datePublished" content="2025-05-28T16:32:52.000Z"></span><!----><!----><span class="page-reading-time-info" aria-label="Reading Time⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 12 min</span><meta property="timeRequired" content="PT12M"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc" vp-toc><!----><div class="vp-toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon" name="print"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#background">Background</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#motivation">Motivation</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#challenge">Challenge</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#overview">Overview</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#joint-prediction">Joint Prediction</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#architecture">Architecture</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#bandwidth">Bandwidth</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#architecture-1">Architecture</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#gmedgeconv">GMEdgeConv</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#gmedgenet">GMEdgeNet</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#training">Training</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#connectivity-prediction">Connectivity Prediction</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#training-1">Training</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#skinning-prediction">Skinning Prediction</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#training-2">Training</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#dataset">Dataset</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#results">Results</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#extraction">Extraction</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#skinning-weight">Skinning Weight</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#quantitative">Quantitative</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#tests">tests</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#evaluation">Evaluation</a></li><!----><!--]--></ul></li><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!----></aside></div><!----><div class="theme-hope-content" vp-content><h1 id="transcript" tabindex="-1"><a class="header-anchor" href="#transcript"><span>Transcript</span></a></h1><h2 id="background" tabindex="-1"><a class="header-anchor" href="#background"><span>Background</span></a></h2><h3 id="motivation" tabindex="-1"><a class="header-anchor" href="#motivation"><span>Motivation</span></a></h3><h3 id="challenge" tabindex="-1"><a class="header-anchor" href="#challenge"><span>Challenge</span></a></h3><p>the task of automatic rigging comes with<br> 00:02:09.520 many challenges</p><p>00:02:11.360 predicting an animation skeleton and<br> 00:02:13.200 skinning from an arbitrary single static<br> 00:02:15.360 3d mesh is not easy</p><p><strong>skeleton</strong></p><p>00:02:18.080 one challenge is that character and can vary a lot in number of parts and the overall structure as i show here in this animator graded rigs</p><p><strong>skinning</strong></p><p>00:02:28.640 similarly when computing skinning<br> 00:02:30.319 weights animator will perceive<br> 00:02:32.400 structures as highly rigid or smoother<br> 00:02:35.360 for example<br> 00:02:36.560 here i visualize the skeleton of a nail<br> 00:02:38.640 character<br> 00:02:39.920 the skinning waist on the shell moves<br> 00:02:41.599 rigidly as a whole according to the bone<br> 00:02:43.519 here<br> 00:02:45.360 well the rest of the body deforms more<br> 00:02:47.280 smoothly according to the rest of the<br> 00:02:48.840 bone<br> 00:02:50.080 a learning method should capture the<br> 00:02:51.599 skinning variability</p><p><strong>level of detail</strong></p><p>00:02:54.319 finally another challenge for ring<br> 00:02:56.480 method is to allow easy and direct<br> 00:02:58.159 control<br> 00:02:58.720 over the level of detail for the output<br> 00:03:00.239 skeleton<br> 00:03:02.000 as you can see from this animator<br> 00:03:03.599 created rigs<br> 00:03:05.519 well animators largely agree on skeletal<br> 00:03:07.920 topology and layout of joints for an<br> 00:03:09.840 input character<br> 00:03:11.440 there is also some ambiguity both in<br> 00:03:13.200 terms of the number and exact joint<br> 00:03:15.120 placement</p><h2 id="overview" tabindex="-1"><a class="header-anchor" href="#overview"><span>Overview</span></a></h2><p>to address these challenges, RigNet propose the deep learning architecture with three modules</p><ol><li><p>00:03:22.480</p><p>firstly, we apply a skeletal joint prediction module which is trained to predict the appropriate number of joints and their placement to capture the articulated mobility of input capture as skeletal joint resolution can depend on the intended animation task we provide users an optional parameter that can control the level of detail of the output skeleton</p></li><li><p>00:03:43.680</p><p>second, to form a skeleton from the predictive joints we apply a skeleton connectivity module to predict the hierarchical tree structure connecting the joints the output bone structure is a function of predicted joints and shape feature of the input capture</p></li><li><p>00:03:59.760</p><p>finally, we apply a skinning prediction module to produce skinning weight indicating the degree of influence each vertex receives from different bones</p></li></ol><p>all these modules are trained in a supervised manner from character rigs mind online i&#39;ll describe input and each module one by one</p><h2 id="joint-prediction" tabindex="-1"><a class="header-anchor" href="#joint-prediction"><span>Joint Prediction</span></a></h2><p>00:04:20.000 the input to the entire network is a<br> 00:04:21.759 single 3d model in its mesh<br> 00:04:23.440 representation<br> 00:04:25.040 our approach operates directly on the<br> 00:04:26.960 input mesh</p><h3 id="architecture" tabindex="-1"><a class="header-anchor" href="#architecture"><span>Architecture</span></a></h3><p>00:04:29.040 the first module of our architecture<br> 00:04:31.120 joint prediction module<br> 00:04:32.560 is trained to predict the location of<br> 00:04:34.320 joints that will be used to form the<br> 00:04:36.080 animation skeleton<br> 00:04:38.400 to this end it learns to displace mesh<br> 00:04:40.720 geometry towards candidate<br> 00:04:42.000 joint locations the module is based on a<br> 00:04:45.600 neural network<br> 00:04:46.800 which extracts topology and geometry<br> 00:04:48.880 aware features from the mesh to learn<br> 00:04:50.639 these displacements<br> 00:04:53.199 at the same time we also learn a weight<br> 00:04:55.199 function over the input mesh<br> 00:04:56.880 which is used to reveal which surface<br> 00:04:58.639 areas are more relevant for localizing joints<br> 00:05:01.759 this can be seen as a form of neural<br> 00:05:03.440 mesh attention<br> 00:05:05.600 after that we introduce a differentiable<br> 00:05:08.160 clustering scheme<br> 00:05:09.600 which uses both displaced vertices and<br> 00:05:11.600 the neural mesh attention<br> 00:05:13.120 to collapse the vertices further to<br> 00:05:14.960 potential positions of the joints<br> 00:05:18.240 since areas with higher point density<br> 00:05:20.240 and greater mesh attention are strong<br> 00:05:22.080 indicators of joint presence<br> 00:05:24.479 with results to mean shift clustering<br> 00:05:26.639 and then maximum separation to extract<br> 00:05:28.400 joints<br> 00:05:30.160 in classical mean shift clustering each<br> 00:05:32.400 data point is equipped with the kernel<br> 00:05:34.400 function<br> 00:05:35.520 at each iteration all points are shifted<br> 00:05:37.919 towards density modes<br> 00:05:40.000 here we show the mean shift equation<br> 00:05:43.199 we employ a variant of mean shift<br> 00:05:44.960 clustering where the kernel is also<br> 00:05:46.960 modulated by the vertex attention<br> 00:05:49.919 in this manner points with greater<br> 00:05:52.080 attention influence the estimation of<br> 00:05:53.600 density more<br> 00:05:55.039 here we show the shift equation in our<br> 00:05:57.039 implementation<br> 00:05:59.680 we use epinephrine curve kernel in our<br> 00:06:01.840 implementation<br> 00:06:03.280 note that the kernel function takes the<br> 00:06:04.800 parameter h as bandwidth<br> 00:06:07.039 the bandwidth can be learned<br> 00:06:08.240 simultaneously as we turn the network<br> 00:06:11.039 the bandwidth also allow optional user<br> 00:06:12.880 control on the level of detail<br> 00:06:14.639 or granularity of the output skeleton<br> 00:06:18.080 we found that modifying the bandwidth<br> 00:06:19.759 directly affects the level of detail of<br> 00:06:21.520 the output skeleton</p><h3 id="bandwidth" tabindex="-1"><a class="header-anchor" href="#bandwidth"><span>Bandwidth</span></a></h3><p>00:06:23.360 here is an example lowering the<br> 00:06:26.000 bandwidth parameter results in denser<br> 00:06:27.919 joint placement<br> 00:06:29.600 will increase it results in sparse<br> 00:06:31.440 skeleton<br> 00:06:33.039 by overriding the learned bandwidth<br> 00:06:35.280 users can adjust the results to their<br> 00:06:37.440 preference</p><h3 id="architecture-1" tabindex="-1"><a class="header-anchor" href="#architecture-1"><span>Architecture</span></a></h3><p>00:06:39.360 at test time the mode centers of<br> 00:06:41.360 clusters are extracted with no maximum<br> 00:06:43.280 separation as the final detected joins<br> 00:06:46.560 now we&#39;ll discuss more details about the<br> 00:06:48.479 networks used to learn the vertex<br> 00:06:50.160 displacement and attention<br> 00:06:52.960 we call this network gmh net<br> 00:06:56.000 the main operation of this network is<br> 00:06:57.840 geodesic mesh convolution which we call<br> 00:06:59.840 gmh curve</p><h3 id="gmedgeconv" tabindex="-1"><a class="header-anchor" href="#gmedgeconv"><span>GMEdgeConv</span></a></h3><p>00:07:02.479 the gmh convolution is inspired by the h<br> 00:07:05.199 convolution in d g<br> 00:07:06.479 c and n the main difference is that our<br> 00:07:09.440 operators is applied to meshes and<br> 00:07:11.360 geodesic numbers<br> 00:07:13.360 specifically given the surface vertex<br> 00:07:16.880 we consider its one ring mesh neighbors<br> 00:07:19.280 and also the vertices located within the<br> 00:07:21.199 geodesic board centered at it<br> 00:07:24.400 we also found it&#39;s better to learn<br> 00:07:25.840 separate mlps for mesh and geodesic<br> 00:07:28.160 neighborhoods<br> 00:07:29.759 and then concatenate their outputs and<br> 00:07:31.840 process them through another mlp<br> 00:07:34.319 in this manner the networks learn to<br> 00:07:36.560 weight the importance of topology aware<br> 00:07:38.319 feature<br> 00:07:38.800 over more geometry aware ones</p><h3 id="gmedgenet" tabindex="-1"><a class="header-anchor" href="#gmedgenet"><span>GMEdgeNet</span></a></h3><p>00:07:42.560 in gmhnet we stack three gmh cuff layers<br> 00:07:46.639 each of the gmh cuff layers is followed<br> 00:07:49.039 by a global max pooling layer<br> 00:07:51.599 the representation from each pooling<br> 00:07:53.520 layers are concatenated to form a global<br> 00:07:55.840 mesh representation<br> 00:07:58.000 the vertex representations from all gmh<br> 00:08:00.960 curve layers<br> 00:08:02.080 as well as the global mesh<br> 00:08:03.440 representation are further concatenated<br> 00:08:06.879 than processed through a three layer mlp<br> 00:08:09.039 to output the vertex attributes<br> 00:08:11.199 either the displacement or the attention<br> 00:08:15.280 with all the components introduced above<br> 00:08:17.599 we have built the complete joint<br> 00:08:18.960 prediction mode which detects joints<br> 00:08:20.639 from a single input mesh</p><h3 id="training" tabindex="-1"><a class="header-anchor" href="#training"><span>Training</span></a></h3><p>00:08:23.360 the training of the joint prediction<br> 00:08:24.800 module consists of two steps<br> 00:08:27.280 in the first step we pre-treat the word<br> 00:08:29.520 extension module with heuristically<br> 00:08:31.440 generated ground truth masks<br> 00:08:32.958 as you&#39;ll see on the right the masks are<br> 00:08:35.760 1 as you&#39;ll see here in red and blue for<br> 00:08:37.839 zeros<br> 00:08:39.360 for each training mesh we find vertices<br> 00:08:41.679 closest to each joint at different<br> 00:08:43.519 directions perpendicular to the bones<br> 00:08:46.160 we use cross entropy to measure<br> 00:08:47.920 consistency between the masks and the<br> 00:08:49.680 neural tension<br> 00:08:51.839 in the second step we minimize the<br> 00:08:53.839 symmetric transfer distance<br> 00:08:55.440 between collapsed vertices and the<br> 00:08:57.040 training joints<br> 00:08:58.880 the loss is differentiable with regard<br> 00:09:00.720 to all the parameters of the joint<br> 00:09:02.320 prediction stage<br> 00:09:03.680 including the bandwidth the displacement<br> 00:09:06.000 network and the attention network<br> 00:09:08.880 we found that adding supervisory signal<br> 00:09:10.800 to the vertex displacements before<br> 00:09:12.560 clustering<br> 00:09:13.360 help improves joint detection<br> 00:09:14.959 performance<br> 00:09:16.720 to this end we also minimize transfer<br> 00:09:19.040 distance between displaced<br> 00:09:20.480 points and ground truth joints firing<br> 00:09:23.040 tighter clusters</p><h2 id="connectivity-prediction" tabindex="-1"><a class="header-anchor" href="#connectivity-prediction"><span>Connectivity Prediction</span></a></h2><p>00:09:26.320 given the joints extracted from the<br> 00:09:28.160 previous stage the connectivity<br> 00:09:30.160 prediction module determines how these<br> 00:09:32.000 joints should be connected<br> 00:09:33.440 to form the animation skeleton at the</p><p>00:09:36.080 heart of the stage<br> 00:09:37.040 lines a learned neural module that<br> 00:09:38.720 outputs the probability of connecting<br> 00:09:40.560 each pair of joints via a bone<br> 00:09:43.360 such module we call bone net takes as<br> 00:09:46.399 input our predicted joints along with<br> 00:09:48.800 the input mesh<br> 00:09:50.160 and outputs the probability for<br> 00:09:51.760 connecting each pair of joints via boom<br> 00:09:55.200 the architecture of the module is shown<br> 00:09:57.040 in the green box<br> 00:09:59.360 for each pair of joints the module<br> 00:10:01.519 processes three representations<br> 00:10:04.320 first we capture the skeleton geometry<br> 00:10:06.640 with the point net operating on the<br> 00:10:08.160 joint locations<br> 00:10:09.760 then we capture the global shape<br> 00:10:11.200 geometry with our mesh network<br> 00:10:14.000 and finally an mlp for each candidate<br> 00:10:16.240 phone<br> 00:10:17.600 the bone probability is computed via a<br> 00:10:19.839 two layer mlp operating on the<br> 00:10:21.440 concatenation of these three<br> 00:10:22.959 representations<br> 00:10:26.000 besides pairwise connectivity<br> 00:10:27.600 probability we also selected the root<br> 00:10:29.920 joint<br> 00:10:30.560 using a neural module called roonette<br> 00:10:33.600 its internal architecture follows<br> 00:10:35.279 boomnet<br> 00:10:37.360 with this pairwise bone probabilities<br> 00:10:39.440 and the predicted route as the starting<br> 00:10:41.200 node<br> 00:10:41.839 we apply prime&#39;s algorithm to create a<br> 00:10:44.160 minimum spanning tree presenting the<br> 00:10:45.760 animation skeleton<br> 00:10:47.680 we found that using these bone<br> 00:10:49.040 probabilities to extract the mst<br> 00:10:51.839 resulted in skeletons that are agreeing<br> 00:10:53.760 with the animator created ones<br> 00:10:55.200 more in topology</p><h3 id="training-1" tabindex="-1"><a class="header-anchor" href="#training-1"><span>Training</span></a></h3><p>00:10:58.240</p><p>to train the connectivity prediction module<br> 00:11:00.160 we build a probability matrix storing<br> 00:11:02.399 the probability for connecting each pair<br> 00:11:04.160 of joints<br> 00:11:04.800 with a bond based on our prediction we<br> 00:11:07.600 also form such metrics with the<br> 00:11:09.200 animator-grade escalator<br> 00:11:11.120 including the connectivity of the<br> 00:11:12.839 skeleton<br> 00:11:14.160 the value is 1 if the corresponding<br> 00:11:16.320 training joints are connected<br> 00:11:19.360 now the parameters of bone net can be<br> 00:11:21.360 learned using binary cross entropy<br> 00:11:23.600 between the training adjacency matrix<br> 00:11:25.839 and the predicted probabilities<br> 00:11:29.360 after producing the animation skeleton<br> 00:11:31.279 the final stage<br> 00:11:32.399 skinning prediction module applies<br> 00:11:34.800 another neural network to predict the<br> 00:11:36.640 skinny ways for each<br> 00:11:37.920 mesh vertex to complete the rigging process</p><h2 id="skinning-prediction" tabindex="-1"><a class="header-anchor" href="#skinning-prediction"><span>Skinning Prediction</span></a></h2><p>00:11:41.200 to perform skinning we first extract the<br> 00:11:43.760 mesh representation<br> 00:11:45.040 capture the spatial relationship of mesh<br> 00:11:47.040 vertices with respect to the skeleton<br> 00:11:49.680 given the vertex on the mesh for example<br> 00:11:52.079 as shown here in purple<br> 00:11:53.920 we compute the volumetric geodestic<br> 00:11:55.920 distance to all the bones passing<br> 00:11:57.519 through the interior mesh volume<br> 00:12:00.079 then we sort the bones according to<br> 00:12:01.839 their geodesic distance to the vertex<br> 00:12:04.399 and created an order feature sequence<br> 00:12:06.320 after the k spoon<br> 00:12:08.240 the feature vector for each bone<br> 00:12:09.600 concatenates the 3d positions of the<br> 00:12:11.760 starting and<br> 00:12:12.320 ending joints of this bone and the<br> 00:12:14.639 inverse of the volumetric geodesic<br> 00:12:16.399 distance from the vertex to this bone<br> 00:12:18.880 the final vertex representation is<br> 00:12:20.959 formed by concatenating the vertex<br> 00:12:22.880 position<br> 00:12:23.600 and the feature representations of the<br> 00:12:25.120 nearest case formed by the ordered<br> 00:12:26.880 sequence<br> 00:12:28.320 the screening prediction module converts<br> 00:12:30.160 the above skeleton aware mesh<br> 00:12:31.680 representations to skinny waves<br> 00:12:33.680 with the gmh net</p><h3 id="training-2" tabindex="-1"><a class="header-anchor" href="#training-2"><span>Training</span></a></h3><p>we train the parameters<br> 00:12:37.600 of our skin network<br> 00:12:39.040 so that the estimated skinning weight is<br> 00:12:40.880 s occurring as much as possible with the<br> 00:12:42.800 training ones<br> 00:12:45.360 by treating the vertex skinning weights<br> 00:12:47.200 as a probability distribution<br> 00:12:49.120 we use cross central ps loss</p><p>now we show<br> 00:12:52.639 the experimental results and the<br> 00:12:54.000 evaluation of our method</p><h3 id="dataset" tabindex="-1"><a class="header-anchor" href="#dataset"><span>Dataset</span></a></h3><p>00:12:56.800 to train our method and alternatives we<br> 00:12:59.200 chose the modus resource rignet dataset<br> 00:13:01.600 of 3d articulated characters which<br> 00:13:03.760 provided now overlapping training and<br> 00:13:05.440 testing split<br> 00:13:06.639 and contains diverse characters the data<br> 00:13:09.440 set contains 2703 rigged characters<br> 00:13:12.880 mined from an online repository spanning<br> 00:13:15.760 several categories<br> 00:13:17.200 including humanoid cultural pets birds<br> 00:13:20.959 fish robots toys and other fictional<br> 00:13:24.560 characters<br> 00:13:26.000 here we show some examples from the data set</p><h2 id="results" tabindex="-1"><a class="header-anchor" href="#results"><span>Results</span></a></h2><h3 id="extraction" tabindex="-1"><a class="header-anchor" href="#extraction"><span>Extraction</span></a></h3><p>00:13:29.519 for skeleton extraction here we show<br> 00:13:31.680 some artists created models<br> 00:13:34.320 and the results from our approach<br> 00:13:37.360 our results are going well with the ones<br> 00:13:39.120 created by animators<br> 00:13:42.480 here we show the skeleton produced by<br> 00:13:44.240 pinocchio<br> 00:13:45.680 we highlight the mistakes introduced the<br> 00:13:47.199 weaponoqueue with red boxes<br> 00:13:51.199 we also show the results produced by a<br> 00:13:53.040 previous volumetric technique<br> 00:13:55.199 with mistakes highlighted with red boxes</p><h3 id="skinning-weight" tabindex="-1"><a class="header-anchor" href="#skinning-weight"><span>Skinning Weight</span></a></h3><p>00:14:00.240 next we show the skinny weight<br> 00:14:01.680 prediction comparison<br> 00:14:04.160 first are the results from our method<br> 00:14:06.720 here we visualize skinny ways<br> 00:14:08.720 where red means hair value and skinning<br> 00:14:11.600 arrow maps<br> 00:14:12.399 where yellow color indicates hair arrow<br> 00:14:15.279 and a different<br> 00:14:16.000 pose when we move the characters<br> 00:14:17.680 according to skinning<br> 00:14:19.760 our predicted skinning weights capture<br> 00:14:21.440 the underlying articulate parts<br> 00:14:22.800 accurately<br> 00:14:25.760 here we show the results from a previous<br> 00:14:27.760 learning based method for skinning<br> 00:14:30.399 their results overextend the skinny ways<br> 00:14:32.480 to a larger area than expected<br> 00:14:35.839 finally we show results from a previous<br> 00:14:37.920 geometry method for skinning<br> 00:14:40.000 the arrow is even higher<br> 00:14:43.360 after training our method is able to rig<br> 00:14:45.839 diverse 3d models<br> 00:14:47.360 and they even generalize to models with<br> 00:14:49.199 different structure and parts<br> 00:14:51.600 here are examples from our test set</p><h3 id="quantitative" tabindex="-1"><a class="header-anchor" href="#quantitative"><span>Quantitative</span></a></h3><p>00:14:57.040 here we show the quantitative comparison<br> 00:14:59.040 results of skeleton prediction to other<br> 00:15:00.720 skeleton prediction methods<br> 00:15:02.959 including pinocchio and my previous<br> 00:15:05.199 volumetric based learning approach<br> 00:15:09.040 we applied the hungarian algorithm to<br> 00:15:10.959 form a matching between predicted joints<br> 00:15:12.959 and the ground choose ones<br> 00:15:14.560 the iou precession and recall are<br> 00:15:16.959 defined based on the resulting matching<br> 00:15:19.440 higher numbers indicate better<br> 00:15:21.040 performance<br> 00:15:23.760 we also evaluate transfer distance<br> 00:15:25.839 between predict and ground truth joints<br> 00:15:30.000 between joints and bones and between<br> 00:15:32.320 bones<br> 00:15:34.720 to measure transfer distance of bones we<br> 00:15:36.880 density sample on the bone<br> 00:15:38.399 and calculate the transfer distance<br> 00:15:39.920 between the samples<br> 00:15:41.839 for transfer distance lower numbers<br> 00:15:44.160 indicate better performance<br> 00:15:46.560 from the table we can see our method<br> 00:15:48.800 outperforms the rest according to all<br> 00:15:52.839 measures</p><p>00:15:54.000 here we show the quantitative comparison<br> 00:15:55.759 of skinning waste prediction<br> 00:15:57.199 to geometrical proteins bbw and geovoxel<br> 00:16:00.639 as well as a learning based approach<br> 00:16:02.320 neuroscience<br> 00:16:04.639 precession and recall are measured by<br> 00:16:06.399 finding the bones that influence each<br> 00:16:08.079 vertex significantly<br> 00:16:10.320 average l1 measures the l1 norm of the<br> 00:16:12.880 difference between the predicted<br> 00:16:14.079 skinning vector<br> 00:16:15.120 and the reference one averaged for all<br> 00:16:17.199 measuring vertices<br> 00:16:18.800 average and the maximal distance<br> 00:16:20.560 measures the euclidean distance between<br> 00:16:22.160 the position of vertices<br> 00:16:23.680 deformed based on the ground true<br> 00:16:25.120 skinning and the predictive skinning<br> 00:16:28.000 our approach achieves the best result on<br> 00:16:30.160 all measures</p><h3 id="tests" tabindex="-1"><a class="header-anchor" href="#tests"><span>tests</span></a></h3><p>00:16:32.880 here we should test the 3d models that<br> 00:16:35.040 were animated based on our predictor<br> 00:16:36.880 rigs<br> 00:16:38.160 our predicted rigs capture the arm and<br> 00:16:40.399 leg&#39;s articulation correctly<br> 00:16:43.199 here we show another example that were<br> 00:16:45.040 animated based on our predicted skeleton<br> 00:16:47.120 and skinning<br> 00:16:48.079 where all the limbs are rigged correctly<br> 00:16:49.920 by our method<br> 00:16:52.399 our method can handle night human noise<br> 00:16:54.720 predicting reasonable bones for this cat<br> 00:16:56.720 here<br> 00:17:00.160 our predicted skeleton across within<br> 00:17:02.160 families of test shapes are fairly<br> 00:17:03.920 consistent<br> 00:17:05.039 we can apply automatic motion transfer<br> 00:17:06.799 techniques to animate them all together</p><h3 id="evaluation" tabindex="-1"><a class="header-anchor" href="#evaluation"><span>Evaluation</span></a></h3><p>00:17:13.359 to summarize our method presents the<br> 00:17:15.599 first step<br> 00:17:16.400 towards the learning-based complete<br> 00:17:18.240 solution to character rigs<br> 00:17:20.079 including skeleton creation and skinny<br> 00:17:21.919 waste prediction<br> 00:17:24.559 our approach doesn&#39;t make assumption<br> 00:17:26.240 about the input on shape class and<br> 00:17:27.839 structure<br> 00:17:29.200 therefore it can be generalized to<br> 00:17:31.120 characters with varying structure and<br> 00:17:32.880 morphology<br> 00:17:34.960 our approach also provides a single<br> 00:17:36.640 parameter that users can tune to control<br> 00:17:38.799 the output granularity<br> 00:17:41.360 our approach does have limitations and<br> 00:17:43.679 exciting avenues for future work<br> 00:17:46.400 first our method currently use a per<br> 00:17:48.799 stage training approach<br> 00:17:50.880 ideally the skinning loss could be back<br> 00:17:53.360 propagated to all states of network<br> 00:17:55.520 to improve joint prediction second<br> 00:17:59.200 our data set has the limitations it can<br> 00:18:01.679 turns one rig promoter<br> 00:18:04.160 many rigs often don&#39;t include bones for<br> 00:18:06.240 small parts<br> 00:18:07.280 like feet fingers clothing and<br> 00:18:10.480 accessories which makes our train model<br> 00:18:13.280 less<br> 00:18:13.600 predictive for these joints<br> 00:18:17.280 finally our current bandwidth parameter<br> 00:18:19.440 explores one mode of<br> 00:18:20.559 variation exploring a richer space to<br> 00:18:23.600 interactively control skeletal<br> 00:18:25.120 morphology and resolution<br> 00:18:26.880 is another interesting research<br> 00:18:28.320 direction<br> 00:18:30.799 we show here our project page including<br> 00:18:33.039 source code and our data set<br> 00:18:35.600 thank you for your attention</p></div><!----><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="auto-link external-link vp-meta-label" href="https://github.com/davidliuk/blog/edit/main/docs/education/ms/6600/siggraph/pre/transcript (copy).md" aria-label="Edit this page on GitHub" rel="noopener noreferrer" target="_blank"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon" name="edit"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Edit this page on GitHub<!----></a></div><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">Last update: </span><span class="vp-meta-info" data-allow-mismatch="text">5/28/2025, 4:32:52 PM</span></div><div class="contributors"><span class="vp-meta-label">Contributors: </span><!--[--><!--[--><span class="vp-meta-info" title="email: l729641074@163.com">David</span><!--]--><!--]--></div></div></footer><!----><div id="comment" class="giscus-wrapper input-top vp-comment" vp-comment style="display:block;"><div class="loading-icon-wrapper" style="display:flex;align-items:center;justify-content:center;height:96px"><svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" preserveAspectRatio="xMidYMid" viewBox="25 25 50 50"><animateTransform attributeName="transform" type="rotate" dur="2s" keyTimes="0;1" repeatCount="indefinite" values="0;360"></animateTransform><circle cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="4" stroke-linecap="round"><animate attributeName="stroke-dasharray" dur="1.5s" keyTimes="0;0.5;1" repeatCount="indefinite" values="1,200;90,200;1,200"></animate><animate attributeName="stroke-dashoffset" dur="1.5s" keyTimes="0;0.5;1" repeatCount="indefinite" values="0;-35px;-125px"></animate></circle></svg></div></div><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper" vp-footer><div class="vp-footer">David's Blog</div><div class="vp-copyright">Copyright © 2025 David Liu </div></footer></div><!--]--><!--[--><!----><!----><!--[--><!--]--><!--]--><!--]--></div>
    <script type="module" src="/blog/assets/app-isOblzBz.js" defer></script>
  </body>
</html>
