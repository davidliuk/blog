<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.18" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.59" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://davidliuk.github.io/blog/blog/education/ms/6600/siggraph/pre/transcript.html"><meta property="og:site_name" content="David's Blog"><meta property="og:title" content="Transcript"><meta property="og:description" content="Transcript Hi everyone, I’m David Liu. Today I’ll walk you through RigNet, a neural approach to automating character rigging — a traditionally time-consuming task. Overview Let'..."><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><meta property="og:updated_time" content="2025-05-28T16:32:52.000Z"><meta property="article:modified_time" content="2025-05-28T16:32:52.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Transcript","image":[""],"dateModified":"2025-05-28T16:32:52.000Z","author":[{"@type":"Person","name":"David Liu","url":"https://github.com/davidliuk"}]}</script><link rel="icon" href="/blog/favicon.ico"><link rel="icon" href="/blog/assets/icon/512.png" type="image/png" sizes="512x512"><link rel="icon" href="/blog/assets/icon/196.png" type="image/png" sizes="192x192"><link rel="icon" href="/blog/assets/icon/196.png" type="image/png" sizes="196x196"><link rel="manifest" href="/blog/manifest.webmanifest" crossorigin="use-credentials"><meta name="theme-color" content="#46bd87"><link rel="apple-touch-icon" href="/blog/assets/icon/152.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="msapplication-TileImage" content="/blog/assets/icon/144.png"><meta name="msapplication-TileColor" content="#ffffff"><title>Transcript | David's Blog</title><meta name="description" content="Transcript Hi everyone, I’m David Liu. Today I’ll walk you through RigNet, a neural approach to automating character rigging — a traditionally time-consuming task. Overview Let'...">
    <link rel="preload" href="/blog/assets/style-CBOU5JPC.css" as="style"><link rel="stylesheet" href="/blog/assets/style-CBOU5JPC.css">
    <link rel="modulepreload" href="/blog/assets/app-CGXHKXsa.js"><link rel="modulepreload" href="/blog/assets/transcript.html-DWJr3T9S.js"><link rel="modulepreload" href="/blog/assets/plugin-vue_export-helper-DlAUqK2U.js">
    
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><div class="theme-container no-sidebar external-link-icon has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><!--[--><a class="route-link vp-brand" href="/blog/" aria-label="Take me home"><img class="vp-nav-logo" src="/blog/favicon.ico" alt><!----><span class="vp-site-name hide-in-pad">David&#39;s Blog</span></a><!--]--><!----></div><div class="vp-navbar-center"><!----><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/blog/" aria-label="About Me"><!---->About Me<!----></a></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="Algorithm"><!--[--><span class="font-icon icon edit" style=""></span>Algorithm<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/blog/algo/framework/" aria-label="Framework"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->Framework<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/blog/algo/summary/" aria-label="Summary"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->Summary<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/blog/algo/faq/" aria-label="FAQ"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->FAQ<!----></a></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="Computer Science"><!--[--><span class="font-icon icon edit" style=""></span>Computer Science<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Basic</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/cs/basic/network/" aria-label="Network"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->Network<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/cs/basic/os/" aria-label="Operating System"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->Operating System<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Database</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/cs/database/mysql/" aria-label="MySQL"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->MySQL<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/cs/database/redis/" aria-label="Redis"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->Redis<!----></a></li></ul></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="Software Engineer"><!--[--><span class="font-icon icon edit" style=""></span>Software Engineer<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Lang</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/se/lang/java/" aria-label="Java"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->Java<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">DevOps</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/se/devops/unix/" aria-label="Unix"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->Unix<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/se/devops/docker/" aria-label="Docker"><!---->Docker<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/se/devops/k8s/" aria-label="Kubernetes"><!---->Kubernetes<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Design</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/se/design/pattern/" aria-label="Design Pattern"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->Design Pattern<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/se/design/ood/" aria-label="Object-Oriented Design"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->Object-Oriented Design<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/se/design/system/" aria-label="System Design"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->System Design<!----></a></li></ul></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/blog/se/tools/" aria-label="Tools"><!---->Tools<!----></a></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="Tests"><!--[--><span class="font-icon icon edit" style=""></span>Tests<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">GRE</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/test/GRE/QUANTITATIVE/" aria-label="QUANTITATIVE"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->QUANTITATIVE<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/test/GRE/VERBAL/" aria-label="VERBAL"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->VERBAL<!----></a></li></ul></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">TOEFL</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/test/TOEFL/LISTENING/" aria-label="LISTENING"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->LISTENING<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/test/TOEFL/READING/" aria-label="READING"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->READING<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/test/TOEFL/SPEAKING/" aria-label="SPEAKING"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->SPEAKING<!----></a></li><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/blog/test/TOEFL/WRITING/" aria-label="WRITING"><!--[--><span class="font-icon icon edit" style=""></span><!--]-->WRITING<!----></a></li></ul></li></ul></button></div></div></nav><!--]--><!----></div><div class="vp-navbar-end"><!----><!--[--><!----><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/davidliuk/blog" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-color-mode-switch" id="color-mode-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" name="auto" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" name="dark" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" name="light" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!----><!--]--><!----><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->Transcript</h1><div class="page-info"><span class="page-author-info" aria-label="Author🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://github.com/davidliuk" target="_blank" rel="noopener noreferrer">David Liu</a></span><span property="author" content="David Liu"></span></span><!----><span class="page-date-info" aria-label="Writing Date📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span data-allow-mismatch="text">May 28, 2025</span><meta property="datePublished" content="2025-05-28T16:32:52.000Z"></span><!----><!----><span class="page-reading-time-info" aria-label="Reading Time⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 9 min</span><meta property="timeRequired" content="PT9M"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc" vp-toc><!----><div class="vp-toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon" name="print"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#overview">Overview</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#background">Background</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#rigging-binding-skinning">Rigging, Binding, Skinning</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#motivation">Motivation</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#challenge">Challenge</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#overview-1">Overview</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#joint-prediction">Joint Prediction</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#architecture">Architecture</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#bandwidth">Bandwidth</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#architecture-1">Architecture</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#gmedgeconv">GMEdgeConv</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#gmedgenet">GMEdgeNet</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#training">Training</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#connectivity-prediction">Connectivity Prediction</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#architecture-2">Architecture</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#bonenet">BoneNet</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#architecture-3">Architecture</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#training-1">Training</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#skinning-prediction">Skinning Prediction</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#training-2">Training</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#dataset">Dataset</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#results">Results</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#skeleton-extraction">Skeleton Extraction</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#skinning-weight">Skinning Weight</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#quantitative-comparison">Quantitative Comparison</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#generalization">Generalization</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#evaluation">Evaluation</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#questions">Questions</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#evaluation-1">Evaluation</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#aceptance">Aceptance</a></li><!----><!--]--></ul></li><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!----></aside></div><!----><div class="theme-hope-content" vp-content><h1 id="transcript" tabindex="-1"><a class="header-anchor" href="#transcript"><span>Transcript</span></a></h1><p>Hi everyone, I’m David Liu. Today I’ll walk you through <strong>RigNet</strong>, a neural approach to automating character rigging — a traditionally time-consuming task.</p><h2 id="overview" tabindex="-1"><a class="header-anchor" href="#overview"><span>Overview</span></a></h2><p>Let&#39;s have a quick view of today&#39;s topics:</p><ol><li>I’ll start with some background including the related knowledge we&#39;ve learned, challenges and motivation.</li><li>Then, I’ll have an quick overview of RigNet’s approach and walk through the three core modules: joint prediction, connectivity prediction, and skinning prediction.</li><li>After that, I’ll show some results and discuss evaluations.</li></ol><h2 id="background" tabindex="-1"><a class="header-anchor" href="#background"><span>Background</span></a></h2><h3 id="rigging-binding-skinning" tabindex="-1"><a class="header-anchor" href="#rigging-binding-skinning"><span><strong>Rigging, Binding, Skinning</strong></span></a></h3><p>From last semester&#39;s CI562, we&#39;ve learned a chapter of <strong>Rigging, Binding, Skinning</strong>. We took a lot time on implementing the skinning methods like LBS or DQS. But before apply these skinning methods, we should firstly have model&#39;s skeleton and skinning weights.</p><h3 id="motivation" tabindex="-1"><a class="header-anchor" href="#motivation"><span>Motivation</span></a></h3><p>Character rigs have been the <strong>backbone of articulated figure animation</strong> for over decades.</p><p>Without rigs, it&#39;s difficult for animators to animate character models effectively.</p><p>The motivation behind RigNet is to <strong>automate the rigging process</strong>.</p><p>As a result, animators can directly <strong>use the generated skeletons</strong> to animate their models, which will save a lot of time and effort.</p><h3 id="challenge" tabindex="-1"><a class="header-anchor" href="#challenge"><span>Challenge</span></a></h3><p>Predicting a skeleton and skinning weight from an arbitrary single static mesh is not easy.</p><hr><p><strong>Skeleton</strong></p><p>As shown here, one challenge is that <strong>characters can vary significantly</strong> in both the <strong>number of parts</strong> and their <strong>overall structure</strong>.</p><hr><p><strong>Skinning</strong></p><p>Similarly, when computing skinning weights, animators perceive some structures as highly <strong>rigid</strong> and others as more <strong>flexible</strong>.</p><p>For example, here we can see the skeleton of a <strong>snail character</strong>.</p><ul><li>The skinning weights on the shell move <strong>rigidly as a whole</strong> according to a single bone,</li><li>while the rest of the body <strong>deforms more smoothly</strong> in response to the other bones.</li></ul><p><em>A learning method should be capable of <strong>capturing this variability in skinning behavior</strong>.</em></p><hr><p><strong>Level of Detail</strong></p><p>Finally, another challenge for a rigging method is to allow <strong>easy and direct control</strong> over the <strong>level of detail</strong> in the output skeleton.</p><p>As you can see from these animator-created rigs, while animators largely agree on the <strong>topology</strong> and <strong>layout of joints</strong> for a given character, there is still <strong>ambiguity</strong>—both in terms of the <strong>number of joints</strong> and their <strong>exact placement</strong>.</p><h2 id="overview-1" tabindex="-1"><a class="header-anchor" href="#overview-1"><span>Overview</span></a></h2><p>To address these challenges, <strong>RigNet</strong> proposes a deep learning architecture composed of <strong>three modules</strong>:</p><hr><p><strong>1. Skeletal Joint Prediction Module</strong></p><p>First, we apply a <strong>joint prediction module</strong>, which is trained to predict both the <strong>appropriate number of joints</strong> and their <strong>placement</strong>.</p><p>Here we provide users with an <strong>optional parameter</strong> that allows them to control the <strong>level of detail</strong> in the output skeleton.</p><hr><p><strong>2. Skeleton Connectivity Module</strong></p><p>Second, to form a skeleton from the predicted joints, we apply a <strong>connectivity prediction module</strong>, which predicts the <strong>hierarchical tree structure</strong> connecting the joints.</p><p>The resulting bone structure is a function of both the <strong>predicted joints</strong> and the <strong>shape features</strong> of the input character.</p><hr><p><strong>3. Skinning Prediction Module</strong></p><p>Finally, we apply a <strong>skinning prediction module</strong> to produce <strong>skinning weights</strong>, indicating the degree of influence each <strong>mesh vertex</strong> receives from different <strong>bones</strong>.</p><hr><p>All of these modules are trained in a <strong>supervised manner</strong> from animator-created character rigs.</p><p>In the following sections, we will describe the <strong>input</strong> and <strong>internal design</strong> of each module in detail.</p><h2 id="joint-prediction" tabindex="-1"><a class="header-anchor" href="#joint-prediction"><span>Joint Prediction</span></a></h2><h3 id="architecture" tabindex="-1"><a class="header-anchor" href="#architecture"><span>Architecture</span></a></h3><p>00:04:29.040</p><p>The joint prediction module is trained to predict the number and location of skeleton joints.</p><p>To this end, it learns to displace mesh geometry towards candidate joint locations.</p><p>The module is based on a GMEdgeNet, which extracts topology- and geometry-aware features from the mesh to learn these displacements.</p><p>At the same time, we also learn a weight function over the input mesh, which is used to reveal which surface areas are more relevant for localizing joints. This can be seen as a form of neural mesh attention.</p><p>After that, we introduce a diffe&#39;rentiable clustering scheme, which uses both displaced vertices and the neural mesh attention to collapse the vertices further to potential positions of the joints. Since areas with higher point density and greater mesh attention are strong indicators of joint presence, we resort to mean shift clustering and then maximum separation to extract joints.</p><p>In classical mean shift clustering, each data point is equipped with a kernel function. At each iteration, all points are shifted towards density modes.</p><p>Here, we show the mean shift equation. We employ a variant of mean shift clustering, where the kernel is also adjusted by the vertex attention. In this manner, points with greater attention influence the estimation of density more.</p><p>Here is the shift equation in our implementation. <em>We use the Epanechnikov kernel in our implementation.</em></p><p>Note that the kernel function takes the parameter <em>h</em> as bandwidth. The bandwidth can be learned simultaneously as we train the network.</p><p>The bandwidth also allows user to control on the level of detail of the output skeleton.</p><p>Modifying the bandwidth directly affects the level of detail of the output skeleton.</p><h3 id="bandwidth" tabindex="-1"><a class="header-anchor" href="#bandwidth"><span>Bandwidth</span></a></h3><p>Here is an example:</p><ul><li>lowering the bandwidth parameter results in denser joint placement,</li><li>while increasing it results in a sparse skeleton.</li></ul><p>By overriding the learned bandwidth, users can adjust the results to their preference.</p><hr><h3 id="architecture-1" tabindex="-1"><a class="header-anchor" href="#architecture-1"><span>Architecture</span></a></h3><p><em>At test time, the mode centers of clusters are extracted with no maximum separation as the final detected joints.</em></p><p>Now we&#39;ll discuss more details about the networks used to learn the vertex displacement and attention.</p><p>We call this network <strong>GMEdgeNet</strong>. The main operation of this network is geodesic mesh convolution, which we call <strong>GMEdgeConv</strong>.</p><hr><h3 id="gmedgeconv" tabindex="-1"><a class="header-anchor" href="#gmedgeconv"><span>GMEdgeConv</span></a></h3><p>The GMEdgeConv is inspired by the H-convolution. The main difference is that our operator is applied to meshes and geodesic numbers.</p><p>Specifically, given a surface vertex, we consider its <strong>one-ring mesh neighbors</strong> and also the vertices located within the geodesic ball centered at it.</p><p>We also found it’s better to learn separate MLPs for mesh and geodesic neighborhoods, and then con&#39;catenate their outputs and process them through another MLP.</p><p>In this manner, the networks learn to weight the importance of topology-aware features over more geometry-aware ones.</p><hr><h3 id="gmedgenet" tabindex="-1"><a class="header-anchor" href="#gmedgenet"><span>GMEdgeNet</span></a></h3><p>In GMEdgeNet, we stack three GMEdgeConv layers. Each of the GMEdgeConv layers is followed by a global max pooling layer.</p><p>The representations from each pooling layer are con&#39;catenated to form a global mesh representation.</p><p>The vertex representations from all GMEdgeConv layers, as well as the global mesh representation, are further con&#39;catenated, then processed through a three-layer MLP to output the vertex attributes—either the displacement or the attention.</p><p>With all the components introduced above, we have built the complete <strong>joint prediction module</strong>, which detects joints from a single input mesh.</p><h3 id="training" tabindex="-1"><a class="header-anchor" href="#training"><span>Training</span></a></h3><p>The training of the joint prediction module consists of two steps.</p><ol><li><p>In the first step, we pre-train the weight attention module with heu&#39;ristically generated ground truth masks.</p><p>As you’ll see on the right, the masks are red for 1 and blue for 0.</p><p>For each training mesh, we find vertices closest to each joint at different directions perpen&#39;dicular to the bones.</p><p>We use <strong>cross-entropy</strong> to measure consistency between the masks and the neural attention.</p></li><li><p>In the second step, we minimize the mesh chamfer distance between collapsed vertices and the training joints.</p><p>The loss is diffe&#39;rentiable with regard to all the parameters of the joint prediction stage, including the bandwidth, the displacement network, and the attention network.</p><p><em>We found that adding supervisory signal to the vertex displacements before clustering helps improve joint detection performance.</em></p><p><em>To this end, we also minimize the transfer distance between displaced points and ground truth joints, forming tighter clusters.</em></p></li></ol><h2 id="connectivity-prediction" tabindex="-1"><a class="header-anchor" href="#connectivity-prediction"><span>Connectivity Prediction</span></a></h2><h3 id="architecture-2" tabindex="-1"><a class="header-anchor" href="#architecture-2"><span>Architecture</span></a></h3><p>Given the joints extracted from the previous stage, the connectivity prediction module determines how these joints should be connected to form the skeleton.</p><p>This module outputs the probability of connecting each pair of joints via a bone, which we call <strong>BoneNet</strong>. It takes the predicted joints along with the input mesh as input, and outputs the probability for connecting each pair of joints via a bone.</p><h3 id="bonenet" tabindex="-1"><a class="header-anchor" href="#bonenet"><span>BoneNet</span></a></h3><p>For each pair of joints, the module processes three representations:</p><ol><li>First, we use PointNet to capture the <strong>skeleton geometry</strong> from the joint locations.</li><li>Then, we use GMEdgeNet to capture the <strong>global shape geometry</strong>.</li><li>Finally, an MLP processes features for each <strong>candidate bone</strong>.</li></ol><p>The bone probability is computed via a two-layer MLP operating on the con&#39;catenation of these three representations.</p><h3 id="architecture-3" tabindex="-1"><a class="header-anchor" href="#architecture-3"><span>Architecture</span></a></h3><p>Besides pairwise connectivity probability, we also select the <strong>root joint</strong> using a neural module called <strong>RootNet</strong>. Its internal architecture follows BoneNet.</p><p>With these pairwise bone probabilities and the predicted root as the starting node, we apply <strong>Prim&#39;s algorithm</strong> to create a minimum spanning tree that represents the skeleton.</p><p><em>We found that using these bone probabilities to extract the MST resulted in skeletons that better agree with animator-created ones in terms of topology.</em></p><h3 id="training-1" tabindex="-1"><a class="header-anchor" href="#training-1"><span>Training</span></a></h3><p>To train the connectivity prediction module, we build a <strong>matrix</strong> to store the probability for connecting each pair of joints with a bone based on our prediction.</p><p>We also form such a matrix from the animator-created skeleton, including the connectivity of the skeleton. The value is 1 if the corresponding joints are connected.</p><p>Now, the parameters of BoneNet can be learned using <strong>binary cross-entropy</strong> between the training matrix and the predicted probability matrix.</p><h2 id="skinning-prediction" tabindex="-1"><a class="header-anchor" href="#skinning-prediction"><span>Skinning Prediction</span></a></h2><p>After producing the animation skeleton, the final stage applies another network to predict the <strong>skinning weights</strong> for each mesh vertex.</p><hr><p>To perform skinning, we first extract the mesh representation and capture the spatial relationship of mesh vertices with respect to the skeleton.</p><p>Given a vertex on the mesh—for example, as shown here in purple—we compute the <strong>&#39;volumetric geodesic distance</strong> to all the bones passing through the interior of the mesh volume.</p><p>Then, we sort the bones according to their geodesic distance to the vertex and create an ordered feature sequence after the <em>k</em>-shortest bones.</p><p>The feature vector for each bone consists:</p><ul><li>The 3D <strong>positions</strong> of the starting and ending joints of the bone, and</li><li>The inverse of the <strong>&#39;volumetric geodesic distance</strong> from the vertex to this bone.</li></ul><p>The final <strong>vertex representation</strong> is formed by con&#39;catenating the vertex position and the feature representations of the <em>k</em> nearest bones from the ordered sequence.</p><hr><p>The skinning prediction module then converts the above skeleton-aware mesh representations into <strong>skinning weights</strong>, using <strong>GMEdgeNet</strong>.</p><hr><h3 id="training-2" tabindex="-1"><a class="header-anchor" href="#training-2"><span>Training</span></a></h3><p>We train the parameters of the skinning network so that the estimated skinning weights align as closely as possible with the training weights.</p><p>By treating the vertex skinning weights as a <strong>probability distribution</strong>, we use <strong>cross-entropy loss</strong> to supervise the learning.</p><p><em>Now we show the experimental results and the evaluation of our method.</em></p><h3 id="dataset" tabindex="-1"><a class="header-anchor" href="#dataset"><span>Dataset</span></a></h3><p>The dataset contains <strong>2,703 characters</strong> and spans a wide range of categories—including:</p><ul><li>Humanoids</li><li>Cultural figures</li><li>Pets</li><li>Birds</li><li>Fish</li><li>Robots</li><li>Toys</li><li>Other fictional characters</li></ul><p>Here, we show some example models from the dataset, illustrating its <strong>diversity in shape, topology, and articulation complexity</strong>.</p><h2 id="results" tabindex="-1"><a class="header-anchor" href="#results"><span>Results</span></a></h2><h3 id="skeleton-extraction" tabindex="-1"><a class="header-anchor" href="#skeleton-extraction"><span>Skeleton Extraction</span></a></h3><p>For skeleton extraction, we show some artist-created models alongside the results from our approach. Our results align well with those created by professional animators.</p><p>We also present skeletons produced by <strong>Pinocchio</strong>,</p><p>red boxes highlights its mistakes.</p><hr><h3 id="skinning-weight" tabindex="-1"><a class="header-anchor" href="#skinning-weight"><span>Skinning Weight</span></a></h3><p>Next, we compare skinning weight prediction results.</p><p>First are the results from our method. We visualize:</p><ul><li><strong>Skinning weights</strong> (red indicates high values)</li><li><strong>Skinning error maps</strong> (yellow indicates high error)</li></ul><p>When animating the characters with the predicted skinning, our method accurately captures the underlying articulated parts.</p><p>We then show results from a <strong>previous learning-based skinning method</strong>, whose skinning weights tend to <strong>overextend</strong> to larger areas than necessary.</p><p>Finally, results from a <strong>geometry-based skinning method</strong> are shown, with even <strong>higher error</strong>.</p><hr><h3 id="quantitative-comparison" tabindex="-1"><a class="header-anchor" href="#quantitative-comparison"><span>Quantitative Comparison</span></a></h3><p>We show quantitative results of <strong>skeleton prediction</strong> with other methods.</p><p>We apply the <strong>Hungarian algorithm</strong> to match predicted joints to ground truth ones. Metrics such as <strong>IoU, precision, and recall</strong> are defined based on this matching. Higher values indicate better performance.</p><p>We also evaluate <strong>transfer distance</strong> between:</p><ul><li>Predicted and ground truth joints</li><li>Joints and bones</li><li>Bones themselves</li></ul><p>To measure bone transfer distance, we <strong>densely sample</strong> along the bone and compute distances between samples.</p><p>In this metric, <strong>lower values indicate better results</strong>.</p><p>From the table, we can see our method <strong>outperforms all others across all metrics</strong>.</p><hr><p>For <strong>skinning weight prediction</strong>, we compare against:</p><ul><li>Geometrical approaches: <strong>BBW</strong> and <strong>GeoVoxel</strong></li><li>A learning-based approach: <strong>NeuroSkinning</strong></li></ul><p>Precision and recall are measured by identifying bones that significantly influence each vertex.<br> We also report:</p><ul><li><strong>Average L1 norm</strong> between predicted and reference skinning vectors</li><li><strong>Average and maximal Euclidean distance</strong> between deformed vertex positions (using ground-truth vs. predicted skinning)</li></ul><p>Our approach achieves the <strong>best results across all measures</strong>.</p><hr><h3 id="generalization" tabindex="-1"><a class="header-anchor" href="#generalization"><span><strong>Generalization</strong></span></a></h3><p>After training, RigNet is able to <strong>rig diverse 3D models</strong> and even <strong>generalizes to models with different structures and parts</strong>.</p><p>Here are some examples.</p><p>We test 3D models animated using our predicted rigs.</p><p>The rigs correctly capture the <strong>articulations of arms and legs</strong>.</p><p>We present another example animated with predicted skeleton and skinning, where <strong>all limbs are rigged correctly</strong>.</p><p>Our method can also <strong>handle non-human models</strong>, predicting reasonable bones for a <strong>cat model</strong>, for instance.</p><p><em>Our predicted skeletons are <strong>consistent across different shape families</strong>, allowing for <strong>automatic motion transfer</strong> to animate them together.</em></p><h2 id="evaluation" tabindex="-1"><a class="header-anchor" href="#evaluation"><span>Evaluation</span></a></h2><h3 id="questions" tabindex="-1"><a class="header-anchor" href="#questions"><span>Questions</span></a></h3><p>Here are the questions I would like to ask the author:</p><ol><li><p><strong>First</strong>, how does it handle characters with highly non-organic or mechanically complex structures, like robots or abstract shapes?</p></li><li><p><strong>Second</strong>, are there optimizations for real-time prediction?</p><p>Currently, the method focuses on accuracy over speed. There’s no built-in runtime optimization, which limits real-time applicability—but this is a promising area for future work.</p></li><li><p><strong>Third</strong>, can users enforce constraints or edit the output?</p><p>Not directly. RigNet doesn’t currently support user-defined constraints like fixing the root joint, nor does it provide post-prediction editing. This is one of the major limitations in terms of usability for animators.</p></li></ol><h3 id="evaluation-1" tabindex="-1"><a class="header-anchor" href="#evaluation-1"><span>Evaluation</span></a></h3><p>Let’s dive into the evaluation.</p><p><strong>RigNet has several strengths:</strong></p><ul><li>It’s the first end-to-end neural method that predicts both skeletons and skinning weights directly from raw meshes.</li><li>It offers <strong>user flexibility</strong>—you can adjust the level of skeletal detail.</li><li>It generalizes well across different character types and morphologies.</li><li>And it shows strong performance compared to existing methods.</li></ul><p><strong>But there are also limitations:</strong></p><ul><li>The paper doesn’t include a runtime analysis, which is critical for real-time applications.</li><li>And as mentioned earlier, user control is limited—there’s no way to apply constraints or refine the predicted rig manually.</li></ul><h3 id="aceptance" tabindex="-1"><a class="header-anchor" href="#aceptance"><span>Aceptance</span></a></h3><p>From my point of view, the reception of RigNet would be vaery positive.</p><p><strong>On the plus side</strong>, reviewers appreciated its novelty, thorough evaluation, and real-world relevance to animation workflows.</p><p><strong>The main criticism</strong> was around computational cost, particularly the lack of performance benchmarks for real-time or interactive scenarios.</p><p><strong>Overall</strong>, RigNet represents a significant step forward in automated rigging and opens the door to more flexible and efficient character setup in the future.</p><p><strong>Thank you for your attention!</strong></p><p>To summarize, RigNet is the <strong>first learning-based, complete solution for character rigging</strong>, including both <strong>skeleton creation</strong> and <strong>skinning weight prediction</strong>.</p><p>Our approach:</p><ul><li><strong>Makes no assumptions</strong> about input shape class or structure</li><li><strong>Generalizes well</strong> to characters with varying structures and morphologies</li><li><strong>Provides a single parameter</strong> for users to control output granularity</li></ul><p>However, there are some <strong>limitations and directions for future work</strong>:</p><ol><li><strong>Per-stage training</strong>: Currently, each stage is trained separately. Ideally, the <strong>skinning loss could be backpropagated</strong> to earlier stages to improve joint prediction.</li><li><strong>Dataset limitations</strong>: Our dataset contains only one rig per model. Many rigs <strong>omit bones for small parts</strong> (e.g., feet, fingers, clothing, accessories), which <strong>reduces prediction accuracy</strong> for such joints.</li><li><strong>Bandwidth parameter</strong>: Our current approach only explores <strong>one mode of variation</strong>. Future work could explore a <strong>richer space for interactively controlling skeletal morphology and resolution</strong>.</li></ol></div><!----><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="auto-link external-link vp-meta-label" href="https://github.com/davidliuk/blog/edit/main/docs/education/ms/6600/siggraph/pre/transcript.md" aria-label="Edit this page on GitHub" rel="noopener noreferrer" target="_blank"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon" name="edit"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Edit this page on GitHub<!----></a></div><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">Last update: </span><span class="vp-meta-info" data-allow-mismatch="text">5/28/2025, 4:32:52 PM</span></div><div class="contributors"><span class="vp-meta-label">Contributors: </span><!--[--><!--[--><span class="vp-meta-info" title="email: l729641074@163.com">David</span><!--]--><!--]--></div></div></footer><!----><div id="comment" class="giscus-wrapper input-top vp-comment" vp-comment style="display:block;"><div class="loading-icon-wrapper" style="display:flex;align-items:center;justify-content:center;height:96px"><svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" preserveAspectRatio="xMidYMid" viewBox="25 25 50 50"><animateTransform attributeName="transform" type="rotate" dur="2s" keyTimes="0;1" repeatCount="indefinite" values="0;360"></animateTransform><circle cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="4" stroke-linecap="round"><animate attributeName="stroke-dasharray" dur="1.5s" keyTimes="0;0.5;1" repeatCount="indefinite" values="1,200;90,200;1,200"></animate><animate attributeName="stroke-dashoffset" dur="1.5s" keyTimes="0;0.5;1" repeatCount="indefinite" values="0;-35px;-125px"></animate></circle></svg></div></div><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper" vp-footer><div class="vp-footer">David's Blog</div><div class="vp-copyright">Copyright © 2025 David Liu </div></footer></div><!--]--><!--[--><!----><!----><!--[--><!--]--><!--]--><!--]--></div>
    <script type="module" src="/blog/assets/app-CGXHKXsa.js" defer></script>
  </body>
</html>
