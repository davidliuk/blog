# 大文件处理问题

海量数据大多数，分成小文件处理，分治

https://blog.csdn.net/wanger61/article/details/110004130

https://blog.csdn.net/v_JULY_v/article/details/7382693?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-7382693-blog-110004130.pc_relevant_3mothn_strategy_recovery&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-7382693-blog-110004130.pc_relevant_3mothn_strategy_recovery



## 大文件排序

外排序



## 大文件找众数

摩尔投票法



## 两个大文件求交集

假设文件为a，b 

1. 直接遍历法。一般人第一时间都是想遍历吧。读取每一行a，在b中遍历，这样时间复杂度为O（n^2），显然一般人都不能接受这个时间复杂度。 

2. 哈希 + 分片的思想。先把a文件hash，在遍历b文件，去判断是否存在。

   时间复杂度降低为O（n） ，但是空间复杂度上来了，以空间换时间。 

   1. 将文件A中的hash(url)%100，生成100个小文件。
   2. 文件B中也hash(url)%100，生成100个小文件。
   3. 然后将A子文件001和B子文件001求交集，放入一个结果文件即可。

3. 布隆过滤器。但是布隆过滤器是有可能出现错误的，当时应该问问他是否可以出现小的错误？

对于大文件的话，一般都是使用分治思想，将文件分割成多个小文件来处理。




题目背景
给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url

主体思路
分治+hash

实现步骤
遍历文件A，对每个url使用hash(url) % 1000，根据所得的取值将url存储到1000个小文件中（a1,a2,…,a1000）（根据内存大小设定hash函数）
遍历文件B，使用同样的hash函数将B中的url存储到1000个小文件中（b1,b2,…,b1000）（这样相同的url就会被映射到下标相同的小文件中）
读取文件a1，简历hash表，再读取文件b1，遍历其中的url，若url在hash表中出现，说明为两文件共有，存入结果中。