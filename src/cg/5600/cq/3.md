# Concept Questions for HW03

(2 pts) What are the three different configuration cases when determining the intersection of a pixel row with a triangle edge? In all three cases, what simple criterion can one use to determine whether the triangle edge overlaps the pixel row at all?

(3 pts) How might one use barycentric interpolation to determine whether or not a given point in space lies within the bounds of a triangle? In rasterization, would this method be more efficient than row bound checking for determining which pixels lie within a triangle? Why or why not?

(5 pts) Describe in detail the sequence of transformations that must occur in order to project a set of triangles given in 3D world space into the coordinate system of the pixelized screen.



## Question 1

### three different configuration cases

- Point-slope equation solving alone does not account for the triangle’s bounds
- The edge is colinear to the pixel row being tested (perfectly horizontal, Slope of the triangle edge is 0.)
- Slope of the triangle edge is undefined (i.e. it’s perfectly vertical)

### simple criterion

If the pixel row’s Y coord is between the Y coords of an edge’s endpoints, it will overlap the edge within the bounding box.

## Question 2

### Determine

If

$S_1+S_2+S_3>S$

Where
$$
S=area(P_1,P_2,P_3)\\
S_1=area(P,P_2,P_3)\\
S_2=area(P,P_3,P_1)\\
S_3=area(P,P_1,P_2)\\
$$

### vs. Row bound checking

No, barycentric interpolation is generally less efficient for rasterization because it checks **individual points**. On the other hand, row-bound checking is faster, as it operates by scanning across pixel rows and only checking for **intersections**.

## Question 3

1. **View Transformation**: Converts vertices from world space to camera space by applying the inverse of the camera's position and orientation with View Orient Matrix and View Translate Matrix. This aligns the scene to the camera's perspective.
2. **Projection Transformation**: Project objects from 3D space into the screen’s 2D space with projection matrix.
3. **Perspective Divide**: Divide each vertex in P with $U_w$ (where $U_w=C_z$), to make unhomogenized screen space homogenized. The transformation divides by the Z-coordinate, creating the depth illusion (perspective divide).
4. **Viewport Transformation**: Maps NDC (Normalized Device Coordinates) coordinates to the pixel grid on the screen. The NDC space (which ranges from -1 to 1) is scaled and translated to fit the screen resolution.
