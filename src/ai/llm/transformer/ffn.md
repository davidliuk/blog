# FeedForward

## MOE

MOE特点

- 相同计算代价下，可以增大网络参数规模，性能更好。
- 基本可以达到相同参数规模的稠密网络性能。
- 相比同等参数规模的稠密网络，计算代价变小。
- 相比同等参数规模的稠密网络，显存占用不变。
- 可能有专家负载不均衡问题，训练难度增大。

### 专家负载均衡

- 训练时对每个tokent最少选择2个专家。选择Top1专家和在剩余专家里按概率再选择一个。
- 给每个专家设置tokn容量，达到容量后，则跳过处理，输出为全0。通过残差连接后边。
- 设置一个负载均衡的辅助损失。

### 负载均衡衡损失

- 希望每个专家被调用的频率是相等的。
- $f_i=(该专家被调用的次数)/(所有专家被调用的次数)$
- $loss_{balance}=\sum\limits^N_{i=1}(f_i)^2$
- 假设有2个专家：
  - f1=1;f2=0;losspalance =12+02=1
  - f1=0.8;f2=0.2;l0 SSpalance=0.82+0.22=0.68
  - f1=0.5;f2=0.5;l0 SSpalance=0.52+0.52=0.5

#### fi

- $f_i=(该专家被调用的次数)/(所有专家被调用的次数)$
- $loss_{balance}=\sum\limits^N_{i=1}(f_i)^2$

是否调用某个专家是通过torch.topk操作得到的，这个操作不可微，无法通过梯度下降优化。

#### 近似

$loss_{balance}=\sum\limits^N_{i=1}f_ip_i$

- $f_i=(该专家被调用的次数)/(所有专家被调用的次数)$
- $p_i=$ 一个批次中所有token对该专家的路由概率的平均值。
  - 这个是softmax得到的，可微，f作为常数，对pi优化

## DeepSeekMoE

- 细分更多专家
- 抽取共享专家

效果已经达到MoE极限：与Dense网络loss一致