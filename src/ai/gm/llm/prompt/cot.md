# 思维链

CoT

ToT

GPT-o1

人类的思维

在心理学中，System-1任务和System-2任务分别代表两种不同的思维方式所处理的任务类型。

- System-l任务：
  - Intuition instinct
  - System-l的思考过程是快速、自动且无意识的。主要依靠直觉和经验进行判断，不需要你刻意去思考，往往在瞬间完成。
- System-2任务：
  - Rational thinking
  - System-2的思考过程比较缓慢，需要集中注意力和耗费精力，运用逻辑分析、计算和有意识的思考来解决问题。

System1与System2的差异

随着大模型参数量、算力开销、数据量协同增长，在标准提示下，其在System-l任务上性能显著增强。然而在System-2任务上，大模型表现出了“Flat Scaling Curves"现象一即模型规模增长未带来预期性能提升。

- 面对System-1问题，如常识问答、情感分类、意图识别等，随规模变大，大模型性能显著提升。
- 面对System-2问题，如复杂数学计算、逻辑推理等，大模型性能提升缓慢甚至停滞不前。

### 思维链的定义

思维链(Chain-of-Thought, CoT)通过在提示中嵌入一系列中间推理步骤，引导大语言模型模拟人类解决问题时的思考过程，以提升模型处理System2任务的能力。

### 思维链的分类

在标准的CoT方法上，出现了许多扩展的方法，这些扩展的方法按照其推理方式的不同，可以归纳为三种模式：按部就班、三思后行和集思广益。

## 按部就班模式

按部就班模式强调的是逻辑的连贯性和步骤的顺序性。在这种模式下，模型像是在遵循一条预设的路径，每一步都紧密依赖于前一步的结论。

- CoT：通过手工构造一步一步推理回答问题的例子，作为示例放入Prompt，来引导模型一步一步生成推理步骤。 
- Zero-Shot CoT：通过特定提示，自动化构造一步一步推理回答问题的例子。
-  Auto CoT：从问题库中检索多个相关的问题，并利用 Zero-Shot CoT针对每个问题自动化构造推理示例。

### Zero-Shot CoT

Zero-Shot CoT通过简单的提示，如“Let's think step by step”，引导模型自行生成条推理链。其无需手工标注的CoT示例，减少了对人工示例的依赖，多个推理任务上展现出了与原始少样本CoT相媲美甚至更优的性能。

CoT对比Zero-Shot CoT

标准的CoT方法在推理任务上性能优越，但是需要手工编写推理链示例。Zero- Shot CoT无需人工编写少样本样例，但是效果不如标准的CoT方法。

### Auto CoT

在Zero-Shot CoT的基础之上，Auto-CoT引入与待解决问题相关的问题及其推理链作为示例，以继续提升CoT的效果。Auto-CoT无需人工标注成本，但是性能超过了需要手工标注的CoT和无需手工标注的Zero-Shot CoT。

- 利用Sentence-Bert对问题库样本进行表征，并使用K-Means算法筛选出与用户提问提问相关的问题样本。
- 在筛选出的问题样本上，利用Zero-Shot CoT生成思维链内容，作为示例。
- 以这些示例作为少样本示例，引导大语言模型生成针对用户问题的推理链和答案。

### 按部就班模式的不足

人类在解决System-2类问题时，会有一个反复选择以及回溯的过程。已有的CoT提示方法无法模拟这种过程，从而导致其能力受限。

System-2任务决策过程 

- System-2任务决策过程中会维护并探索当前选择的各种不同替代方案，而不仅仅挑选其中一个。
-  System-2任务决策过程中会评估自身当前状态，并展望未来或回溯以做出更具全局性的决策。

已有CoT提示的问题

- 在局部上，它们在思维过程中不会探索不同的后续内容一即树的分支。
- 在全局上，大语言模型顺序链式输出，不存在规划、前瞻性思考、自我评估和回溯

## 三思后行模式

为了解决按部就班模式的不足，三思后行模式在决策过程中融入审慎和灵活性。这种模式下，模型在每一步会停下来评估当前的情况，判断是否需要调整方向。

- Tree of Thoughts：将推理过程视为一棵思维树，从拆解、衍生、评估、搜索四个方面进行构造。 
- Graph of Thoughts：通过特定提示，自动化构造一步一步推理回答问题的例子。

### Tree of Thoughts

为了模拟人类在做System-2任务时审时度势的过程，Tree-of-Thought (ToT) 将推理过程构造为一棵思维树，在构造树过程中，允许模型在遇到困难或不确定性时进行回溯和重新选择。

ToT从拆解、衍生、评估、搜索四个角度构造思维树。

1. 拆解：将复杂问题拆分成多个简单子问题，每个子问题的解答过程对应一个思维过程。
2. 衍生：模型需要根据当前子问题生成可能的下一步推理方向。衍生有两种模式：样本启发和命令提示。
3. 评估：利用模型评估推理节点合理性。根据任务是否便于量化评分，选择投票或打分模式。
4. 搜索：从一个或多个当前状态出发，搜索通往问题解决方案的路径。

## 集思广益模式

集思广益模式强调的是通过汇集多种不同的观点和方法来优化决策过程。在这种模式下，模型不仅仅依赖于单一的推理路径，而是通过探索多种可能的解决方案从中选择最优的答案。

- Self-Consistency：引入多样性的推理路径，从中提取并选择最一致的答案，从而提高了模型的推理准确性。
- Universal Self-Consistency：利用LLMs自身选择最一致答案，支持更多种任务，无需答案提取过程。在多任务中性能良好且输出匹配度高、对顺序鲁棒。

### Self-Consistency

Self-Consistency引入多样性的推理路径并从中选择最一致的答案，从而提高了模型的推理准确性。Self.Consistency不依赖于特定的CoT形式，可以与其他CoT方法兼容，共同作用于模型的推理过程。 

Self-Consistency的设计源于以下的直觉：

- 复杂的推理任务通常允许多种能够得出正确答案的推理路径。
- 对于一个问题，所需的深思熟虑和分析越多，能够得出答案的推理路径的多样性就越大。

Self-Consistency的过程分为三个步骤：

1. 推理路径生成：在随机采样策略下，使用CoT或Zero-Shot CoT的方式来引导大语言模型针对待解决问题生成一组多样化的推理路径；
2. 汇总答案：针对大语言模型生成的每个推理内容，收集其最终的答案，并统计每个答案在所有推理路径中出现的频率：
3. 选择答案：选择出现频率最高的答案作为最终的、最一致的答案。

### Universal Self-Consistency 

Self-Consistency依赖于答案提取过程来聚合多个解决方案，不适用于自由形式的答案，例如文本摘要等任务。Universal Self-Consistency利用大语言模型本身来选择最一致答案，显著拓宽了Self-Consistency的使用场景。



GPT-o1

三种思维链模式都是作用于模型推理侧，OpenAI尝试在训练和推理时深度融合思维链技术，并提出了GPT-ol。它包括ol、ol-preview和ol-mini三个版本，在回答问题前会花费更多时间来思考，擅长处理科学、编程、数学等领域的复杂问题。

GPT-o1对比GPT-4o 

GPT-o1在通用能力上与GPT-4o持平，推理能力和解决复杂问题方面更胜一筹。值得注意的是，GPT-o1mini推理能力也显著高于GPT-4o。GPT-ol的推理运行开销显著高于GPT-4o。

GPT-o1引领新Scaling Law 

OpenAI团队利用大规模强化学习算法教会模型如何利用其思维链进行高效思考。并发现o1的性能随着更多的强化学习（训练时的计算）和更多的思考时间（测试时的计算)而不断提高。

训练时的大规模强化学习

谷歌提出的SCoRe!)设计两阶段学习方法，训练语言模型学会自我反思与纠正的方法，通过增大训练时计算，提升模型的思维片段自我纠正能力以及长推理能力

1. 阶段：针对模型第一次差强人意的回答，进行微调，使其第二次能回答得更好。
2. 阶段：设计面向自我纠正的多轮强化学习策略优化模型，进一步增强模型能力。

测试时的大规模搜索采祥

谷歌另一个工作又提出在模型测试时，利用奖励模型指导大规模搜索，探索多种思维片段，从而增大测试时计算，显著增强模型复杂推理问题的能力。
