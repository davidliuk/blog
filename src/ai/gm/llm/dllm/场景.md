双向注意力机制，可以看到上下文，做修改是非常合适的场景

加速

- Cache
- Sampling

> 损失少量经典，提高大量速度

ARM

- 量化，也是精度损失

长序列

采样策略

- 自回归

- 半自回归（Block Diffusion）

LLaDA



训练过程中很难scale长度



问题：scaling