# Diffusion Large Language Model

ç›¸æ¯”äºŽLLM AR(Auto-Regression)

[Gemini Diffusion](https://deepmind.google/models/gemini-diffusion/#what-is-a-diffusion-model)

[LLaDA](https://ml-gsai.github.io/LLaDA-demo/)

LLaDA-V

[MMaDA](https://github.com/Gen-Verse/MMaDA), Princeton, SEED

[Fast-dllm](http://arxiv.org/abs/2505.22618), NVIDIAï¼šå¼•å…¥KV-Cache

é€Ÿåº¦å¾ˆå¿«ï¼Œä½†æ˜¯æ•ˆæžœä¸€èˆ¬

Traditional autoregressive language models generate text one word â€“ or token â€“ at a time. This sequential process can be slow, and limit the quality and coherence of the output.

Diffusion models work differently. Instead of predicting text directly, they learn to generate outputs by refining noise, step-by-step. This means they can iterate on a solution very quickly and error correct during the generation process. This helps them excel at tasks like editing, including in the context of math and code.



ç¼ºç‚¹

- æ—  Cache
- å®šé•¿

å˜é•¿æ€è·¯ï¼šBlock Diffusion



---



- 

```python
cat > simple_dataset_download.py << 'EOF'
import os
import subprocess
import time

# è®¾ç½®çŽ¯å¢ƒ
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HOME'] = '/root/autodl-tmp/cache/'
os.environ['CUDA_VISIBLE_DEVICES'] = ''  # æ— GPUæ¨¡å¼

def trigger_dataset_download(task_name):
    """è§¦å‘æ•°æ®é›†ä¸‹è½½"""
    print(f"\n=== è§¦å‘ {task_name} æ•°æ®é›†ä¸‹è½½ ===")
    
    cmd = [
        'python', '-m', 'lmms_eval',
        '--tasks', task_name,
        '--limit', '1',
        '--verbosity', 'DEBUG'
    ]
    
    try:
        # è¿è¡Œå‘½ä»¤ä½†é¢„æœŸä¼šå¤±è´¥ï¼ˆå› ä¸ºæ²¡æœ‰æ¨¡åž‹ï¼‰ï¼Œä½†ä¼šè§¦å‘æ•°æ®ä¸‹è½½
        result = subprocess.run(
            cmd,
            cwd='./eval',
            timeout=180,  # 3åˆ†é’Ÿè¶…æ—¶
            capture_output=True,
            text=True
        )
        
        # æ£€æŸ¥è¾“å‡ºä¸­æ˜¯å¦æœ‰ä¸‹è½½ä¿¡æ¯
        if any(keyword in result.stdout.lower() + result.stderr.lower() 
               for keyword in ['downloading', 'download', 'fetching']):
            print(f"âœ“ {task_name} è§¦å‘äº†æ•°æ®ä¸‹è½½")
        else:
            print(f"âš  {task_name} å¯èƒ½å·²ç¼“å­˜æˆ–ä¸‹è½½å¤±è´¥")
            
    except subprocess.TimeoutExpired:
        print(f"âš  {task_name} è¶…æ—¶ï¼Œä½†å¯èƒ½å·²å¼€å§‹ä¸‹è½½")
    except Exception as e:
        print(f"âŒ {task_name} å‡ºé”™: {e}")

def main():
    tasks = [
        "mme", "vqav2_val_lite", "mmbench_en_dev_lite", "textvqa_val",
        "docvqa_val", "chartqa_lite", "infovqa_val_lite", "scienceqa_full",
        "ai2d", "coco2017_cap_val_lite", "mathverse_testmini_vision_dominant",
        "mathvista_testmini_format"
    ]
    
    print("ðŸš€ å¼€å§‹è§¦å‘æ•°æ®é›†ä¸‹è½½...")
    
    for i, task in enumerate(tasks):
        print(f"\nè¿›åº¦: {i+1}/{len(tasks)}")
        trigger_dataset_download(task)
        time.sleep(10)  # ç­‰å¾…10ç§’å†ä¸‹è½½ä¸‹ä¸€ä¸ª
    
    print("\nâœ… æ‰€æœ‰æ•°æ®é›†ä¸‹è½½è§¦å‘å®Œæˆ!")
    print("è¯·æ£€æŸ¥ /root/autodl-tmp/cache/ ç›®å½•æŸ¥çœ‹ä¸‹è½½çš„æ•°æ®")

if __name__ == "__main__":
    main()
EOF
```





```
cd ~/diffusion_lm/LaViDa

cat > download_eval_datasets.py << 'EOF'
import os
import sys
import subprocess
from pathlib import Path

# è®¾ç½®çŽ¯å¢ƒå˜é‡
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HOME'] = '/root/autodl-tmp/cache/'

# æ·»åŠ evalè·¯å¾„
sys.path.append('./eval')

def setup_environment():
    """è®¾ç½®è¯„æµ‹çŽ¯å¢ƒ"""
    print("=== è®¾ç½®è¯„æµ‹çŽ¯å¢ƒ ===")
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs('/root/autodl-tmp/cache/datasets', exist_ok=True)
    os.makedirs('./logs', exist_ok=True)
    
    print("âœ“ çŽ¯å¢ƒè®¾ç½®å®Œæˆ")

def download_dataset_with_lmms_eval(task_name):
    """ä½¿ç”¨lmms_evalä¸‹è½½å•ä¸ªæ•°æ®é›†"""
    print(f"\n=== ä¸‹è½½æ•°æ®é›†: {task_name} ===")
    
    try:
        # ä½¿ç”¨lmms_evalçš„--download_onlyæ¨¡å¼ï¼ˆå¦‚æžœæ”¯æŒï¼‰
        cmd = [
            'python', '-m', 'lmms_eval',
            '--model', 'llava_llada',
            '--model_args', 'pretrained=dummy',  # è™šæ‹Ÿæ¨¡åž‹è·¯å¾„
            '--tasks', task_name,
            '--limit', '1',  # åªå¤„ç†1ä¸ªæ ·æœ¬æ¥è§¦å‘æ•°æ®é›†ä¸‹è½½
            '--verbosity', 'INFO'
        ]
        
        # è®¾ç½®çŽ¯å¢ƒå˜é‡
        env = os.environ.copy()
        env['CUDA_VISIBLE_DEVICES'] = ''  # æ— GPUæ¨¡å¼
        
        result = subprocess.run(
            cmd, 
            capture_output=True, 
            text=True, 
            env=env,
            cwd='./eval',
            timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
        )
        
        if result.returncode == 0 or "Downloading" in result.stdout:
            print(f"âœ“ {task_name} æ•°æ®é›†ä¸‹è½½æˆåŠŸ")
            return True
        else:
            print(f"âš  {task_name} ä¸‹è½½å¯èƒ½æœ‰é—®é¢˜ï¼Œä½†ç»§ç»­...")
            print(f"è¾“å‡º: {result.stdout[-200:]}")  # æ˜¾ç¤ºæœ€åŽ200å­—ç¬¦
            return False
            
    except subprocess.TimeoutExpired:
        print(f"âš  {task_name} ä¸‹è½½è¶…æ—¶ï¼Œä½†æ•°æ®å¯èƒ½å·²éƒ¨åˆ†ä¸‹è½½")
        return False
    except Exception as e:
        print(f"âŒ {task_name} ä¸‹è½½å¤±è´¥: {e}")
        return False

def download_datasets_manually():
    """æ‰‹åŠ¨ä¸‹è½½å¸¸è§æ•°æ®é›†"""
    print("\n=== æ‰‹åŠ¨ä¸‹è½½å¸¸è§æ•°æ®é›† ===")
    
    datasets_to_download = [
        'coco/annotations_train2017.json',
        'coco/annotations_val2017.json',
    ]
    
    from huggingface_hub import hf_hub_download
    
    for dataset in datasets_to_download:
        try:
            print(f"ä¸‹è½½ {dataset}...")
            # è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„æ•°æ®é›†ä¸‹è½½é€»è¾‘
        except Exception as e:
            print(f"âš  {dataset} ä¸‹è½½å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ðŸš€ å¼€å§‹é¢„ä¸‹è½½è¯„æµ‹æ•°æ®é›†...")
    
    # ä»»åŠ¡åˆ—è¡¨ï¼ˆä»Žrun.shæå–ï¼‰
    tasks = [
        "mme",
        "vqav2_val_lite", 
        "mmbench_en_dev_lite",
        "textvqa_val",
        "docvqa_val", 
        "chartqa_lite",
        "infovqa_val_lite",
        "scienceqa_full",
        "ai2d",
        "coco2017_cap_val_lite",
        "mathverse_testmini_vision_dominant",
        "mathvista_testmini_format"
    ]
    
    setup_environment()
    
    # ä¸‹è½½æ¯ä¸ªæ•°æ®é›†
    successful_downloads = 0
    for i, task in enumerate(tasks):
        print(f"\nè¿›åº¦: {i+1}/{len(tasks)}")
        if download_dataset_with_lmms_eval(task):
            successful_downloads += 1
    
    print(f"\n=== ä¸‹è½½å®Œæˆ ===")
    print(f"æˆåŠŸä¸‹è½½: {successful_downloads}/{len(tasks)} ä¸ªæ•°æ®é›†")
    
    # æ£€æŸ¥ä¸‹è½½çš„æ•°æ®
    check_downloaded_data()

def check_downloaded_data():
    """æ£€æŸ¥å·²ä¸‹è½½çš„æ•°æ®"""
    print("\n=== æ£€æŸ¥å·²ä¸‹è½½çš„æ•°æ® ===")
    
    cache_dirs = [
        '/root/autodl-tmp/cache/',
        '~/.cache/huggingface/',
        './eval/data/',
        './data/'
    ]
    
    for cache_dir in cache_dirs:
        expanded_dir = os.path.expanduser(cache_dir)
        if os.path.exists(expanded_dir):
            size = subprocess.run(['du', '-sh', expanded_dir], 
                                 capture_output=True, text=True)
            print(f"{expanded_dir}: {size.stdout.strip()}")

if __name__ == "__main__":
    main()
EOF
```

