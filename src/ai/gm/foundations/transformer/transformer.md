# Transformer

- Multi-Head Attention
- Self-Attention
- LayerNorm+Residual
- Position Encoding
- FFN (Feed-Forward Network)

| ç»„ä»¶                 | ä½œç”¨                                        |
| -------------------- | ------------------------------------------- |
| Multi-Head Attention | å¹¶è¡Œå¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œæ•æ‰ä¸åŒè¯­ä¹‰å…³ç³»          |
| Self-Attention       | å¥å­å†…éƒ¨ token ä¹‹é—´äº’ç›¸å…³æ³¨                 |
| LayerNorm + Residual | åŠ å¿«æ”¶æ•›ã€ç¨³å®šè®­ç»ƒ                          |
| Position Encoding    | æä¾› token çš„ä½ç½®ä¿¡æ¯ï¼ˆå› ä¸ºæ³¨æ„åŠ›æ˜¯æ— åºçš„ï¼‰ |
| Feed-Forward Network | æ¯ä¸ªä½ç½®ç‹¬ç«‹çš„éçº¿æ€§å˜æ¢                    |

### ğŸ”§ Decoder-only çš„ç‰¹ç‚¹ï¼š

| ç‰¹æ€§                           | åŸå›                                |
| ------------------------------ | ---------------------------------- |
| **ä»…ç”¨ Masked Self-Attention** | é˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯ï¼ˆåªçœ‹ $x_{ï¼‰      |
| **æ—  Encoder è¾“å…¥**            | è¾“å…¥å°±æ˜¯è®­ç»ƒæ•°æ®æœ¬èº«ï¼Œæ— éœ€å¤–éƒ¨åºåˆ— |
| **å¯é€è¯ç”Ÿæˆ**                 | éå¸¸é€‚åˆ open-ended generation     |
