resnet

残差连接

激活函数（Activation Function）

常见的激活函数都是非线性的，因此也会给神经元引入非线性元素，使得神经网络可以逼近其他的任何非线性函数，这样可以使得神经网络应用到更多非线性模型中。

- **线性激活函数**（线性方程控制输入到输出的映射，如f(x)=x等）
- **非线性激活函数**（非线性方程控制输入到输出的映射，比如
  - Sigmoid、
  - Tanh、
  - ReLU、
    - LReLU
    - PReLU
    - Swish
  - Softmax 是用于多类分类问题的激活函数

attention

