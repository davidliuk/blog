# Artificial Intelligence

[papers with code](https://paperswithcode.com/)

æ·±åº¦å­¦ä¹  $=$ ç¥ç»ç½‘ç»œ $\in$ æœºå™¨å­¦ä¹  $\in$ äººå·¥æ™ºèƒ½

## é¡¶ä¼š

- ICLR
- NeurIPS
- CVPR
- ICCV
- ACL
- ECCV

## 1. æ€»ä½“åˆ†ç±»

### 1. åŸºç¡€å±‚ï¼ˆç®—æ³•ä¸æ¨¡å‹ï¼‰

> ç ”ç©¶å¦‚ä½•**è¡¨ç¤ºã€å­¦ä¹ å’Œæ¨ç†**æ™ºèƒ½ã€‚

#### ğŸ’¡ 1.1 æœºå™¨å­¦ä¹ ï¼ˆMachine Learningï¼‰

- ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰
- æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰
- åŠç›‘ç£å­¦ä¹ ï¼ˆSemi-supervised Learningï¼‰
- è‡ªç›‘ç£å­¦ä¹ ï¼ˆSelf-supervised Learningï¼‰

  - å¯¹æ¯”å­¦ä¹ 

- å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning, RLï¼‰
- å…ƒå­¦ä¹ ï¼ˆMeta-learningï¼‰
- è”é‚¦å­¦ä¹ ï¼ˆFederated Learningï¼‰

#### ğŸ’¡ 1.2 æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰

- å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰
- å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNN, LSTM, GRUï¼‰
- å›¾ç¥ç»ç½‘ç»œï¼ˆGraph Neural Network, GNNï¼‰
- Transformerã€æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttention Mechanismï¼‰
- å¤šæ¨¡æ€å­¦ä¹ ï¼ˆMultimodal Learningï¼‰

#### ğŸ’¡ 1.3 ç”Ÿæˆæ¨¡å‹ï¼ˆGenerative Modelsï¼‰

- ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰
- å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰
- è‡ªå›å½’æ¨¡å‹ï¼ˆAutoregressive Modelsï¼‰
- Diffusion Modelsï¼ˆæ‰©æ•£æ¨¡å‹ï¼‰
- NeRFã€Neural Fields

#### ğŸ’¡ 1.4 æ¦‚ç‡ä¸å†³ç­–ç†è®º

- è´å¶æ–¯æ–¹æ³•ï¼ˆBayesian Methodsï¼‰
- é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰
- å¤šè‡‚è€è™æœºã€å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ

### 2. æ„ŸçŸ¥å±‚ï¼ˆç†è§£ä¸–ç•Œï¼‰

#### ğŸ‘ï¸ 2.1 è®¡ç®—æœºè§†è§‰ï¼ˆComputer Visionï¼‰

- å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ã€å®ä¾‹åˆ†å‰²
- å§¿æ€ä¼°è®¡ã€ä¸‰ç»´é‡å»ºï¼ˆSLAMã€NeRFï¼‰
- è§†é¢‘ç†è§£ã€å›¾åƒç”Ÿæˆ
- å¤šè§†å›¾å‡ ä½•ä¸ä¸‰ç»´æ„ŸçŸ¥ï¼ˆ3D Visionï¼‰

#### ğŸ—£ï¸ 2.2 è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNatural Language Processingï¼‰

- è¯­è¨€æ¨¡å‹ï¼ˆGPTã€BERTã€T5ï¼‰
- æ–‡æœ¬ç”Ÿæˆä¸ç†è§£
- æƒ…æ„Ÿåˆ†æã€é—®ç­”ç³»ç»Ÿã€å¯¹è¯ç³»ç»Ÿ
- ä¿¡æ¯æŠ½å–ä¸çŸ¥è¯†å›¾è°±

#### ğŸ”Š 2.3 è¯­éŸ³ä¸éŸ³é¢‘ï¼ˆSpeech & Audioï¼‰

- è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰
- æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰
- å£°çº¹è¯†åˆ«ã€å£°éŸ³ç”Ÿæˆã€å£°éŸ³äº‹ä»¶æ£€æµ‹

### 3. å†³ç­–å±‚ï¼ˆåšå‡ºè¡ŒåŠ¨ï¼‰

#### ğŸ§  3.1 å¼ºåŒ–å­¦ä¹ ä¸æ§åˆ¶ï¼ˆRL & Planningï¼‰

- æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDQN, PPO, A3Cï¼‰
- å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMARLï¼‰
- æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰
- æœºå™¨äººå¯¼èˆªä¸è·¯å¾„è§„åˆ’

#### ğŸ•¹ï¸ 3.2 äººå·¥æ™ºèƒ½æ¸¸æˆï¼ˆGame AIï¼‰

- AlphaGoã€AlphaZeroã€MuZero
- æ¸¸æˆä¸­çš„ç­–ç•¥å­¦ä¹ ã€å¤šæ™ºèƒ½ä½“åšå¼ˆ

## ğŸ”¹ äºŒã€äº¤å‰ä¸æ–°å…´æ–¹å‘

### ğŸ¤ å¤šæ¨¡æ€æ™ºèƒ½ï¼ˆMultimodal AIï¼‰

> å›¾æ–‡ã€è§†å¬ã€è§†é¢‘+æ–‡æœ¬çš„è”åˆå­¦ä¹ ï¼ˆå¦‚ CLIPã€GPT-4Vã€SAMï¼‰

### ğŸ§  å¤§æ¨¡å‹ä¸é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆLLM & AGIï¼‰

> GPTã€Geminiã€Claude ç­‰ï¼Œç ”ç©¶æ¨¡å‹å¯¹é½ã€æç¤ºå·¥ç¨‹ã€èƒ½åŠ›è¿ç§»ã€‚

### ğŸ§© è§£é‡Šæ€§ä¸å¯è§£é‡Š AIï¼ˆXAIï¼‰

> ä¸ºä»€ä¹ˆæ¨¡å‹åšå‡ºæŸä¸ªå†³ç­–ï¼Œå¦‚ä½•å¢å¼ºé€æ˜åº¦ä¸ä¿¡ä»»ã€‚

### ğŸ“š çŸ¥è¯†å›¾è°±ä¸å› æœæ¨ç†

> ç»“æ„åŒ–çŸ¥è¯†ç®¡ç†ã€å› æœå…³ç³»å»ºæ¨¡ã€ç§‘å­¦å‘ç°ã€‚

### ğŸŒ è”é‚¦å­¦ä¹ ä¸éšç§ä¿æŠ¤ï¼ˆFL & Privacy-preserving MLï¼‰

> åœ¨ä¿è¯æ•°æ®éšç§çš„å‰æä¸‹è®­ç»ƒæ¨¡å‹ã€‚

### ğŸ‘¥ äººå·¥æ™ºèƒ½ä¼¦ç†ä¸ç¤¾ä¼šå½±å“ï¼ˆAI Ethics & Safetyï¼‰

> å¯¹é½é—®é¢˜ã€åè§æ£€æµ‹ã€äººå·¥æ™ºèƒ½æ²»ç†ã€AIGC é£é™©ã€‚

## 3. åº”ç”¨å±‚åˆ†ç±»

- åŒ»ç–— AIï¼ˆMedical AIï¼‰
- é‡‘è AIï¼ˆé‡‘èé£æ§ã€æ™ºèƒ½æŠ•é¡¾ï¼‰
- æ•™è‚² AIï¼ˆæ™ºèƒ½è¾…å¯¼ã€çŸ¥è¯†è¿½è¸ªï¼‰
- æ— äººé©¾é©¶ï¼ˆAutonomous Drivingï¼‰
- äººæœºäº¤äº’ï¼ˆHuman-AI Interactionï¼‰
- å…·èº«æ™ºèƒ½/æ™ºèƒ½æœºå™¨äººï¼ˆEmbodied AIï¼‰
- AIGCï¼ˆAI Generated Contentï¼‰
- æ™ºèƒ½åˆ¶é€ ã€å·¥ä¸š 4.0

## å…¶ä»–åˆ†ç±»

**åŸºäºå­¦ä¹ ç­–ç•¥çš„åˆ†ç±»**:

- ç›‘ç£å­¦ä¹  Supervised Learning

  1ï¼‰çº¿æ€§å›å½’ï¼ˆLinear Regressionï¼‰ï¼šçº¿æ€§å›å½’ç”¨äºå»ºç«‹è¾“å…¥ç‰¹å¾ä¸è¿ç»­æ•°å€¼ç›®æ ‡ä¹‹é—´çš„çº¿æ€§å…³ç³»æ¨¡å‹ã€‚å®ƒé€šè¿‡æ‹Ÿåˆä¸€æ¡ç›´çº¿æˆ–è¶…å¹³é¢æ¥è¿›è¡Œé¢„æµ‹ã€‚

  2ï¼‰é€»è¾‘å›å½’ï¼ˆLogistic Regressionï¼‰ï¼šé€»è¾‘å›å½’é€‚ç”¨äºåˆ†ç±»é—®é¢˜ï¼Œå…¶ä¸­ç›®æ ‡å˜é‡æ˜¯ç¦»æ•£çš„ã€‚å®ƒä½¿ç”¨é€»è¾‘å‡½æ•°ï¼ˆå¦‚ sigmoid å‡½æ•°ï¼‰æ¥å»ºç«‹è¾“å…¥ç‰¹å¾ä¸ç›®æ ‡ç±»åˆ«ä¹‹é—´çš„å…³ç³»æ¨¡å‹ã€‚

  3ï¼‰å†³ç­–æ ‘ï¼ˆDecision Treesï¼‰ï¼šå†³ç­–æ ‘é€šè¿‡æ„å»ºä¸€ç³»åˆ—å†³ç­–è§„åˆ™æ¥è¿›è¡Œåˆ†ç±»æˆ–å›å½’ã€‚å®ƒæ ¹æ®ç‰¹å¾çš„ä¸åŒåˆ†å‰²æ•°æ®ï¼Œå¹¶æ„å»ºä¸€ä¸ªæ ‘çŠ¶ç»“æ„æ¥è¿›è¡Œé¢„æµ‹ã€‚

  4ï¼‰æ”¯æŒå‘é‡æœºï¼ˆSupport Vector Machinesï¼ŒSVMï¼‰ï¼šSVM æ˜¯ä¸€ç§ç”¨äºåˆ†ç±»å’Œå›å½’çš„ç›‘ç£å­¦ä¹ ç®—æ³•ã€‚å®ƒé€šè¿‡å¯»æ‰¾ä¸€ä¸ªæœ€ä¼˜çš„è¶…å¹³é¢æˆ–è€…éçº¿æ€§å˜æ¢ï¼Œå°†ä¸åŒç±»åˆ«çš„æ•°æ®æ ·æœ¬åˆ†éš”å¼€ã€‚

  5ï¼‰éšæœºæ£®æ—ï¼ˆRandom Forestï¼‰ï¼šéšæœºæ£®æ—æ˜¯ä¸€ç§é›†æˆå­¦ä¹ ç®—æ³•ï¼Œå®ƒç»“åˆäº†å¤šä¸ªå†³ç­–æ ‘è¿›è¡Œåˆ†ç±»æˆ–å›å½’ã€‚æ¯ä¸ªå†³ç­–æ ‘åŸºäºéšæœºé€‰æ‹©çš„ç‰¹å¾å­é›†è¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡æŠ•ç¥¨æˆ–å¹³å‡æ¥è·å¾—æœ€ç»ˆé¢„æµ‹ç»“æœã€‚

  6ï¼‰ç¥ç»ç½‘ç»œï¼ˆNeural Networksï¼‰ï¼šåœ¨ç›‘ç£å­¦ä¹ ä¸­ï¼Œç¥ç»ç½‘ç»œæ¥æ”¶ä¸€ç»„è¾“å…¥æ•°æ®ï¼Œå¹¶å°†å…¶ä¼ é€’åˆ°ç½‘ç»œä¸­çš„å¤šä¸ªç¥ç»å…ƒå±‚ä¸­è¿›è¡Œå¤„ç†ã€‚æ¯ä¸ªç¥ç»å…ƒéƒ½æœ‰ä¸€ç»„æƒé‡ï¼Œç”¨äºåŠ æƒè¾“å…¥æ•°æ®ã€‚ç„¶åï¼Œè¾“å…¥æ•°æ®é€šè¿‡æ¿€æ´»å‡½æ•°è¿›è¡Œéçº¿æ€§å˜æ¢ï¼Œå¹¶ä¼ é€’åˆ°ä¸‹ä¸€å±‚ã€‚è¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¸ºå‰å‘ä¼ æ’­ã€‚åœ¨å‰å‘ä¼ æ’­åï¼Œç½‘ç»œäº§ç”Ÿä¸€ä¸ªè¾“å‡ºï¼Œä¸é¢„æœŸçš„ç›®æ ‡è¾“å‡ºè¿›è¡Œæ¯”è¾ƒã€‚ç„¶åï¼Œé€šè¿‡ä½¿ç”¨æŸå¤±å‡½æ•°æ¥åº¦é‡é¢„æµ‹è¾“å‡ºä¸ç›®æ ‡è¾“å‡ºä¹‹é—´çš„å·®å¼‚ã€‚æŸå¤±å‡½æ•°çš„ç›®æ ‡æ˜¯æœ€å°åŒ–é¢„æµ‹è¾“å‡ºä¸ç›®æ ‡è¾“å‡ºä¹‹é—´çš„è¯¯å·®ã€‚æ¥ä¸‹æ¥ï¼Œç½‘ç»œä½¿ç”¨åå‘ä¼ æ’­ç®—æ³•æ¥æ›´æ–°æƒé‡ï¼Œä»¥å‡å°æŸå¤±å‡½æ•°ã€‚åå‘ä¼ æ’­é€šè¿‡è®¡ç®—æŸå¤±å‡½æ•°ç›¸å¯¹äºæ¯ä¸ªæƒé‡çš„æ¢¯åº¦ï¼Œç„¶åæ²¿ç€æ¢¯åº¦çš„æ–¹å‘æ›´æ–°æƒé‡ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸æ–­è¿­ä»£ï¼Œç›´åˆ°ç½‘ç»œçš„æ€§èƒ½è¾¾åˆ°æ»¡æ„çš„ç¨‹åº¦ã€‚

- æ— ç›‘ç£å­¦ä¹  Unsupervised Learning

  1ï¼‰K å‡å€¼èšç±»ï¼ˆK-means Clusteringï¼‰ï¼šK å‡å€¼èšç±»æ˜¯ä¸€ç§å¸¸è§çš„èšç±»ç®—æ³•ï¼Œç”¨äºå°†æ•°æ®ç‚¹åˆ’åˆ†ä¸ºé¢„å…ˆå®šä¹‰çš„ K ä¸ªç°‡ã€‚ç®—æ³•é€šè¿‡è¿­ä»£åœ°å°†æ•°æ®ç‚¹åˆ†é…åˆ°æœ€è¿‘çš„è´¨å¿ƒï¼Œå¹¶æ›´æ–°è´¨å¿ƒä½ç½®æ¥ä¼˜åŒ–èšç±»ç»“æœã€‚K å‡å€¼èšç±»é€‚ç”¨äºå‘ç°æ•°æ®ä¸­çš„ç´§å¯†èšé›†æ¨¡å¼ã€‚

  2ï¼‰å±‚æ¬¡èšç±»ï¼ˆHierarchical Clusteringï¼‰ï¼šå±‚æ¬¡èšç±»æ˜¯ä¸€ç§å°†æ•°æ®ç‚¹ç»„ç»‡æˆæ ‘çŠ¶ç»“æ„çš„èšç±»æ–¹æ³•ã€‚å®ƒå¯ä»¥åŸºäºæ•°æ®ç‚¹ä¹‹é—´çš„ç›¸ä¼¼æ€§é€æ­¥åˆå¹¶æˆ–åˆ†å‰²èšç±»ç°‡ã€‚å±‚æ¬¡èšç±»æœ‰ä¸¤ç§ä¸»è¦æ–¹æ³•ï¼šå‡èšå±‚æ¬¡èšç±»ï¼ˆè‡ªåº•å‘ä¸Šï¼‰å’Œåˆ†è£‚å±‚æ¬¡èšç±»ï¼ˆè‡ªé¡¶å‘ä¸‹ï¼‰ã€‚å±‚æ¬¡èšç±»é€‚ç”¨äºå‘ç°ä¸åŒå±‚æ¬¡çš„èšç±»ç»“æ„ã€‚

  3ï¼‰ä¸»æˆåˆ†åˆ†æï¼ˆPrincipal Component Analysisï¼ŒPCAï¼‰ï¼šä¸»æˆåˆ†åˆ†ææ˜¯ä¸€ç§é™ç»´æŠ€æœ¯ï¼Œç”¨äºä»é«˜ç»´æ•°æ®ä¸­æå–æœ€é‡è¦çš„ç‰¹å¾ã€‚å®ƒé€šè¿‡æ‰¾åˆ°æ•°æ®ä¸­çš„ä¸»è¦æ–¹å·®æ–¹å‘ï¼Œå¹¶å°†æ•°æ®æŠ•å½±åˆ°è¿™äº›æ–¹å‘ä¸Šçš„ä½ç»´ç©ºé—´ä¸­æ¥å®ç°é™ç»´ã€‚PCA å¹¿æ³›åº”ç”¨äºæ•°æ®å¯è§†åŒ–ã€å™ªå£°è¿‡æ»¤å’Œç‰¹å¾æå–ç­‰é¢†åŸŸã€‚

  4ï¼‰å…³è”è§„åˆ™å­¦ä¹ ï¼ˆAssociation Rule Learningï¼‰ï¼šå…³è”è§„åˆ™å­¦ä¹ ç”¨äºå‘ç°æ•°æ®é›†ä¸­çš„é¡¹é›†ä¹‹é—´çš„å…³è”å…³ç³»ã€‚å®ƒé€šè¿‡è¯†åˆ«é¢‘ç¹é¡¹é›†å¹¶ç”Ÿæˆå…³è”è§„åˆ™æ¥å®ç°ã€‚å…³è”è§„åˆ™é€šå¸¸é‡‡ç”¨"If-Then"çš„å½¢å¼ï¼Œè¡¨ç¤ºæ•°æ®é¡¹ä¹‹é—´çš„å…³è”æ€§ã€‚å…³è”è§„åˆ™å­¦ä¹ å¯åº”ç”¨äºå¸‚åœºç¯®å­åˆ†æã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸã€‚

- åŠç›‘ç£å­¦ä¹ 

- å¼ºåŒ–å­¦ä¹ 
- è‡ªç›‘ç£å­¦ä¹ 
- å…ƒå­¦ä¹ 
- è”é‚¦å­¦ä¹ 

åŸºäºå½¢å¼

- æ·±åº¦å­¦ä¹ 

  DNN æ·±åº¦ç¥ç»ç½‘ç»œ $\in$ ANN äººå·¥ç¥ç»ç½‘ç»œ = DL æ·±åº¦å­¦ä¹ 

  - CNN å·ç§¯ç¥ç»ç½‘ç»œ
  - RNN å¾ªç¯ç¥ç»ç½‘ç»œ LSTM, GRU
  - æ³¨æ„åŠ›æœºåˆ¶ Transformer, Attention
  - GNN åœŸç¥ç»ç½‘ç»œ
  - å¤šæ¨¡æ€å­¦ä¹  Multimodal Learning

- ä¼ ç»Ÿæœºå™¨å­¦ä¹ 

- ç”Ÿæˆæ¨¡å‹ï¼ˆGenerative Modelsï¼‰

  - ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰
  - å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰
  - è‡ªå›å½’æ¨¡å‹ï¼ˆAutoregressive Modelsï¼‰
  - Diffusion Modelsï¼ˆæ‰©æ•£æ¨¡å‹
  - NeRFã€Neural Fields

---

ML Workflow

1. Problem formulation

2. Collect & process data
3. Train & tune models
4. Deploy models
5. Monitor

Challenges

1. Formulate problem:focus on the most impactful industrial problems(self-service supermarket, self-driving cars)
2. Data:high-quality data is scarce,privacy issues
3. Train models:models are more and more complex,data-hungry,expensive
4. Deploy models:heavy computation is not suitable for real-time inference
5. Monitor: data distributions shifts, fairness issues

---

Machine Learningâ‰ˆLooking for Function

## Different types of Functions

Regression: The function outputs a scalar.

Classification: Given options(classes),the function outputs the correct one.

Structured Learning: create something with structure (image, document)

## Framework of ML

Training data:

Testing data:

Training

1. Function with Unknown Parameters

   1. Define Loss from Training Data
      1. Loss is a function of parameters L(b,w)
      2. Loss: how good a set of values is.
         1. $L=\frac{1}{n}\sum\limits_n e_n$
            1. MAE: $e=|y-\hat y|$
            2. MSE: $e=(y-\hat y)^2$
         2. If y and are both probability distributions Cross-entropy

1. Optimization: $\theta^*=\arg\min\limits_{\theta}L$

   Gradient Descent

   1. (Randomly) Pick an initial value $\theta^0$
   2. Compute $g=\nabla L(\theta^0)=\frac{\part L}{\part w}|_{w=w^0}$
   3. $\eta\frac{\part L}{\part w}|_{w=w^0}$, $\eta$: learning rate (hyper parameters)
   4. **Update** $w$ for each batch $\theta^1=\theta^0-\eta\vec g$

   Does local minima truly cause the problem?

   - Local minima
   - global minima

**Model Bias**: Linear models have severe limitation

All Piecewise Linear Curves = constant + sum of a set of (unlinear)

- Sigmoid: $sigmoid=\frac{1}{1+e^{(b+wx)}}$

![image-20250905101009749](https://gcore.jsdelivr.net/gh/davidliuk/images@master/image-20250905101009749.png)

Activation Function

- Sigmoid
- Rectified Linear Unit (ReLU)

Neuron

Neuron Network

Deep Learning

Deep = Many hidden layers

Overfitting

General Guide

![image-20250905104528162](https://gcore.jsdelivr.net/gh/davidliuk/images@master/image-20250905104528162.png)

Fully-connected

CNN

- Less parameters, sharing parameters
- Less features
- Early stopping
- Regularization
- Dropout

Cross Validation

Training Set

- Training Set
- Validation Set
