# CNN 卷积神经网络



## 全连接网络

- 每一个细胞与下一层所有细胞都连接



输入层、隐藏层、输出层



每个神经元都有自己单独的权重向量



目标/损失函数：

$C=\frac{1}{2}||y-a^2||=\frac{1}{2}\sum_j(y_j-a_j)^2$

随机初始化每个神经元的参数，然后把样本集的每个样本的特征带进去，算出来概率和真实概率做一个差，利用随机梯度下降最小化这个差



核心问题：如何求出一个神经元的参数（的梯度）



可以解决线性不可分的问题：

可以认为相当于一个升维度的过程，所以可以解决低纬度下线性不可分的问题



为什么引入激活函数：模型假设，灵感来自于大脑的神经元，输入一个信号，对于下一个节点的输出也就两种，传递下去或者不传递（即0/1）



层数太多也有很多风险

比如过拟合、



通常是随机砍掉50%



## 反向传播

算梯度的时候，从最后一层算误差，然后把残差向前传递，根据每一个神经元的残差去求每个神经元的偏导数



公式2：



第l层的误差可以由l+1层的残差推导



设计模式：管道过滤器

图论问题：管道过滤器



## 卷积

卷积：按位运算

灰度图像0-255



卷积核：

- -1和1可以检测各种方向上的边缘，还可以 检测边缘的强度



通过边缘来认知事物的

卷积核求和，可以得到矢量和的法向量的边缘



卷积核，可以33和55，大的可以获得更复杂的形状

彩色图像：3通道，最后是堆叠起来



通道、卷积核个数

CNN：对文本的处理用的比较少



每一层都有一个激活



池化

Pooling 池化，随着网络的延伸，feature map变小一些

- Max Pooling，这个用的多，取最大值
- Mean Pooling

channel



在数学，尤其是概率论和相关领域中，归一化指数函数，或称Softmax函数，是逻辑函数的一种推广。它能将一个含任意实数的K维向量z“压缩”到另一个K维实向量σ(z)中，使得每一个元素的范围都在(0,1)之间，并且所有元素的和为1。该函数多用于多分类问题中。



多模态任务：转换成矩阵任务，如

- 文本，利用霍夫曼编码转化成数字编码
- 视频，是很多帧图像，形成一个三维的矩阵

深度可分离卷积

### LeNet



### VGG 16



### GoogLeNet

Inception Model

320,000

1\*1深度可分离卷积

- 改变通道数
- 深度可分离卷积，降低参数规模
- 替代全连接层，可以使输入的图的模型是任意大小

网络中一旦有全连接，那输入图像的大小就要固定了，这个是不太好的，这样需要把图像重采样

Transformer、多任务网络、迁移学习网络、在线学习等都是使用了GoogLeNet中这个结构的创新inception

Tensorflow中GoogleNet底层源码可以看看

GoogleNet有三个损失函数，原因

- 网络越深，会出现梯度消失的问题

inception：特征融合

### ResNet

避免梯度消失

上采样、下采样：插值

