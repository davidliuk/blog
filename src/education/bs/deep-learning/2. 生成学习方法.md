# 生成学习方法

机器学习的两个分支：

- 判别学习方法

  对条件概率$P(y|x)$进行建模，然后就可以根据学习结果对数据进行分类

- 生成学习方法

  $P(x|y), P(y)\Rightarrow P(y|x)$

  对先验概率进行建模，然后利用贝叶斯法求后验概率
  
  对条件概率建模，需要的假设更多

常见判别方法

- 分类模式



机器学习模型有一种分类方式：判别模型和生成模型。它们之间的区别在于判别模型是直接从数据特征到标签，而生成模型是从标签到数据特征。形式化的表示就是是否使用了贝叶斯公式：



## 高斯判别分析 GDA

$\underset{y}{\mathrm{\mathop{argmax}}}P(y|x)= \underset{y}{\mathrm{\mathop{argmax}}}\frac{P(x|y)P(y)}{P(x)=1}$

如果判别学习条件都符合，在这种情况下判别模型的效果好于逻辑回归模型

但是判别模型的条件相对多，鲁棒性差



逻辑回归：对分类进行一个伯努利的假设，所以泛化效果更好

高斯判别：需要两个前置条件



### 条件

伯努利分布、特征向量符合高斯分布



#### 条件一：

$y\sim Bernoulli(\phi)$

#### 条件二：

$P(x|y)\sim N$ 多变量正态分布

$(x|y=0)\sim N(\mu_0,\Sigma_0)$

$(x|y=1)\sim N(\mu_1,\Sigma_1)$

认为是高斯分布：中心极限定理，如果认为这几维度相互独立，则很可能属于高斯分布

> 乘积可以写成epsilon和



### 建模

向量域的正态分布：每一维度都符合正态分布

均值向量：$\mu\in R^n$

协方差矩阵：$\sum\in R^{n\times n}$

概率密度函数：$P(x;\mu,\Sigma)=\frac{1}{(2\pi)^\frac{n}{2}|\Sigma|^{\frac{1}{2}}}\exp(-\frac{1}{2}{(x-\mu)}^T)\Sigma^{-1}(x-\mu)$



可以写成指数分布组，属于广义线性模型

$P(y)=\phi^y(1-\phi^{(1-y)})$

$P(x|y=0)=\frac{1}{(2\pi)^\frac{n}{2}|\Sigma|^{\frac{1}{2}}}\exp(-\frac{1}{2}{(x-\mu_0)}^T)\Sigma^{-1}_0(x-\mu_0)$

$P(x|y=1)=\frac{1}{(2\pi)^\frac{n}{2}|\Sigma|^{\frac{1}{2}}}\exp(-\frac{1}{2}{(x-\mu_1)}^T)\Sigma_1^{-1}(x-\mu_1)$



极大似然函数如下：

$\max \mathcal L(\phi,\mu_0,\mu_1,\Sigma_0,\Sigma_1)=\prod\limits_{i=1}^mp(x^{(i)},y^{(i)};\phi,\mu_0,\mu_1,\Sigma_0,\Sigma_1)$

得到这个函数以后，就可以做SGD方法：

对称矩阵

最后只需要比较$P(y=1|x)$和$P(y=0|x)$比较，谁大就认为是谁





从上图可以看出，高斯判别模型通过建立两类样本的特征模型，对于二分类问题，然后通过比较后验概率的大小来得到一个分类边界。



高斯判别模型的假设是P(X|Y)服从一个高斯分布，P(Y)服从一个伯努利分布

Logistic回归的概率解释中可以看出它的假设是P(Y|X,θ)服从伯努利分布

这里我们可以发现高斯判别模型的假设强于Logistic模型，也就是说Logistic回归模型的鲁棒性更强。这就表示在数据量足够大时，跟倾向于选择Logistic回归模型。而在数据量较小，且P(X|Y)服从一个高斯分布非常合理时，选择高斯判别分析模型更适合。



这里我们可以发现高斯判别模型的假设强于Logistic模型，也就是说Logistic回归模型的鲁棒性更强。这就表示在数据量足够大时，跟倾向于选择Logistic回归模型。而在数据量较小，且$P(X|Y)$服从一个高斯分布非常合理时，选择高斯判别分析模型更适合。



煤矿人脸识别的难点：

- 网络环境差，带宽有问题，传大图片很难传出来

  想着试Zigbee（自动组网），但是效果也不好

- 



理解业务比只懂技术更有优势



Logistics、GDA解决问题：特征向量x是连续时的问题



## 朴素贝叶斯法 NBM

概率学派

最大似然法对概率问题进行求解

解决问题：特征向量x是离散时的问题

例如：

- NLP，文本分类模型，如垃圾邮件分析

  暴力方法：用字典来表达一个向量，每个单词存在与否是01，这种量很大很难分析；故采用朴素贝叶斯模型分析

- 超市商品分析，商品种类是离散空间



$P(x_1, x_2,...x_{50000}|y)=P(x_1|y_1)P(x2|y_1x_1)P(x_1)$



马尔可夫模型，是朴素贝叶斯的一种应用

常用于天气预报，马尔可夫认为只和前一个有关、与后一个无关



朴素贝叶斯假设：假设自变量完全是独立分布，自变量互相之间不影响

$\phi_{j}=P(y=1)$

$\phi_{j|y=1}=P(x_j=1|y=1)$

$\phi_{j|y=0}=P(x_j=1|y=0)$

极大似然函数如下：
$$
L(\phi_y,\phi_{j|y=1})=
$$


二项式分布，而非高斯分布

结果就是特征的概率化



特征：就是某个单词是否出现

生成学习，最后看0高还是1高



所有特征的出现相互独立互不影响，每一特征同等重要。但事实上这个假设在现实世界中并不成立：



主要是一个假设是否容易出现的问题，即使不是一定完全成立，但是大多的特征相互独立，那



这个是传统简单方法：所以没法处理临域分析，这种需要深度学习了，深度学习有卷积可以对临域相关性分析



清洗：会改变数据分布

对图像和文本分析，很少做预处理，因为这样会丢失信息；一般是可能只在确实信息的时候进行数据填充的预处理，其他很少用



小样本学习问题：

生成对抗网络：生成更多数据放里面，扩充样本集



生成小样本集

- 加入随机噪声



以前降维：计算机性能差，现在这个目的不需要了

现在降维：语意化



可以拓展为多分类

把最后的伯努利换成softmax

线性的，没有性能差异



最大似然函数：每个邮件是垃圾邮件的概率



多元伯努利事件模型：可以拓展到多项式分布



传统伯努利模型NB-MBEM，每一个特征的取值是0/1，表示是否出现

多项式事件模型NB-MEM，每一个特征的取值是0-位置长度，表示在字典中的索引值



拉普拉斯平滑处理，是为了防止分母是0



到这里，线性模型就完了



非线性模型

- 随机森林
- 神经网络
- SVM

等



