import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,a as t,o as n}from"./app-z8Dpj-As.js";const o={};function s(r,e){return n(),a("div",null,e[0]||(e[0]=[t('<h1 id="paper-evaluation-online-modeling-for-realtime-facial-animation" tabindex="-1"><a class="header-anchor" href="#paper-evaluation-online-modeling-for-realtime-facial-animation"><span>Paper Evaluation: Online Modeling for Realtime Facial Animation</span></a></h1><h2 id="_1-paper-title-authors-and-affiliations" tabindex="-1"><a class="header-anchor" href="#_1-paper-title-authors-and-affiliations"><span>1. Paper Title, Authors, and Affiliations</span></a></h2><p><strong>Title</strong>: Online Modeling for Realtime Facial Animation</p><p><strong>Authors</strong>: Sofien Bouaziz, Yangang Wang, Mark Pauly</p><p><strong>Affiliations</strong>:</p><ul><li>École Polytechnique Fédérale de Lausanne (EPFL)</li><li>Tsinghua University</li></ul><h2 id="_2-main-contribution" tabindex="-1"><a class="header-anchor" href="#_2-main-contribution"><span>2. Main Contribution</span></a></h2><p>This paper presents a real-time facial tracking method using RGB-D sensors without requiring user-specific training or calibration. The key innovation is an adaptive Dynamic Expression Model (DEM) that learns the user&#39;s facial expressions on the fly while tracking, improving accuracy over time. This method enhances accessibility by removing the need for extensive setup, making real-time facial animation more practical for consumer applications.</p><h2 id="_3-outline-of-the-major-topics" tabindex="-1"><a class="header-anchor" href="#_3-outline-of-the-major-topics"><span>3. Outline of the Major Topics</span></a></h2><p>The paper covers several important aspects:</p><ol><li><strong>Introduction</strong> – Discusses the significance of real-time facial animation and the limitations of existing methods.</li><li><strong>Related Work</strong> – Reviews previous approaches, including marker-based tracking, pre-trained expression models, and methods requiring user calibration.</li><li><strong>Adaptive Dynamic Expression Model (DEM)</strong> – Introduces a framework that combines a blendshape model, identity PCA model, and corrective deformation fields to continuously refine tracking accuracy.</li><li><strong>Optimization for Tracking and Model Refinement</strong> – Describes the two-stage optimization approach: (1) tracking facial expressions using depth and image data, and (2) refining the expression model to better fit the user’s face over time.</li><li><strong>Implementation and Results</strong> – Demonstrates the method’s effectiveness through experiments, evaluating tracking accuracy, convergence speed, and computational efficiency.</li><li><strong>Conclusion and Future Work</strong> – Summarizes the contributions and discusses potential improvements, such as integrating speech analysis for better lip synchronization.</li></ol><h2 id="_4-one-thing-i-liked" tabindex="-1"><a class="header-anchor" href="#_4-one-thing-i-liked"><span>4. One Thing I Liked</span></a></h2><p>I found it particularly interesting that the system learns and adapts to the user’s facial expressions dynamically, instead of relying on predefined expressions or a lengthy calibration process. This makes real-time tracking more flexible and accessible, which is crucial for applications in gaming, virtual avatars, and social interactions.</p><h2 id="_5-what-i-did-not-like" tabindex="-1"><a class="header-anchor" href="#_5-what-i-did-not-like"><span>5. What I Did Not Like</span></a></h2><p>While the method is efficient and practical, the paper does not extensively discuss potential failure cases, such as how the model handles extreme expressions or rapid movements. Additionally, the reliance on depth sensors like Kinect may limit its applicability, as not all consumer devices support this technology.</p><h2 id="_6-questions-for-the-authors" tabindex="-1"><a class="header-anchor" href="#_6-questions-for-the-authors"><span>6. Questions for the Authors</span></a></h2><ol><li>How does the system handle significant variations in facial structure, such as differences in age, ethnicity, or facial hair? Would additional tuning be needed for better accuracy across diverse users?</li><li>What are the main challenges in extending this approach to track not just facial expressions but also fine-grained muscle movements, such as subtle micro expressions?</li></ol>',17)]))}const d=i(o,[["render",s],["__file","facial.html.vue"]]),h=JSON.parse(`{"path":"/education/ms/6600/eval/week3/facial.html","title":"Paper Evaluation: Online Modeling for Realtime Facial Animation","lang":"en-US","frontmatter":{"description":"Paper Evaluation: Online Modeling for Realtime Facial Animation 1. Paper Title, Authors, and Affiliations Title: Online Modeling for Realtime Facial Animation Authors: Sofien Bo...","head":[["meta",{"property":"og:url","content":"https://davidliuk.github.io/blog/blog/education/ms/6600/eval/week3/facial.html"}],["meta",{"property":"og:site_name","content":"David's Blog"}],["meta",{"property":"og:title","content":"Paper Evaluation: Online Modeling for Realtime Facial Animation"}],["meta",{"property":"og:description","content":"Paper Evaluation: Online Modeling for Realtime Facial Animation 1. Paper Title, Authors, and Affiliations Title: Online Modeling for Realtime Facial Animation Authors: Sofien Bo..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-05-28T16:32:52.000Z"}],["meta",{"property":"article:modified_time","content":"2025-05-28T16:32:52.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Paper Evaluation: Online Modeling for Realtime Facial Animation\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-05-28T16:32:52.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"David Liu\\",\\"url\\":\\"https://github.com/davidliuk\\"}]}"]]},"headers":[{"level":2,"title":"1. Paper Title, Authors, and Affiliations","slug":"_1-paper-title-authors-and-affiliations","link":"#_1-paper-title-authors-and-affiliations","children":[]},{"level":2,"title":"2. Main Contribution","slug":"_2-main-contribution","link":"#_2-main-contribution","children":[]},{"level":2,"title":"3. Outline of the Major Topics","slug":"_3-outline-of-the-major-topics","link":"#_3-outline-of-the-major-topics","children":[]},{"level":2,"title":"4. One Thing I Liked","slug":"_4-one-thing-i-liked","link":"#_4-one-thing-i-liked","children":[]},{"level":2,"title":"5. What I Did Not Like","slug":"_5-what-i-did-not-like","link":"#_5-what-i-did-not-like","children":[]},{"level":2,"title":"6. Questions for the Authors","slug":"_6-questions-for-the-authors","link":"#_6-questions-for-the-authors","children":[]}],"git":{"createdTime":1748449972000,"updatedTime":1748449972000,"contributors":[{"name":"David","email":"l729641074@163.com","commits":1}]},"readingTime":{"minutes":1.43,"words":428},"filePathRelative":"education/ms/6600/eval/week3/facial.md","localizedDate":"May 28, 2025","excerpt":"\\n<h2>1. Paper Title, Authors, and Affiliations</h2>\\n<p><strong>Title</strong>: Online Modeling for Realtime Facial Animation</p>\\n<p><strong>Authors</strong>: Sofien Bouaziz, Yangang Wang, Mark Pauly</p>\\n<p><strong>Affiliations</strong>:</p>\\n<ul>\\n<li>École Polytechnique Fédérale de Lausanne (EPFL)</li>\\n<li>Tsinghua University</li>\\n</ul>","autoDesc":true}`);export{d as comp,h as data};
