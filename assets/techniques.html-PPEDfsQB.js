import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,a as t,o as e}from"./app-BBRqDuqm.js";const i={};function l(p,s){return e(),n("div",null,s[0]||(s[0]=[t(`<h1 id="techniques" tabindex="-1"><a class="header-anchor" href="#techniques"><span>Techniques</span></a></h1><p>残差连接 Skip Connection, ResNet</p><p>激活函数（Activation Function）</p><p>常见的激活函数都是非线性的，因此也会给神经元引入非线性元素，使得神经网络可以逼近其他的任何非线性函数，这样可以使得神经网络应用到更多非线性模型中。</p><ul><li><strong>线性激活函数</strong>（线性方程控制输入到输出的映射，如f(x)=x等）</li><li><strong>非线性激活函数</strong>（非线性方程控制输入到输出的映射，比如 <ul><li>Sigmoid、</li><li>Tanh、</li><li>ReLU、 <ul><li>LReLU</li><li>PReLU</li><li>Swish</li></ul></li><li>Softmax 是用于多类分类问题的激活函数</li></ul></li></ul><p>attention</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msub><msqrt><mi>d</mi></msqrt><mi>k</mi></msub></mfrac><mo stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt d_k})V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">a</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.6775em;vertical-align:-0.588em;"></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0895em;"><span style="top:-2.5335em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9378em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal mtight" style="padding-left:0.833em;">d</span></span><span style="top:-2.8978em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1022em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3224em;"><span style="top:-2.3264em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1736em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">Q</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9191em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.588em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></p><p>Attention机制是一种广泛应用于深度学习中的技术，最初用于解决神经机器翻译任务中的长序列问题，现在已经扩展到各种任务中。以下是常见的Attention机制分类和实现：</p><hr><h3 id="_1-按实现方法分类" tabindex="-1"><a class="header-anchor" href="#_1-按实现方法分类"><span><strong>1. 按实现方法分类</strong></span></a></h3><h4 id="_1-1-基于加法的attention-additive-attention" tabindex="-1"><a class="header-anchor" href="#_1-1-基于加法的attention-additive-attention"><span><strong>1.1. 基于加法的Attention（Additive Attention）</strong></span></a></h4><ul><li><p>提出于Bahdanau Attention（2014）。</p></li><li><p>使用一个可训练的前馈神经网络（通常是一层全连接层）计算注意力得分。</p></li><li><p><strong>公式</strong>: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msup><mtext>v</mtext><mi>T</mi></msup><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>W</mi><mn>1</mn></msub><msub><mi>h</mi><mi>i</mi></msub><mo>+</mo><msub><mi>W</mi><mn>2</mn></msub><msub><mi>s</mi><mi>j</mi></msub><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">e_{ij} = \\text{v}^T \\tanh(W_1 h_i + W_2 s_j + b)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord text"><span class="mord">v</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">tanh</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span></span></span></span></p><p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">h_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是输入序列的编码器隐藏状态，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">s_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>是解码器隐藏状态。</p></li></ul><h4 id="_1-2-基于点积的attention-dot-product-attention" tabindex="-1"><a class="header-anchor" href="#_1-2-基于点积的attention-dot-product-attention"><span><strong>1.2. 基于点积的Attention（Dot-Product Attention）</strong></span></a></h4><ul><li><p>提出于Vaswani等人的Transformer模型（2017）。</p></li><li><p>通过计算Query和Key向量的点积来获得注意力得分。</p></li><li><p><strong>公式</strong>: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msub><mi>Q</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>K</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">e_{ij} = Q_i \\cdot K_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></p><p>点积的结果常使用Softmax归一化，并可加入缩放因子，称为<strong>Scaled Dot-Product Attention</strong>:</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mrow><mo fence="true">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo fence="true">)</mo></mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.8em;vertical-align:-0.65em;"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0895em;"><span style="top:-2.5864em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8622em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mtight" style="padding-left:0.833em;"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8222em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1778em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">Q</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9191em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.538em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">)</span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></p></li></ul><h4 id="_1-3-多头attention-multi-head-attention" tabindex="-1"><a class="header-anchor" href="#_1-3-多头attention-multi-head-attention"><span><strong>1.3. 多头Attention（Multi-Head Attention）</strong></span></a></h4><ul><li>将输入划分为多个头，每个头独立计算Attention，结果通过线性变换进行组合。</li><li><strong>优点</strong>: 增强模型的表示能力，捕获不同子空间的特征。</li></ul><hr><h3 id="_2-按功能分类" tabindex="-1"><a class="header-anchor" href="#_2-按功能分类"><span><strong>2. 按功能分类</strong></span></a></h3><h4 id="_2-1-自注意力-self-attention" tabindex="-1"><a class="header-anchor" href="#_2-1-自注意力-self-attention"><span><strong>2.1. 自注意力（Self-Attention）</strong></span></a></h4><ul><li>Query、Key和Value都来源于同一个输入序列。</li><li>常用于序列建模，特别是在Transformer中。</li></ul><h4 id="_2-2-跨注意力-cross-attention" tabindex="-1"><a class="header-anchor" href="#_2-2-跨注意力-cross-attention"><span><strong>2.2. 跨注意力（Cross-Attention）</strong></span></a></h4><ul><li>Query来自一个序列（如解码器），Key和Value来自另一个序列（如编码器）。</li><li>通常用于Encoder-Decoder架构中。</li></ul><h4 id="_2-3-全局注意力-global-attention" tabindex="-1"><a class="header-anchor" href="#_2-3-全局注意力-global-attention"><span><strong>2.3. 全局注意力（Global Attention）</strong></span></a></h4><ul><li>考虑整个输入序列作为注意力的计算范围。</li></ul><h4 id="_2-4-局部注意力-local-attention" tabindex="-1"><a class="header-anchor" href="#_2-4-局部注意力-local-attention"><span><strong>2.4. 局部注意力（Local Attention）</strong></span></a></h4><ul><li>只在输入序列中选取一个子窗口，减少计算量，适用于长序列。</li></ul><hr><h3 id="_3-按架构改进分类" tabindex="-1"><a class="header-anchor" href="#_3-按架构改进分类"><span><strong>3. 按架构改进分类</strong></span></a></h3><h4 id="_3-1-transformer-attention" tabindex="-1"><a class="header-anchor" href="#_3-1-transformer-attention"><span><strong>3.1. Transformer Attention</strong></span></a></h4><ul><li><strong>核心特点</strong>: Scaled Dot-Product Attention和Multi-Head Attention结合。</li><li>应用于自然语言处理、计算机视觉等任务中。</li></ul><h4 id="_3-2-soft-attention-和-hard-attention" tabindex="-1"><a class="header-anchor" href="#_3-2-soft-attention-和-hard-attention"><span><strong>3.2. Soft Attention 和 Hard Attention</strong></span></a></h4><ul><li><strong>Soft Attention</strong>: 通过Softmax函数得到注意力分布，可微，适合反向传播。</li><li><strong>Hard Attention</strong>: 选择单一注意力焦点，使用采样技术训练，非可微。</li></ul><h4 id="_3-3-sparse-attention" tabindex="-1"><a class="header-anchor" href="#_3-3-sparse-attention"><span><strong>3.3. Sparse Attention</strong></span></a></h4><ul><li>降低全局注意力的计算复杂度，仅关注部分输入。</li><li>例子：Longformer、BigBird。</li></ul><h4 id="_3-4-memory-augmented-attention" tabindex="-1"><a class="header-anchor" href="#_3-4-memory-augmented-attention"><span><strong>3.4. Memory-Augmented Attention</strong></span></a></h4><ul><li>引入外部记忆模块，帮助捕获更多上下文信息。</li><li>例子：Memory Networks。</li></ul><h4 id="_3-5-low-rank-attention" tabindex="-1"><a class="header-anchor" href="#_3-5-low-rank-attention"><span><strong>3.5. Low-Rank Attention</strong></span></a></h4><ul><li>使用低秩分解降低计算复杂度。</li><li>例子：Performer。</li></ul><h4 id="_3-6-dynamic-attention" tabindex="-1"><a class="header-anchor" href="#_3-6-dynamic-attention"><span><strong>3.6. Dynamic Attention</strong></span></a></h4><ul><li>根据上下文动态调整注意力计算范围和策略。</li><li>例子：Deformable Attention。</li></ul><h3 id="_4-应用场景中的变种" tabindex="-1"><a class="header-anchor" href="#_4-应用场景中的变种"><span><strong>4. 应用场景中的变种</strong></span></a></h3><h4 id="_4-1-自然语言处理" tabindex="-1"><a class="header-anchor" href="#_4-1-自然语言处理"><span><strong>4.1. 自然语言处理</strong></span></a></h4><ul><li><strong>BERT</strong>: 基于双向Transformer的模型。</li><li><strong>GPT</strong>: 采用单向自回归Transformer。</li></ul><h4 id="_4-2-计算机视觉" tabindex="-1"><a class="header-anchor" href="#_4-2-计算机视觉"><span><strong>4.2. 计算机视觉</strong></span></a></h4><ul><li><strong>Vision Transformer（ViT）</strong>: 使用Self-Attention替代卷积操作。</li><li><strong>Deformable DETR</strong>: 使用可变形注意力提高目标检测的效率。</li></ul><h4 id="_4-3-多模态学习" tabindex="-1"><a class="header-anchor" href="#_4-3-多模态学习"><span><strong>4.3. 多模态学习</strong></span></a></h4><ul><li><strong>CLIP</strong>: 文本和图像跨模态对齐的Attention模型。</li><li><strong>UNITER</strong>: 多模态统一表征。</li></ul><h3 id="_5-未来方向" tabindex="-1"><a class="header-anchor" href="#_5-未来方向"><span><strong>5. 未来方向</strong></span></a></h3><ul><li><strong>Efficient Attention</strong>: 专注于减少长序列中的计算量。</li><li><strong>Hierarchical Attention</strong>: 在不同层次上关注局部与全局特征。</li><li><strong>Hybrid Models</strong>: 与卷积网络等结合以提高模型性能。</li></ul>`,49)]))}const o=a(i,[["render",l],["__file","techniques.html.vue"]]),c=JSON.parse(`{"path":"/ai/dl/techniques.html","title":"Techniques","lang":"en-US","frontmatter":{"description":"Techniques 残差连接 Skip Connection, ResNet 激活函数（Activation Function） 常见的激活函数都是非线性的，因此也会给神经元引入非线性元素，使得神经网络可以逼近其他的任何非线性函数，这样可以使得神经网络应用到更多非线性模型中。 线性激活函数（线性方程控制输入到输出的映射，如f(x)=x等） 非线性激活...","head":[["meta",{"property":"og:url","content":"https://davidliuk.github.io/blog/blog/ai/dl/techniques.html"}],["meta",{"property":"og:site_name","content":"David's Blog"}],["meta",{"property":"og:title","content":"Techniques"}],["meta",{"property":"og:description","content":"Techniques 残差连接 Skip Connection, ResNet 激活函数（Activation Function） 常见的激活函数都是非线性的，因此也会给神经元引入非线性元素，使得神经网络可以逼近其他的任何非线性函数，这样可以使得神经网络应用到更多非线性模型中。 线性激活函数（线性方程控制输入到输出的映射，如f(x)=x等） 非线性激活..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-10-26T05:16:21.000Z"}],["meta",{"property":"article:modified_time","content":"2025-10-26T05:16:21.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Techniques\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-10-26T05:16:21.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"David Liu\\",\\"url\\":\\"https://github.com/davidliuk\\"}]}"]]},"headers":[{"level":3,"title":"1. 按实现方法分类","slug":"_1-按实现方法分类","link":"#_1-按实现方法分类","children":[]},{"level":3,"title":"2. 按功能分类","slug":"_2-按功能分类","link":"#_2-按功能分类","children":[]},{"level":3,"title":"3. 按架构改进分类","slug":"_3-按架构改进分类","link":"#_3-按架构改进分类","children":[]},{"level":3,"title":"4. 应用场景中的变种","slug":"_4-应用场景中的变种","link":"#_4-应用场景中的变种","children":[]},{"level":3,"title":"5. 未来方向","slug":"_5-未来方向","link":"#_5-未来方向","children":[]}],"git":{"createdTime":1736210466000,"updatedTime":1761455781000,"contributors":[{"name":"dawei.liu","email":"dawei.liu@bytedance.com","commits":1}]},"readingTime":{"minutes":3.19,"words":957},"filePathRelative":"ai/dl/techniques.md","localizedDate":"January 7, 2025","excerpt":"\\n<p>残差连接 Skip Connection, ResNet</p>\\n<p>激活函数（Activation Function）</p>\\n<p>常见的激活函数都是非线性的，因此也会给神经元引入非线性元素，使得神经网络可以逼近其他的任何非线性函数，这样可以使得神经网络应用到更多非线性模型中。</p>\\n<ul>\\n<li><strong>线性激活函数</strong>（线性方程控制输入到输出的映射，如f(x)=x等）</li>\\n<li><strong>非线性激活函数</strong>（非线性方程控制输入到输出的映射，比如\\n<ul>\\n<li>Sigmoid、</li>\\n<li>Tanh、</li>\\n<li>ReLU、\\n<ul>\\n<li>LReLU</li>\\n<li>PReLU</li>\\n<li>Swish</li>\\n</ul>\\n</li>\\n<li>Softmax 是用于多类分类问题的激活函数</li>\\n</ul>\\n</li>\\n</ul>","autoDesc":true}`);export{o as comp,c as data};
