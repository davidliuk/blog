import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as l,a as t,o as a}from"./app-B6aCd_WP.js";const n={};function p(r,i){return a(),l("div",null,i[0]||(i[0]=[t('<h1 id="sim" tabindex="-1"><a class="header-anchor" href="#sim"><span>SIM</span></a></h1><p>用户长期兴趣</p><ul><li>注意力层的计算量∝（用户行为序列的长度)</li><li>只能记录最近几百个物品’否则计算量太大。</li><li>缺点：关注短期兴趣，遗忘长期兴趣·</li></ul><p>目标：保留用户长期行为序列（很大)，而且计算量不会过大。</p><p>改进DIN:</p><ul><li><p>·DIN对LastN向量做加权平均’权重是相似度</p></li><li><p>·如果某La$tN物品与候选物品差异很大，则权重接近零。</p></li><li><p>·快速排除掉与候选物品无关的LαstN物品，降低注意力层的计算量。</p></li><li><p>Qi et al.Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate <a href="http://Prediction.In" target="_blank" rel="noopener noreferrer">Prediction.In</a> CIKM,2020.</p></li><li><p>保留用户长期行为记录，的大小可以是几千。</p></li><li><p>对于每个候选物品，在用户Lα$tN记录中做快速查找，找到k个相似物品。</p></li><li><p>把LastN变成TopK,然后输入到注意力层·</p></li><li><p>SIM模型减小计算量（从n降到k)</p></li></ul><h2 id="第一步-查找" tabindex="-1"><a class="header-anchor" href="#第一步-查找"><span>第一步：查找</span></a></h2><ul><li>方法一：Iard Search <ul><li>·根据候选物品的类目，保留Lα$tN物品中类目相同的。</li><li>·简单’快速’无需训练。</li></ul></li><li>方法二：Soft Search <ul><li>·把物品做embedding,变成向量。</li><li>·把候选物品向量作为query,做k近邻查找’保留LastN 物品中最接近的飞个。</li><li>·效果更好’编程实现更复杂。</li></ul></li></ul><h2 id="第二部-注意力机制" tabindex="-1"><a class="header-anchor" href="#第二部-注意力机制"><span>第二部：注意力机制</span></a></h2><p><img src="https://gcore.jsdelivr.net/gh/davidliuk/images@master/image-20250824162708642.png" alt="image-20250824162708642" loading="lazy"></p><p>使用时间信息</p><ul><li><p>用户与某个LαstN物品的交互时刻距今为δ。</p></li><li><p>对δ做离散化，再做embedding，变成向量d。</p><blockquote><p>1d, 7d, 30d, 1y, 1+y</p></blockquote></li><li><p>把两个向量做concatenation，表征一个LastN物品。</p><ul><li>向量x是物品的embedding。</li><li>向量d是时间的embedding。</li></ul></li></ul><p><img src="https://gcore.jsdelivr.net/gh/davidliuk/images@master/image-20250824162928012.png" alt="image-20250824162928012" loading="lazy"></p><p>为什么SIM使用时间信息？</p><ul><li>DIN的序列短，记录用户近期行为。</li><li>SIM的序列长’记录用户长期行为。</li><li>时间越久远，重要性越低。</li></ul><p>结论</p><ul><li>长序列（长期兴趣）优于短序列（近期兴趣)。</li><li>注意力机制优于简单平均。</li><li>Soft search还是hard search?取决于工程基建。</li><li>使用时间信息有提升。</li></ul>',17)]))}const d=e(n,[["render",p],["__file","sim.html.vue"]]),c=JSON.parse(`{"path":"/ai/rec/lastn/sim.html","title":"SIM","lang":"en-US","frontmatter":{"description":"SIM 用户长期兴趣 注意力层的计算量∝（用户行为序列的长度) 只能记录最近几百个物品’否则计算量太大。 缺点：关注短期兴趣，遗忘长期兴趣· 目标：保留用户长期行为序列（很大)，而且计算量不会过大。 改进DIN: ·DIN对LastN向量做加权平均’权重是相似度 ·如果某La$tN物品与候选物品差异很大，则权重接近零。 ·快速排除掉与候选物品无关的Lα...","head":[["meta",{"property":"og:url","content":"https://davidliuk.github.io/blog/blog/ai/rec/lastn/sim.html"}],["meta",{"property":"og:site_name","content":"David's Blog"}],["meta",{"property":"og:title","content":"SIM"}],["meta",{"property":"og:description","content":"SIM 用户长期兴趣 注意力层的计算量∝（用户行为序列的长度) 只能记录最近几百个物品’否则计算量太大。 缺点：关注短期兴趣，遗忘长期兴趣· 目标：保留用户长期行为序列（很大)，而且计算量不会过大。 改进DIN: ·DIN对LastN向量做加权平均’权重是相似度 ·如果某La$tN物品与候选物品差异很大，则权重接近零。 ·快速排除掉与候选物品无关的Lα..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://gcore.jsdelivr.net/gh/davidliuk/images@master/image-20250824162708642.png"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-08-31T05:52:07.000Z"}],["meta",{"property":"article:modified_time","content":"2025-08-31T05:52:07.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"SIM\\",\\"image\\":[\\"https://gcore.jsdelivr.net/gh/davidliuk/images@master/image-20250824162708642.png\\",\\"https://gcore.jsdelivr.net/gh/davidliuk/images@master/image-20250824162928012.png\\"],\\"dateModified\\":\\"2025-08-31T05:52:07.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"David Liu\\",\\"url\\":\\"https://github.com/davidliuk\\"}]}"]]},"headers":[{"level":2,"title":"第一步：查找","slug":"第一步-查找","link":"#第一步-查找","children":[]},{"level":2,"title":"第二部：注意力机制","slug":"第二部-注意力机制","link":"#第二部-注意力机制","children":[]}],"git":{"createdTime":1756619527000,"updatedTime":1756619527000,"contributors":[{"name":"dawei.liu","email":"dawei.liu@bytedance.com","commits":1}]},"readingTime":{"minutes":1.73,"words":519},"filePathRelative":"ai/rec/lastn/sim.md","localizedDate":"August 31, 2025","excerpt":"\\n<p>用户长期兴趣</p>\\n<ul>\\n<li>注意力层的计算量∝（用户行为序列的长度)</li>\\n<li>只能记录最近几百个物品’否则计算量太大。</li>\\n<li>缺点：关注短期兴趣，遗忘长期兴趣·</li>\\n</ul>\\n<p>目标：保留用户长期行为序列（很大)，而且计算量不会过大。</p>\\n<p>改进DIN:</p>\\n<ul>\\n<li>\\n<p>·DIN对LastN向量做加权平均’权重是相似度</p>\\n</li>\\n<li>\\n<p>·如果某La$tN物品与候选物品差异很大，则权重接近零。</p>\\n</li>\\n<li>\\n<p>·快速排除掉与候选物品无关的LαstN物品，降低注意力层的计算量。</p>\\n</li>\\n<li>\\n<p>Qi et al.Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate <a href=\\"http://Prediction.In\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Prediction.In</a> CIKM,2020.</p>\\n</li>\\n<li>\\n<p>保留用户长期行为记录，的大小可以是几千。</p>\\n</li>\\n<li>\\n<p>对于每个候选物品，在用户Lα$tN记录中做快速查找，找到k个相似物品。</p>\\n</li>\\n<li>\\n<p>把LastN变成TopK,然后输入到注意力层·</p>\\n</li>\\n<li>\\n<p>SIM模型减小计算量（从n降到k)</p>\\n</li>\\n</ul>","autoDesc":true}`);export{d as comp,c as data};
