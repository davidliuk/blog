import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,a as i,o as r}from"./app-KvbUyAJR.js";const o={};function a(s,e){return r(),n("div",null,e[0]||(e[0]=[i('<h1 id="transcript" tabindex="-1"><a class="header-anchor" href="#transcript"><span>Transcript</span></a></h1><p>Hi everyone, I’m David Liu. Today I’ll walk you through <strong>RigNet</strong>, a neural approach to automating character rigging — a traditionally time-consuming task.</p><h2 id="overview" tabindex="-1"><a class="header-anchor" href="#overview"><span>Overview</span></a></h2><p>Let&#39;s have a quick view of today&#39;s topics:</p><ol><li>I’ll start with some background including the related knowledge we&#39;ve learned, challenges and motivation.</li><li>Then, I’ll have an quick overview of RigNet’s approach and walk through the three core modules: joint prediction, connectivity prediction, and skinning prediction.</li><li>After that, I’ll show some results and discuss evaluations.</li></ol><h2 id="background" tabindex="-1"><a class="header-anchor" href="#background"><span>Background</span></a></h2><h3 id="rigging-binding-skinning" tabindex="-1"><a class="header-anchor" href="#rigging-binding-skinning"><span><strong>Rigging, Binding, Skinning</strong></span></a></h3><p>From last semester&#39;s CI562, we&#39;ve learned a chapter of <strong>Rigging, Binding, Skinning</strong>. We took a lot time on implementing the skinning methods like LBS or DQS. But before apply these skinning methods, we should firstly have model&#39;s skeleton and skinning weights.</p><h3 id="motivation" tabindex="-1"><a class="header-anchor" href="#motivation"><span>Motivation</span></a></h3><p>Character rigs have been the <strong>backbone of articulated figure animation</strong> for over decades.</p><p>Without rigs, it&#39;s difficult for animators to animate character models effectively.</p><p>The motivation behind RigNet is to <strong>automate the rigging process</strong>.</p><p>As a result, animators can directly <strong>use the generated skeletons</strong> to animate their models, which will save a lot of time and effort.</p><h3 id="challenge" tabindex="-1"><a class="header-anchor" href="#challenge"><span>Challenge</span></a></h3><p>Predicting a skeleton and skinning weight from an arbitrary single static mesh is not easy.</p><hr><p><strong>Skeleton</strong></p><p>As shown here, one challenge is that <strong>characters can vary significantly</strong> in both the <strong>number of parts</strong> and their <strong>overall structure</strong>.</p><hr><p><strong>Skinning</strong></p><p>Similarly, when computing skinning weights, animators perceive some structures as highly <strong>rigid</strong> and others as more <strong>flexible</strong>.</p><p>For example, here we can see the skeleton of a <strong>snail character</strong>.</p><ul><li>The skinning weights on the shell move <strong>rigidly as a whole</strong> according to a single bone,</li><li>while the rest of the body <strong>deforms more smoothly</strong> in response to the other bones.</li></ul><p><em>A learning method should be capable of <strong>capturing this variability in skinning behavior</strong>.</em></p><hr><p><strong>Level of Detail</strong></p><p>Finally, another challenge for a rigging method is to allow <strong>easy and direct control</strong> over the <strong>level of detail</strong> in the output skeleton.</p><p>As you can see from these animator-created rigs, while animators largely agree on the <strong>topology</strong> and <strong>layout of joints</strong> for a given character, there is still <strong>ambiguity</strong>—both in terms of the <strong>number of joints</strong> and their <strong>exact placement</strong>.</p><h2 id="overview-1" tabindex="-1"><a class="header-anchor" href="#overview-1"><span>Overview</span></a></h2><p>To address these challenges, <strong>RigNet</strong> proposes a deep learning architecture composed of <strong>three modules</strong>:</p><hr><p><strong>1. Skeletal Joint Prediction Module</strong></p><p>First, we apply a <strong>joint prediction module</strong>, which is trained to predict both the <strong>appropriate number of joints</strong> and their <strong>placement</strong>.</p><p>Here we provide users with an <strong>optional parameter</strong> that allows them to control the <strong>level of detail</strong> in the output skeleton.</p><hr><p><strong>2. Skeleton Connectivity Module</strong></p><p>Second, to form a skeleton from the predicted joints, we apply a <strong>connectivity prediction module</strong>, which predicts the <strong>hierarchical tree structure</strong> connecting the joints.</p><p>The resulting bone structure is a function of both the <strong>predicted joints</strong> and the <strong>shape features</strong> of the input character.</p><hr><p><strong>3. Skinning Prediction Module</strong></p><p>Finally, we apply a <strong>skinning prediction module</strong> to produce <strong>skinning weights</strong>, indicating the degree of influence each <strong>mesh vertex</strong> receives from different <strong>bones</strong>.</p><hr><p>All of these modules are trained in a <strong>supervised manner</strong> from animator-created character rigs.</p><p>In the following sections, we will describe the <strong>input</strong> and <strong>internal design</strong> of each module in detail.</p><h2 id="joint-prediction" tabindex="-1"><a class="header-anchor" href="#joint-prediction"><span>Joint Prediction</span></a></h2><h3 id="architecture" tabindex="-1"><a class="header-anchor" href="#architecture"><span>Architecture</span></a></h3><p>00:04:29.040</p><p>The joint prediction module is trained to predict the number and location of skeleton joints.</p><p>To this end, it learns to displace mesh geometry towards candidate joint locations.</p><p>The module is based on a GMEdgeNet, which extracts topology- and geometry-aware features from the mesh to learn these displacements.</p><p>At the same time, we also learn a weight function over the input mesh, which is used to reveal which surface areas are more relevant for localizing joints. This can be seen as a form of neural mesh attention.</p><p>After that, we introduce a diffe&#39;rentiable clustering scheme, which uses both displaced vertices and the neural mesh attention to collapse the vertices further to potential positions of the joints. Since areas with higher point density and greater mesh attention are strong indicators of joint presence, we resort to mean shift clustering and then maximum separation to extract joints.</p><p>In classical mean shift clustering, each data point is equipped with a kernel function. At each iteration, all points are shifted towards density modes.</p><p>Here, we show the mean shift equation. We employ a variant of mean shift clustering, where the kernel is also adjusted by the vertex attention. In this manner, points with greater attention influence the estimation of density more.</p><p>Here is the shift equation in our implementation. <em>We use the Epanechnikov kernel in our implementation.</em></p><p>Note that the kernel function takes the parameter <em>h</em> as bandwidth. The bandwidth can be learned simultaneously as we train the network.</p><p>The bandwidth also allows user to control on the level of detail of the output skeleton.</p><p>Modifying the bandwidth directly affects the level of detail of the output skeleton.</p><h3 id="bandwidth" tabindex="-1"><a class="header-anchor" href="#bandwidth"><span>Bandwidth</span></a></h3><p>Here is an example:</p><ul><li>lowering the bandwidth parameter results in denser joint placement,</li><li>while increasing it results in a sparse skeleton.</li></ul><p>By overriding the learned bandwidth, users can adjust the results to their preference.</p><hr><h3 id="architecture-1" tabindex="-1"><a class="header-anchor" href="#architecture-1"><span>Architecture</span></a></h3><p><em>At test time, the mode centers of clusters are extracted with no maximum separation as the final detected joints.</em></p><p>Now we&#39;ll discuss more details about the networks used to learn the vertex displacement and attention.</p><p>We call this network <strong>GMEdgeNet</strong>. The main operation of this network is geodesic mesh convolution, which we call <strong>GMEdgeConv</strong>.</p><hr><h3 id="gmedgeconv" tabindex="-1"><a class="header-anchor" href="#gmedgeconv"><span>GMEdgeConv</span></a></h3><p>The GMEdgeConv is inspired by the H-convolution. The main difference is that our operator is applied to meshes and geodesic numbers.</p><p>Specifically, given a surface vertex, we consider its <strong>one-ring mesh neighbors</strong> and also the vertices located within the geodesic ball centered at it.</p><p>We also found it’s better to learn separate MLPs for mesh and geodesic neighborhoods, and then con&#39;catenate their outputs and process them through another MLP.</p><p>In this manner, the networks learn to weight the importance of topology-aware features over more geometry-aware ones.</p><hr><h3 id="gmedgenet" tabindex="-1"><a class="header-anchor" href="#gmedgenet"><span>GMEdgeNet</span></a></h3><p>In GMEdgeNet, we stack three GMEdgeConv layers. Each of the GMEdgeConv layers is followed by a global max pooling layer.</p><p>The representations from each pooling layer are con&#39;catenated to form a global mesh representation.</p><p>The vertex representations from all GMEdgeConv layers, as well as the global mesh representation, are further con&#39;catenated, then processed through a three-layer MLP to output the vertex attributes—either the displacement or the attention.</p><p>With all the components introduced above, we have built the complete <strong>joint prediction module</strong>, which detects joints from a single input mesh.</p><h3 id="training" tabindex="-1"><a class="header-anchor" href="#training"><span>Training</span></a></h3><p>The training of the joint prediction module consists of two steps.</p><ol><li><p>In the first step, we pre-train the weight attention module with heu&#39;ristically generated ground truth masks.</p><p>As you’ll see on the right, the masks are red for 1 and blue for 0.</p><p>For each training mesh, we find vertices closest to each joint at different directions perpen&#39;dicular to the bones.</p><p>We use <strong>cross-entropy</strong> to measure consistency between the masks and the neural attention.</p></li><li><p>In the second step, we minimize the mesh chamfer distance between collapsed vertices and the training joints.</p><p>The loss is diffe&#39;rentiable with regard to all the parameters of the joint prediction stage, including the bandwidth, the displacement network, and the attention network.</p><p><em>We found that adding supervisory signal to the vertex displacements before clustering helps improve joint detection performance.</em></p><p><em>To this end, we also minimize the transfer distance between displaced points and ground truth joints, forming tighter clusters.</em></p></li></ol><h2 id="connectivity-prediction" tabindex="-1"><a class="header-anchor" href="#connectivity-prediction"><span>Connectivity Prediction</span></a></h2><h3 id="architecture-2" tabindex="-1"><a class="header-anchor" href="#architecture-2"><span>Architecture</span></a></h3><p>Given the joints extracted from the previous stage, the connectivity prediction module determines how these joints should be connected to form the skeleton.</p><p>This module outputs the probability of connecting each pair of joints via a bone, which we call <strong>BoneNet</strong>. It takes the predicted joints along with the input mesh as input, and outputs the probability for connecting each pair of joints via a bone.</p><h3 id="bonenet" tabindex="-1"><a class="header-anchor" href="#bonenet"><span>BoneNet</span></a></h3><p>For each pair of joints, the module processes three representations:</p><ol><li>First, we use PointNet to capture the <strong>skeleton geometry</strong> from the joint locations.</li><li>Then, we use GMEdgeNet to capture the <strong>global shape geometry</strong>.</li><li>Finally, an MLP processes features for each <strong>candidate bone</strong>.</li></ol><p>The bone probability is computed via a two-layer MLP operating on the con&#39;catenation of these three representations.</p><h3 id="architecture-3" tabindex="-1"><a class="header-anchor" href="#architecture-3"><span>Architecture</span></a></h3><p>Besides pairwise connectivity probability, we also select the <strong>root joint</strong> using a neural module called <strong>RootNet</strong>. Its internal architecture follows BoneNet.</p><p>With these pairwise bone probabilities and the predicted root as the starting node, we apply <strong>Prim&#39;s algorithm</strong> to create a minimum spanning tree that represents the skeleton.</p><p><em>We found that using these bone probabilities to extract the MST resulted in skeletons that better agree with animator-created ones in terms of topology.</em></p><h3 id="training-1" tabindex="-1"><a class="header-anchor" href="#training-1"><span>Training</span></a></h3><p>To train the connectivity prediction module, we build a <strong>matrix</strong> to store the probability for connecting each pair of joints with a bone based on our prediction.</p><p>We also form such a matrix from the animator-created skeleton, including the connectivity of the skeleton. The value is 1 if the corresponding joints are connected.</p><p>Now, the parameters of BoneNet can be learned using <strong>binary cross-entropy</strong> between the training matrix and the predicted probability matrix.</p><h2 id="skinning-prediction" tabindex="-1"><a class="header-anchor" href="#skinning-prediction"><span>Skinning Prediction</span></a></h2><p>After producing the animation skeleton, the final stage applies another network to predict the <strong>skinning weights</strong> for each mesh vertex.</p><hr><p>To perform skinning, we first extract the mesh representation and capture the spatial relationship of mesh vertices with respect to the skeleton.</p><p>Given a vertex on the mesh—for example, as shown here in purple—we compute the <strong>&#39;volumetric geodesic distance</strong> to all the bones passing through the interior of the mesh volume.</p><p>Then, we sort the bones according to their geodesic distance to the vertex and create an ordered feature sequence after the <em>k</em>-shortest bones.</p><p>The feature vector for each bone consists:</p><ul><li>The 3D <strong>positions</strong> of the starting and ending joints of the bone, and</li><li>The inverse of the <strong>&#39;volumetric geodesic distance</strong> from the vertex to this bone.</li></ul><p>The final <strong>vertex representation</strong> is formed by con&#39;catenating the vertex position and the feature representations of the <em>k</em> nearest bones from the ordered sequence.</p><hr><p>The skinning prediction module then converts the above skeleton-aware mesh representations into <strong>skinning weights</strong>, using <strong>GMEdgeNet</strong>.</p><hr><h3 id="training-2" tabindex="-1"><a class="header-anchor" href="#training-2"><span>Training</span></a></h3><p>We train the parameters of the skinning network so that the estimated skinning weights align as closely as possible with the training weights.</p><p>By treating the vertex skinning weights as a <strong>probability distribution</strong>, we use <strong>cross-entropy loss</strong> to supervise the learning.</p><p><em>Now we show the experimental results and the evaluation of our method.</em></p><h3 id="dataset" tabindex="-1"><a class="header-anchor" href="#dataset"><span>Dataset</span></a></h3><p>The dataset contains <strong>2,703 characters</strong> and spans a wide range of categories—including:</p><ul><li>Humanoids</li><li>Cultural figures</li><li>Pets</li><li>Birds</li><li>Fish</li><li>Robots</li><li>Toys</li><li>Other fictional characters</li></ul><p>Here, we show some example models from the dataset, illustrating its <strong>diversity in shape, topology, and articulation complexity</strong>.</p><h2 id="results" tabindex="-1"><a class="header-anchor" href="#results"><span>Results</span></a></h2><h3 id="skeleton-extraction" tabindex="-1"><a class="header-anchor" href="#skeleton-extraction"><span>Skeleton Extraction</span></a></h3><p>For skeleton extraction, we show some artist-created models alongside the results from our approach. Our results align well with those created by professional animators.</p><p>We also present skeletons produced by <strong>Pinocchio</strong>,</p><p>red boxes highlights its mistakes.</p><hr><h3 id="skinning-weight" tabindex="-1"><a class="header-anchor" href="#skinning-weight"><span>Skinning Weight</span></a></h3><p>Next, we compare skinning weight prediction results.</p><p>First are the results from our method. We visualize:</p><ul><li><strong>Skinning weights</strong> (red indicates high values)</li><li><strong>Skinning error maps</strong> (yellow indicates high error)</li></ul><p>When animating the characters with the predicted skinning, our method accurately captures the underlying articulated parts.</p><p>We then show results from a <strong>previous learning-based skinning method</strong>, whose skinning weights tend to <strong>overextend</strong> to larger areas than necessary.</p><p>Finally, results from a <strong>geometry-based skinning method</strong> are shown, with even <strong>higher error</strong>.</p><hr><h3 id="quantitative-comparison" tabindex="-1"><a class="header-anchor" href="#quantitative-comparison"><span>Quantitative Comparison</span></a></h3><p>We show quantitative results of <strong>skeleton prediction</strong> with other methods.</p><p>We apply the <strong>Hungarian algorithm</strong> to match predicted joints to ground truth ones. Metrics such as <strong>IoU, precision, and recall</strong> are defined based on this matching. Higher values indicate better performance.</p><p>We also evaluate <strong>transfer distance</strong> between:</p><ul><li>Predicted and ground truth joints</li><li>Joints and bones</li><li>Bones themselves</li></ul><p>To measure bone transfer distance, we <strong>densely sample</strong> along the bone and compute distances between samples.</p><p>In this metric, <strong>lower values indicate better results</strong>.</p><p>From the table, we can see our method <strong>outperforms all others across all metrics</strong>.</p><hr><p>For <strong>skinning weight prediction</strong>, we compare against:</p><ul><li>Geometrical approaches: <strong>BBW</strong> and <strong>GeoVoxel</strong></li><li>A learning-based approach: <strong>NeuroSkinning</strong></li></ul><p>Precision and recall are measured by identifying bones that significantly influence each vertex.<br> We also report:</p><ul><li><strong>Average L1 norm</strong> between predicted and reference skinning vectors</li><li><strong>Average and maximal Euclidean distance</strong> between deformed vertex positions (using ground-truth vs. predicted skinning)</li></ul><p>Our approach achieves the <strong>best results across all measures</strong>.</p><hr><h3 id="generalization" tabindex="-1"><a class="header-anchor" href="#generalization"><span><strong>Generalization</strong></span></a></h3><p>After training, RigNet is able to <strong>rig diverse 3D models</strong> and even <strong>generalizes to models with different structures and parts</strong>.</p><p>Here are some examples.</p><p>We test 3D models animated using our predicted rigs.</p><p>The rigs correctly capture the <strong>articulations of arms and legs</strong>.</p><p>We present another example animated with predicted skeleton and skinning, where <strong>all limbs are rigged correctly</strong>.</p><p>Our method can also <strong>handle non-human models</strong>, predicting reasonable bones for a <strong>cat model</strong>, for instance.</p><p><em>Our predicted skeletons are <strong>consistent across different shape families</strong>, allowing for <strong>automatic motion transfer</strong> to animate them together.</em></p><h2 id="evaluation" tabindex="-1"><a class="header-anchor" href="#evaluation"><span>Evaluation</span></a></h2><h3 id="questions" tabindex="-1"><a class="header-anchor" href="#questions"><span>Questions</span></a></h3><p>Here are the questions I would like to ask the author:</p><ol><li><p><strong>First</strong>, how does it handle characters with highly non-organic or mechanically complex structures, like robots or abstract shapes?</p></li><li><p><strong>Second</strong>, are there optimizations for real-time prediction?</p><p>Currently, the method focuses on accuracy over speed. There’s no built-in runtime optimization, which limits real-time applicability—but this is a promising area for future work.</p></li><li><p><strong>Third</strong>, can users enforce constraints or edit the output?</p><p>Not directly. RigNet doesn’t currently support user-defined constraints like fixing the root joint, nor does it provide post-prediction editing. This is one of the major limitations in terms of usability for animators.</p></li></ol><h3 id="evaluation-1" tabindex="-1"><a class="header-anchor" href="#evaluation-1"><span>Evaluation</span></a></h3><p>Let’s dive into the evaluation.</p><p><strong>RigNet has several strengths:</strong></p><ul><li>It’s the first end-to-end neural method that predicts both skeletons and skinning weights directly from raw meshes.</li><li>It offers <strong>user flexibility</strong>—you can adjust the level of skeletal detail.</li><li>It generalizes well across different character types and morphologies.</li><li>And it shows strong performance compared to existing methods.</li></ul><p><strong>But there are also limitations:</strong></p><ul><li>The paper doesn’t include a runtime analysis, which is critical for real-time applications.</li><li>And as mentioned earlier, user control is limited—there’s no way to apply constraints or refine the predicted rig manually.</li></ul><h3 id="aceptance" tabindex="-1"><a class="header-anchor" href="#aceptance"><span>Aceptance</span></a></h3><p>From my point of view, the reception of RigNet would be vaery positive.</p><p><strong>On the plus side</strong>, reviewers appreciated its novelty, thorough evaluation, and real-world relevance to animation workflows.</p><p><strong>The main criticism</strong> was around computational cost, particularly the lack of performance benchmarks for real-time or interactive scenarios.</p><p><strong>Overall</strong>, RigNet represents a significant step forward in automated rigging and opens the door to more flexible and efficient character setup in the future.</p><p><strong>Thank you for your attention!</strong></p><p>To summarize, RigNet is the <strong>first learning-based, complete solution for character rigging</strong>, including both <strong>skeleton creation</strong> and <strong>skinning weight prediction</strong>.</p><p>Our approach:</p><ul><li><strong>Makes no assumptions</strong> about input shape class or structure</li><li><strong>Generalizes well</strong> to characters with varying structures and morphologies</li><li><strong>Provides a single parameter</strong> for users to control output granularity</li></ul><p>However, there are some <strong>limitations and directions for future work</strong>:</p><ol><li><strong>Per-stage training</strong>: Currently, each stage is trained separately. Ideally, the <strong>skinning loss could be backpropagated</strong> to earlier stages to improve joint prediction.</li><li><strong>Dataset limitations</strong>: Our dataset contains only one rig per model. Many rigs <strong>omit bones for small parts</strong> (e.g., feet, fingers, clothing, accessories), which <strong>reduces prediction accuracy</strong> for such joints.</li><li><strong>Bandwidth parameter</strong>: Our current approach only explores <strong>one mode of variation</strong>. Future work could explore a <strong>richer space for interactively controlling skeletal morphology and resolution</strong>.</li></ol>',176)]))}const c=t(o,[["render",a],["__file","transcript.html.vue"]]),g=JSON.parse(`{"path":"/education/ms/6600/siggraph/pre/transcript.html","title":"Transcript","lang":"en-US","frontmatter":{"description":"Transcript Hi everyone, I’m David Liu. Today I’ll walk you through RigNet, a neural approach to automating character rigging — a traditionally time-consuming task. Overview Let'...","head":[["meta",{"property":"og:url","content":"https://davidliuk.github.io/blog/blog/education/ms/6600/siggraph/pre/transcript.html"}],["meta",{"property":"og:site_name","content":"David's Blog"}],["meta",{"property":"og:title","content":"Transcript"}],["meta",{"property":"og:description","content":"Transcript Hi everyone, I’m David Liu. Today I’ll walk you through RigNet, a neural approach to automating character rigging — a traditionally time-consuming task. Overview Let'..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-05-28T16:32:52.000Z"}],["meta",{"property":"article:modified_time","content":"2025-05-28T16:32:52.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Transcript\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-05-28T16:32:52.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"David Liu\\",\\"url\\":\\"https://github.com/davidliuk\\"}]}"]]},"headers":[{"level":2,"title":"Overview","slug":"overview","link":"#overview","children":[]},{"level":2,"title":"Background","slug":"background","link":"#background","children":[{"level":3,"title":"Rigging, Binding, Skinning","slug":"rigging-binding-skinning","link":"#rigging-binding-skinning","children":[]},{"level":3,"title":"Motivation","slug":"motivation","link":"#motivation","children":[]},{"level":3,"title":"Challenge","slug":"challenge","link":"#challenge","children":[]}]},{"level":2,"title":"Overview","slug":"overview-1","link":"#overview-1","children":[]},{"level":2,"title":"Joint Prediction","slug":"joint-prediction","link":"#joint-prediction","children":[{"level":3,"title":"Architecture","slug":"architecture","link":"#architecture","children":[]},{"level":3,"title":"Bandwidth","slug":"bandwidth","link":"#bandwidth","children":[]},{"level":3,"title":"Architecture","slug":"architecture-1","link":"#architecture-1","children":[]},{"level":3,"title":"GMEdgeConv","slug":"gmedgeconv","link":"#gmedgeconv","children":[]},{"level":3,"title":"GMEdgeNet","slug":"gmedgenet","link":"#gmedgenet","children":[]},{"level":3,"title":"Training","slug":"training","link":"#training","children":[]}]},{"level":2,"title":"Connectivity Prediction","slug":"connectivity-prediction","link":"#connectivity-prediction","children":[{"level":3,"title":"Architecture","slug":"architecture-2","link":"#architecture-2","children":[]},{"level":3,"title":"BoneNet","slug":"bonenet","link":"#bonenet","children":[]},{"level":3,"title":"Architecture","slug":"architecture-3","link":"#architecture-3","children":[]},{"level":3,"title":"Training","slug":"training-1","link":"#training-1","children":[]}]},{"level":2,"title":"Skinning Prediction","slug":"skinning-prediction","link":"#skinning-prediction","children":[{"level":3,"title":"Training","slug":"training-2","link":"#training-2","children":[]},{"level":3,"title":"Dataset","slug":"dataset","link":"#dataset","children":[]}]},{"level":2,"title":"Results","slug":"results","link":"#results","children":[{"level":3,"title":"Skeleton Extraction","slug":"skeleton-extraction","link":"#skeleton-extraction","children":[]},{"level":3,"title":"Skinning Weight","slug":"skinning-weight","link":"#skinning-weight","children":[]},{"level":3,"title":"Quantitative Comparison","slug":"quantitative-comparison","link":"#quantitative-comparison","children":[]},{"level":3,"title":"Generalization","slug":"generalization","link":"#generalization","children":[]}]},{"level":2,"title":"Evaluation","slug":"evaluation","link":"#evaluation","children":[{"level":3,"title":"Questions","slug":"questions","link":"#questions","children":[]},{"level":3,"title":"Evaluation","slug":"evaluation-1","link":"#evaluation-1","children":[]},{"level":3,"title":"Aceptance","slug":"aceptance","link":"#aceptance","children":[]}]}],"git":{"createdTime":1748449972000,"updatedTime":1748449972000,"contributors":[{"name":"David","email":"l729641074@163.com","commits":1}]},"readingTime":{"minutes":8.9,"words":2670},"filePathRelative":"education/ms/6600/siggraph/pre/transcript.md","localizedDate":"May 28, 2025","excerpt":"\\n<p>Hi everyone, I’m David Liu. Today I’ll walk you through <strong>RigNet</strong>, a neural approach to automating character rigging — a traditionally time-consuming task.</p>\\n<h2>Overview</h2>\\n<p>Let's have a quick view of today's topics:</p>\\n<ol>\\n<li>I’ll start with some background including the related knowledge we've learned, challenges and motivation.</li>\\n<li>Then, I’ll have an quick overview of RigNet’s approach and walk through the three core modules: joint prediction, connectivity prediction, and skinning prediction.</li>\\n<li>After that, I’ll show some results and discuss evaluations.</li>\\n</ol>","autoDesc":true}`);export{c as comp,g as data};
