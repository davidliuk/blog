import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as e,b as n,o as l}from"./app-B3ooTv29.js";const o={};function a(r,t){return l(),e("div",null,t[0]||(t[0]=[n("<p>resnet</p><p>残差连接</p><p>激活函数（Activation Function）</p><p>常见的激活函数都是非线性的，因此也会给神经元引入非线性元素，使得神经网络可以逼近其他的任何非线性函数，这样可以使得神经网络应用到更多非线性模型中。</p><ul><li><strong>线性激活函数</strong>（线性方程控制输入到输出的映射，如f(x)=x等）</li><li><strong>非线性激活函数</strong>（非线性方程控制输入到输出的映射，比如 <ul><li>Sigmoid、</li><li>Tanh、</li><li>ReLU、 <ul><li>LReLU</li><li>PReLU</li><li>Swish</li></ul></li><li>Softmax 是用于多类分类问题的激活函数</li></ul></li></ul><p>attention</p>",6)]))}const m=i(o,[["render",a],["__file","Untitled.html.vue"]]),s=JSON.parse(`{"path":"/ai/nn/Untitled.html","title":"","lang":"en-US","frontmatter":{"description":"resnet 残差连接 激活函数（Activation Function） 常见的激活函数都是非线性的，因此也会给神经元引入非线性元素，使得神经网络可以逼近其他的任何非线性函数，这样可以使得神经网络应用到更多非线性模型中。 线性激活函数（线性方程控制输入到输出的映射，如f(x)=x等） 非线性激活函数（非线性方程控制输入到输出的映射，比如 Sigmoi...","head":[["meta",{"property":"og:url","content":"https://davidliuk.github.io/blog/blog/ai/nn/Untitled.html"}],["meta",{"property":"og:site_name","content":"David's Blog"}],["meta",{"property":"og:description","content":"resnet 残差连接 激活函数（Activation Function） 常见的激活函数都是非线性的，因此也会给神经元引入非线性元素，使得神经网络可以逼近其他的任何非线性函数，这样可以使得神经网络应用到更多非线性模型中。 线性激活函数（线性方程控制输入到输出的映射，如f(x)=x等） 非线性激活函数（非线性方程控制输入到输出的映射，比如 Sigmoi..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2024-01-08T03:06:14.000Z"}],["meta",{"property":"article:modified_time","content":"2024-01-08T03:06:14.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2024-01-08T03:06:14.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"David Liu\\",\\"url\\":\\"https://github.com/davidliuk\\"}]}"]]},"headers":[],"git":{"createdTime":1704683174000,"updatedTime":1704683174000,"contributors":[{"name":"davidliu","email":"liudawei47@jd.com","commits":1}]},"readingTime":{"minutes":0.5,"words":151},"filePathRelative":"ai/nn/Untitled.md","localizedDate":"January 8, 2024","excerpt":"<p>resnet</p>\\n<p>残差连接</p>\\n<p>激活函数（Activation Function）</p>\\n<p>常见的激活函数都是非线性的，因此也会给神经元引入非线性元素，使得神经网络可以逼近其他的任何非线性函数，这样可以使得神经网络应用到更多非线性模型中。</p>\\n<ul>\\n<li><strong>线性激活函数</strong>（线性方程控制输入到输出的映射，如f(x)=x等）</li>\\n<li><strong>非线性激活函数</strong>（非线性方程控制输入到输出的映射，比如\\n<ul>\\n<li>Sigmoid、</li>\\n<li>Tanh、</li>\\n<li>ReLU、\\n<ul>\\n<li>LReLU</li>\\n<li>PReLU</li>\\n<li>Swish</li>\\n</ul>\\n</li>\\n<li>Softmax 是用于多类分类问题的激活函数</li>\\n</ul>\\n</li>\\n</ul>","autoDesc":true}`);export{m as comp,s as data};
