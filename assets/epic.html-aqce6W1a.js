import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as p,a as t,o as l}from"./app-z8Dpj-As.js";const n={};function a(o,e){return l(),p("div",null,e[0]||(e[0]=[t('<h1 id="epic-lab" tabindex="-1"><a class="header-anchor" href="#epic-lab"><span>EPIC Lab</span></a></h1><h2 id="方向" tabindex="-1"><a class="header-anchor" href="#方向"><span>方向</span></a></h2><h3 id="在研" tabindex="-1"><a class="header-anchor" href="#在研"><span>在研</span></a></h3><p>模型优化</p><ul><li><p>LLM 大模型</p><p>扩散大语言模型</p><p>可以多模态 LLaDA-V，做 token 压缩，正在做这个刚开</p><p>DeepSeek-VL2，多模态 MoE 大模型</p><p>MoE 剪枝，</p></li><li><p>VLM 多模态大模型，token pruning</p><p>ViT+LLM，一般是压缩LLM，目前在考虑在ViT部分压缩，需要算力</p><ul><li>KV Cache</li><li>已经做了三四篇了，最近做的人太多了😭不想硬卷换赛道</li><li>做专门领域的更好：Video, Steaming, <em>Audio</em>, UI Agent, 自动驾驶, <em>AI4Science</em> 方向的</li><li>找哪个场景</li></ul><p>ViT的压缩还有很大可以优化的点</p></li><li><p>文、视频生成</p></li></ul><p>数据集压缩</p><ul><li>Qwen 30T token =&gt; 3T token</li><li>怎么样抽取最关键的数据</li></ul><h3 id="未来" tabindex="-1"><a class="header-anchor" href="#未来"><span>未来</span></a></h3><p>Multi-Agent 多智能体优化</p><ul><li>Multi-Agent</li><li>准确率</li><li>沟通效率更高，花费更小</li><li>UI Agent</li></ul><p>VLA 具身智能</p><hr><p>模型压缩</p><p>专用</p><p>LLaDA-v token 肯定能做，刚刚开始做，已经 3个人</p><p>MMaDA</p><p>MoE</p><p>mllm moe 压缩，可压缩可不压缩，但是了解不多，暂时还没有人开始做</p><p>fastvlm</p><hr><p>基础的Idea，蓝海方向</p><p>AFFlow，蒙特卡洛搜索，解决数学任务，在强化学习做的</p><p>EvoFlow 定义成图问题，边上做遗传算法</p><p>这些一般是数学上求解</p><p>随机搜索应该也能搞定</p><p>效果一定可以比baseline好</p><p>优化点：</p><ul><li>主要关注 Cost token数量，</li><li>可以新的考虑，端到端的时间延迟，通过交流的并发性来优化，作为新的优化指标，线下可以调用大量，国产的卡（跑训练有问题，但是推理没问题），先在小模型上做测试，4o-mini做好实验代码</li></ul>',28)]))}const m=i(n,[["render",a],["__file","epic.html.vue"]]),d=JSON.parse(`{"path":"/ai/llm/epic.html","title":"EPIC Lab","lang":"en-US","frontmatter":{"description":"EPIC Lab 方向 在研 模型优化 LLM 大模型 扩散大语言模型 可以多模态 LLaDA-V，做 token 压缩，正在做这个刚开 DeepSeek-VL2，多模态 MoE 大模型 MoE 剪枝， VLM 多模态大模型，token pruning ViT+LLM，一般是压缩LLM，目前在考虑在ViT部分压缩，需要算力 KV Cache 已经做了三...","head":[["meta",{"property":"og:url","content":"https://davidliuk.github.io/blog/blog/ai/llm/epic.html"}],["meta",{"property":"og:site_name","content":"David's Blog"}],["meta",{"property":"og:title","content":"EPIC Lab"}],["meta",{"property":"og:description","content":"EPIC Lab 方向 在研 模型优化 LLM 大模型 扩散大语言模型 可以多模态 LLaDA-V，做 token 压缩，正在做这个刚开 DeepSeek-VL2，多模态 MoE 大模型 MoE 剪枝， VLM 多模态大模型，token pruning ViT+LLM，一般是压缩LLM，目前在考虑在ViT部分压缩，需要算力 KV Cache 已经做了三..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-08-17T07:52:28.000Z"}],["meta",{"property":"article:modified_time","content":"2025-08-17T07:52:28.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"EPIC Lab\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-08-17T07:52:28.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"David Liu\\",\\"url\\":\\"https://github.com/davidliuk\\"}]}"]]},"headers":[{"level":2,"title":"方向","slug":"方向","link":"#方向","children":[{"level":3,"title":"在研","slug":"在研","link":"#在研","children":[]},{"level":3,"title":"未来","slug":"未来","link":"#未来","children":[]}]}],"git":{"createdTime":1755417148000,"updatedTime":1755417148000,"contributors":[{"name":"dawei.liu","email":"dawei.liu@bytedance.com","commits":1}]},"readingTime":{"minutes":1.38,"words":415},"filePathRelative":"ai/llm/epic.md","localizedDate":"August 17, 2025","excerpt":"\\n<h2>方向</h2>\\n<h3>在研</h3>\\n<p>模型优化</p>\\n<ul>\\n<li>\\n<p>LLM 大模型</p>\\n<p>扩散大语言模型</p>\\n<p>可以多模态 LLaDA-V，做 token 压缩，正在做这个刚开</p>\\n<p>DeepSeek-VL2，多模态 MoE 大模型</p>\\n<p>MoE 剪枝，</p>\\n</li>\\n<li>\\n<p>VLM 多模态大模型，token pruning</p>\\n<p>ViT+LLM，一般是压缩LLM，目前在考虑在ViT部分压缩，需要算力</p>\\n<ul>\\n<li>KV Cache</li>\\n<li>已经做了三四篇了，最近做的人太多了😭不想硬卷换赛道</li>\\n<li>做专门领域的更好：Video, Steaming, <em>Audio</em>, UI Agent, 自动驾驶, <em>AI4Science</em> 方向的</li>\\n<li>找哪个场景</li>\\n</ul>\\n<p>ViT的压缩还有很大可以优化的点</p>\\n</li>\\n<li>\\n<p>文、视频生成</p>\\n</li>\\n</ul>","autoDesc":true}`);export{m as comp,d as data};
