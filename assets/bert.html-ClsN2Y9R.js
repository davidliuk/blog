import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as i,a as r,o as l}from"./app-B_TQ4tbw.js";const p={};function n(o,e){return l(),i("div",null,e[0]||(e[0]=[r('<h1 id="bert" tabindex="-1"><a class="header-anchor" href="#bert"><span>BERT</span></a></h1><p>NLP里的迁移学习</p><ul><li><p>使用预训练好的模型来抽取词、句子的特征</p><ul><li>例如word2vec或语言模型</li></ul></li><li><p>不更新预训练好的模型</p></li><li><p>需要构建新的网络来抓取新任务需要的信息</p><ul><li>Word2vec忽略了时序信息，语言模型只看了一个方向</li></ul></li><li><p>基于微调的NLP模型</p></li><li><p>预训练的模型抽取了足够多的信息</p></li><li><p>新的任务只需要增加一个简单的输出层</p></li></ul><hr><p>AE</p><p>没有Decoder的transformer</p><ul><li><p>只有编码器的Transformer</p></li><li><p>两个版本：</p><ul><li><p>Base:#blocks 12,hidden size 768,#heads 12,</p><p>#parameters 110M</p></li><li><p>Large:#blocks=24,hidden size 1024,#heads 1</p><p>#parameter=340M</p></li></ul></li><li><p>在大规模数据上训练&gt;3B词</p></li></ul><hr><p>每个样本是一个句子对<br> 加入额外的片段嵌入<br> 位置编码可学习</p><p>Position Embeddings<br> Segment Embeddings<br> Token Embeddings</p><p>Transfomer的编码器是双向，标准语言模型要求单向<br> 带掩码的语言模型每次随机（15%概率)将一些词元换成<br> &lt;mask&gt;</p>',11)]))}const m=t(p,[["render",n],["__file","bert.html.vue"]]),c=JSON.parse(`{"path":"/ai/llm/transformer/bert.html","title":"BERT","lang":"en-US","frontmatter":{"description":"BERT NLP里的迁移学习 使用预训练好的模型来抽取词、句子的特征 例如word2vec或语言模型 不更新预训练好的模型 需要构建新的网络来抓取新任务需要的信息 Word2vec忽略了时序信息，语言模型只看了一个方向 基于微调的NLP模型 预训练的模型抽取了足够多的信息 新的任务只需要增加一个简单的输出层 AE 没有Decoder的transform...","head":[["meta",{"property":"og:url","content":"https://davidliuk.github.io/blog/blog/ai/llm/transformer/bert.html"}],["meta",{"property":"og:site_name","content":"David's Blog"}],["meta",{"property":"og:title","content":"BERT"}],["meta",{"property":"og:description","content":"BERT NLP里的迁移学习 使用预训练好的模型来抽取词、句子的特征 例如word2vec或语言模型 不更新预训练好的模型 需要构建新的网络来抓取新任务需要的信息 Word2vec忽略了时序信息，语言模型只看了一个方向 基于微调的NLP模型 预训练的模型抽取了足够多的信息 新的任务只需要增加一个简单的输出层 AE 没有Decoder的transform..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-08-17T07:52:28.000Z"}],["meta",{"property":"article:modified_time","content":"2025-08-17T07:52:28.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"BERT\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-08-17T07:52:28.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"David Liu\\",\\"url\\":\\"https://github.com/davidliuk\\"}]}"]]},"headers":[],"git":{"createdTime":1755417148000,"updatedTime":1755417148000,"contributors":[{"name":"dawei.liu","email":"dawei.liu@bytedance.com","commits":1}]},"readingTime":{"minutes":0.81,"words":243},"filePathRelative":"ai/llm/transformer/bert.md","localizedDate":"August 17, 2025","excerpt":"\\n<p>NLP里的迁移学习</p>\\n<ul>\\n<li>\\n<p>使用预训练好的模型来抽取词、句子的特征</p>\\n<ul>\\n<li>例如word2vec或语言模型</li>\\n</ul>\\n</li>\\n<li>\\n<p>不更新预训练好的模型</p>\\n</li>\\n<li>\\n<p>需要构建新的网络来抓取新任务需要的信息</p>\\n<ul>\\n<li>Word2vec忽略了时序信息，语言模型只看了一个方向</li>\\n</ul>\\n</li>\\n<li>\\n<p>基于微调的NLP模型</p>\\n</li>\\n<li>\\n<p>预训练的模型抽取了足够多的信息</p>\\n</li>\\n<li>\\n<p>新的任务只需要增加一个简单的输出层</p>\\n</li>\\n</ul>","autoDesc":true}`);export{m as comp,c as data};
