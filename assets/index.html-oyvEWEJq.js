import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,a as l,o as t}from"./app-BZLta1H5.js";const o={};function a(r,i){return t(),n("div",null,[...i[0]||(i[0]=[l('<h1 id="large-language-model" tabindex="-1"><a class="header-anchor" href="#large-language-model"><span>Large Language Model</span></a></h1><p>Pre-train 预训练范式</p><ul><li>ARM (Auto-Regressive Model)</li><li>MDM (Masked Diffusion Model)</li><li>AE (Auto-Encoding)</li></ul><p><strong>大规模数据 + Transformer + 自监督学习 + 自回归/自编码目标 + 泛化能力优先</strong></p><ol><li>预训练（Pre-training）</li><li>后训练/对齐（post-training/Alignment） <ol><li>监督/指令微调（Supervised/Instruction Fine-Tuning）</li><li>人类偏好对齐/强化学习 （Alignment） <ol><li>RLHF</li><li>DPO</li><li>PPO (Proximal Policy Optimization) <ol><li>GRPO (Group Relative Policy Optimization)</li></ol></li><li>RLAIF <ol><li>Reward Model</li></ol></li></ol></li><li>对齐与安全机制（Alignment &amp; Safety Tuning）</li></ol></li></ol>',5)])])}const m=e(o,[["render",a]]),s=JSON.parse(`{"path":"/ai/gm/llm/","title":"Large Language Model","lang":"en-US","frontmatter":{"description":"Large Language Model Pre-train 预训练范式 ARM (Auto-Regressive Model) MDM (Masked Diffusion Model) AE (Auto-Encoding) 大规模数据 + Transformer + 自监督学习 + 自回归/自编码目标 + 泛化能力优先 预训练（Pre-trainin...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Large Language Model\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-12-12T20:39:51.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"David Liu\\",\\"url\\":\\"https://github.com/davidliuk\\"}]}"],["meta",{"property":"og:url","content":"https://davidliuk.github.io/blog/blog/ai/gm/llm/"}],["meta",{"property":"og:site_name","content":"David's Blog"}],["meta",{"property":"og:title","content":"Large Language Model"}],["meta",{"property":"og:description","content":"Large Language Model Pre-train 预训练范式 ARM (Auto-Regressive Model) MDM (Masked Diffusion Model) AE (Auto-Encoding) 大规模数据 + Transformer + 自监督学习 + 自回归/自编码目标 + 泛化能力优先 预训练（Pre-trainin..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-12-12T20:39:51.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-12T20:39:51.000Z"}]]},"git":{"createdTime":1765571991000,"updatedTime":1765571991000,"contributors":[{"name":"David Liu","username":"","email":"davidliu02k@gmail.com","commits":1}]},"readingTime":{"minutes":0.39,"words":117},"filePathRelative":"ai/gm/llm/README.md","excerpt":"\\n<p>Pre-train 预训练范式</p>\\n<ul>\\n<li>ARM (Auto-Regressive Model)</li>\\n<li>MDM (Masked Diffusion Model)</li>\\n<li>AE (Auto-Encoding)</li>\\n</ul>\\n<p><strong>大规模数据 + Transformer + 自监督学习 + 自回归/自编码目标 + 泛化能力优先</strong></p>\\n<ol>\\n<li>预训练（Pre-training）</li>\\n<li>后训练/对齐（post-training/Alignment）\\n<ol>\\n<li>监督/指令微调（Supervised/Instruction Fine-Tuning）</li>\\n<li>人类偏好对齐/强化学习 （Alignment）\\n<ol>\\n<li>RLHF</li>\\n<li>DPO</li>\\n<li>PPO (Proximal Policy Optimization)\\n<ol>\\n<li>GRPO (Group Relative Policy Optimization)</li>\\n</ol>\\n</li>\\n<li>RLAIF\\n<ol>\\n<li>Reward Model</li>\\n</ol>\\n</li>\\n</ol>\\n</li>\\n<li>对齐与安全机制（Alignment &amp; Safety Tuning）</li>\\n</ol>\\n</li>\\n</ol>","autoDesc":true}`);export{m as comp,s as data};
