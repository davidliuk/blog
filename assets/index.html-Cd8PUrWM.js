import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as i,a,o}from"./app-BtADw1TI.js";const n={};function r(l,e){return o(),i("div",null,[...e[0]||(e[0]=[a('<h1 id="diffusion-large-language-model" tabindex="-1"><a class="header-anchor" href="#diffusion-large-language-model"><span>Diffusion Large Language Model</span></a></h1><p>相比于 LLM AR(Auto-Regression)</p><p><a href="https://deepmind.google/models/gemini-diffusion/#what-is-a-diffusion-model" target="_blank" rel="noopener noreferrer">Gemini Diffusion</a></p><p><a href="https://ml-gsai.github.io/LLaDA-demo/" target="_blank" rel="noopener noreferrer">LLaDA</a></p><p>LLaDA-V</p><p><a href="https://github.com/Gen-Verse/MMaDA" target="_blank" rel="noopener noreferrer">MMaDA</a>, Princeton, SEED</p><p><a href="http://arxiv.org/abs/2505.22618" target="_blank" rel="noopener noreferrer">Fast-dllm</a>, NVIDIA：引入KV-Cache</p><p>速度很快，但是效果一般</p><p>Traditional autoregressive language models generate text one word – or token – at a time. This sequential process can be slow, and limit the quality and coherence of the output.</p><p>Diffusion models work differently. Instead of predicting text directly, they learn to generate outputs by refining noise, step-by-step. This means they can iterate on a solution very quickly and error correct during the generation process. This helps them excel at tasks like editing, including in the context of math and code.</p><p>缺点</p><ul><li>无 Cache</li><li>定长</li></ul><p>变长思路：Block Diffusion</p><hr><p>双向注意力机制，可以看到上下文，做修改是非常合适的场景</p><p>加速</p><ul><li>Cache</li><li>Sampling</li></ul><blockquote><p>损失少量精度，提高大量速度</p></blockquote><p>ARM</p><ul><li>量化，也是精度损失</li></ul><p>长序列</p><p>采样策略</p><ul><li><p>自回归</p></li><li><p>半自回归（Block Diffusion）</p></li></ul><p>LLaDA</p><p>问题：</p><ul><li>scaling</li><li>训练过程中很难scale长度</li></ul>',26)])])}const d=t(n,[["render",r]]),g=JSON.parse(`{"path":"/ai/gm/text/dllm/","title":"Diffusion Large Language Model","lang":"en-US","frontmatter":{"description":"Diffusion Large Language Model 相比于 LLM AR(Auto-Regression) Gemini Diffusion LLaDA LLaDA-V MMaDA, Princeton, SEED Fast-dllm, NVIDIA：引入KV-Cache 速度很快，但是效果一般 Traditional autoregress...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Diffusion Large Language Model\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-12-15T23:09:49.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"David Liu\\",\\"url\\":\\"https://github.com/davidliuk\\"}]}"],["meta",{"property":"og:url","content":"https://davidliuk.github.io/blog/blog/ai/gm/text/dllm/"}],["meta",{"property":"og:site_name","content":"David's Blog"}],["meta",{"property":"og:title","content":"Diffusion Large Language Model"}],["meta",{"property":"og:description","content":"Diffusion Large Language Model 相比于 LLM AR(Auto-Regression) Gemini Diffusion LLaDA LLaDA-V MMaDA, Princeton, SEED Fast-dllm, NVIDIA：引入KV-Cache 速度很快，但是效果一般 Traditional autoregress..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-12-15T23:09:49.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-15T23:09:49.000Z"}]]},"git":{"createdTime":1765840189000,"updatedTime":1765840189000,"contributors":[{"name":"David Liu","username":"","email":"davidliu02k@gmail.com","commits":1}]},"readingTime":{"minutes":0.75,"words":226},"filePathRelative":"ai/gm/text/dllm/README.md","excerpt":"\\n<p>相比于 LLM AR(Auto-Regression)</p>\\n<p><a href=\\"https://deepmind.google/models/gemini-diffusion/#what-is-a-diffusion-model\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Gemini Diffusion</a></p>\\n<p><a href=\\"https://ml-gsai.github.io/LLaDA-demo/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">LLaDA</a></p>\\n<p>LLaDA-V</p>","autoDesc":true}`);export{d as comp,g as data};
