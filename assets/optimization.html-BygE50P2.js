import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as e,a as o,o as i}from"./app-CGXHKXsa.js";const r={};function a(d,t){return i(),e("div",null,t[0]||(t[0]=[o('<h1 id="optimization" tabindex="-1"><a class="header-anchor" href="#optimization"><span>Optimization</span></a></h1><p>Auto-Regression: KV-Cache</p><table><thead><tr><th>优化项</th><th>解释</th><th>带来的好处</th></tr></thead><tbody><tr><td><strong>Flash Attention</strong></td><td>利用 CUDA kernel 优化 Softmax-attention 的矩阵乘法，减少内存读写</td><td>更快、更省内存，适用于长序列</td></tr><tr><td><strong>Rotary Positional Encoding (RoPE)</strong></td><td>替代传统位置编码，兼容 KV 缓存并支持无限延展</td><td>提升泛化能力</td></tr><tr><td><strong>Quantization (INT8, FP8, etc.)</strong></td><td>用低精度浮点数代替 FP16/FP32</td><td>节省显存，提升吞吐量</td></tr><tr><td><strong>LoRA / QLoRA</strong></td><td>微调时只训练小矩阵，冻结大部分参数</td><td>更轻量、高效微调</td></tr><tr><td><strong>Weight Tying / Sharing</strong></td><td>Embedding 和输出层共享参数</td><td>降低模型大小</td></tr><tr><td><strong>Prefix Caching / Prefill</strong></td><td>将一段提示（prompt）预编码并缓存</td><td>Chat 场景中节省大量计算（典型如长系统提示）</td></tr></tbody></table><h2 id="推理" tabindex="-1"><a class="header-anchor" href="#推理"><span>推理</span></a></h2><ul><li><p>量化</p></li><li><p>缓存</p><ul><li>KV Cache</li></ul></li><li><p>剪枝</p><ul><li>Token Pruning</li><li>Model Pruning <ul><li>To prune, or not to prune, Google</li></ul></li></ul></li></ul><h3 id="token-pruning" tabindex="-1"><a class="header-anchor" href="#token-pruning"><span>Token Pruning</span></a></h3><p>好处，无需训练</p><p>dart=<a href="https://github.com/ZichenWen1/DART" target="_blank" rel="noopener noreferrer">https://github.com/ZichenWen1/DART</a></p>',8)]))}const g=n(r,[["render",a],["__file","optimization.html.vue"]]),s=JSON.parse(`{"path":"/ai/gm/llm/optimization.html","title":"Optimization","lang":"en-US","frontmatter":{"description":"Optimization Auto-Regression: KV-Cache 推理 量化 缓存 KV Cache 剪枝 Token Pruning Model Pruning To prune, or not to prune, Google Token Pruning 好处，无需训练 dart=https://github.com/ZichenWen...","head":[["meta",{"property":"og:url","content":"https://davidliuk.github.io/blog/blog/ai/gm/llm/optimization.html"}],["meta",{"property":"og:site_name","content":"David's Blog"}],["meta",{"property":"og:title","content":"Optimization"}],["meta",{"property":"og:description","content":"Optimization Auto-Regression: KV-Cache 推理 量化 缓存 KV Cache 剪枝 Token Pruning Model Pruning To prune, or not to prune, Google Token Pruning 好处，无需训练 dart=https://github.com/ZichenWen..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-10-26T05:16:21.000Z"}],["meta",{"property":"article:modified_time","content":"2025-10-26T05:16:21.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Optimization\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-10-26T05:16:21.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"David Liu\\",\\"url\\":\\"https://github.com/davidliuk\\"}]}"]]},"headers":[{"level":2,"title":"推理","slug":"推理","link":"#推理","children":[{"level":3,"title":"Token Pruning","slug":"token-pruning","link":"#token-pruning","children":[]}]}],"git":{"createdTime":1755417148000,"updatedTime":1761455781000,"contributors":[{"name":"dawei.liu","email":"dawei.liu@bytedance.com","commits":1}]},"readingTime":{"minutes":0.71,"words":212},"filePathRelative":"ai/gm/llm/optimization.md","localizedDate":"August 17, 2025","excerpt":"\\n<p>Auto-Regression: KV-Cache</p>\\n<table>\\n<thead>\\n<tr>\\n<th>优化项</th>\\n<th>解释</th>\\n<th>带来的好处</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td><strong>Flash Attention</strong></td>\\n<td>利用 CUDA kernel 优化 Softmax-attention 的矩阵乘法，减少内存读写</td>\\n<td>更快、更省内存，适用于长序列</td>\\n</tr>\\n<tr>\\n<td><strong>Rotary Positional Encoding (RoPE)</strong></td>\\n<td>替代传统位置编码，兼容 KV 缓存并支持无限延展</td>\\n<td>提升泛化能力</td>\\n</tr>\\n<tr>\\n<td><strong>Quantization (INT8, FP8, etc.)</strong></td>\\n<td>用低精度浮点数代替 FP16/FP32</td>\\n<td>节省显存，提升吞吐量</td>\\n</tr>\\n<tr>\\n<td><strong>LoRA / QLoRA</strong></td>\\n<td>微调时只训练小矩阵，冻结大部分参数</td>\\n<td>更轻量、高效微调</td>\\n</tr>\\n<tr>\\n<td><strong>Weight Tying / Sharing</strong></td>\\n<td>Embedding 和输出层共享参数</td>\\n<td>降低模型大小</td>\\n</tr>\\n<tr>\\n<td><strong>Prefix Caching / Prefill</strong></td>\\n<td>将一段提示（prompt）预编码并缓存</td>\\n<td>Chat 场景中节省大量计算（典型如长系统提示）</td>\\n</tr>\\n</tbody>\\n</table>","autoDesc":true}`);export{g as comp,s as data};
