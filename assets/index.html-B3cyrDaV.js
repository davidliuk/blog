import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as o,a as i,o as l}from"./app-BBRqDuqm.js";const t={};function r(a,e){return l(),o("div",null,e[0]||(e[0]=[i('<h1 id="inference" tabindex="-1"><a class="header-anchor" href="#inference"><span>Inference</span></a></h1><h2 id="推理过程" tabindex="-1"><a class="header-anchor" href="#推理过程"><span>推理过程</span></a></h2><ul><li><p>Prefill</p><ul><li><p>根据输入Tokens生成第一个输出Token（A），通过一次Forward就可L以完成</p></li><li><p>在Forward中，输入Tokens间可以并行执行，因此执行效率很高</p></li></ul></li><li><p>Decoding:</p><ul><li>从生成第一个Token后，采用自回归一次生成一个Token，直到生成StopToken结束</li><li>设输出共N x Token，Decoding阶段需要执行N-1次Forward，只能串行执行，效率很低</li><li>在生成过程中，需要关注Token越来越多，计算量也会适当增大</li></ul></li></ul><p>KV Cache</p><ul><li>把每个 token 在过Transformer 时乘以Wk，Wv,这俩参数矩阵的结果缓存下来，训l练的时候不需要保存</li><li>推理解码生成时采用自回归auto-regressive方式，即每次生成一个token，都要依赖之前token的结果</li><li>如果每生成一个 token 时候乘以 Wk，W, 这俩参数矩阵要对所有 token 都算一遍，代价非常大，所以缓存起来就叫KVCache</li></ul><p>KV Cache:Decode</p><p>从上述过程中，可以发现KV cache推理时特点：</p><ol><li>随着prompt数量变多和序列变长，KV cache也变大，对GPU显存造成压力</li><li>由于输出的序列长度无法预先知道，所以很难提前为KV cache量身定制存储空间</li></ol><h2 id="瓶颈分析" tabindex="-1"><a class="header-anchor" href="#瓶颈分析"><span>瓶颈分析</span></a></h2><p>LLM推理Prefill&amp;Decoding阶段Roofline Model近似如下，其中：</p><ol><li>三角Prefill：设Batch size=l,Sequence Length越大计算强度越大，通常属于Compute Bound</li><li>原型Decoding：Batch size越大，计算强度越大，理论性能峰值越大，通常属于Memory Bound</li></ol>',11)]))}const d=n(t,[["render",r],["__file","index.html.vue"]]),s=JSON.parse(`{"path":"/ai/gm/llm/inference/","title":"Inference","lang":"en-US","frontmatter":{"description":"Inference 推理过程 Prefill 根据输入Tokens生成第一个输出Token（A），通过一次Forward就可L以完成 在Forward中，输入Tokens间可以并行执行，因此执行效率很高 Decoding: 从生成第一个Token后，采用自回归一次生成一个Token，直到生成StopToken结束 设输出共N x Token，Decod...","head":[["meta",{"property":"og:url","content":"https://davidliuk.github.io/blog/blog/ai/gm/llm/inference/"}],["meta",{"property":"og:site_name","content":"David's Blog"}],["meta",{"property":"og:title","content":"Inference"}],["meta",{"property":"og:description","content":"Inference 推理过程 Prefill 根据输入Tokens生成第一个输出Token（A），通过一次Forward就可L以完成 在Forward中，输入Tokens间可以并行执行，因此执行效率很高 Decoding: 从生成第一个Token后，采用自回归一次生成一个Token，直到生成StopToken结束 设输出共N x Token，Decod..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-10-26T05:16:21.000Z"}],["meta",{"property":"article:modified_time","content":"2025-10-26T05:16:21.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Inference\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-10-26T05:16:21.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"David Liu\\",\\"url\\":\\"https://github.com/davidliuk\\"}]}"]]},"headers":[{"level":2,"title":"推理过程","slug":"推理过程","link":"#推理过程","children":[]},{"level":2,"title":"瓶颈分析","slug":"瓶颈分析","link":"#瓶颈分析","children":[]}],"git":{"createdTime":1755417148000,"updatedTime":1761455781000,"contributors":[{"name":"dawei.liu","email":"dawei.liu@bytedance.com","commits":1}]},"readingTime":{"minutes":1.32,"words":397},"filePathRelative":"ai/gm/llm/inference/README.md","localizedDate":"August 17, 2025","excerpt":"\\n<h2>推理过程</h2>\\n<ul>\\n<li>\\n<p>Prefill</p>\\n<ul>\\n<li>\\n<p>根据输入Tokens生成第一个输出Token（A），通过一次Forward就可L以完成</p>\\n</li>\\n<li>\\n<p>在Forward中，输入Tokens间可以并行执行，因此执行效率很高</p>\\n</li>\\n</ul>\\n</li>\\n<li>\\n<p>Decoding:</p>\\n<ul>\\n<li>从生成第一个Token后，采用自回归一次生成一个Token，直到生成StopToken结束</li>\\n<li>设输出共N x Token，Decoding阶段需要执行N-1次Forward，只能串行执行，效率很低</li>\\n<li>在生成过程中，需要关注Token越来越多，计算量也会适当增大</li>\\n</ul>\\n</li>\\n</ul>","autoDesc":true}`);export{d as comp,s as data};
