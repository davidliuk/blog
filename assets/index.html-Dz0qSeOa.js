import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,a as n,o as t}from"./app-BtADw1TI.js";const l={};function e(p,s){return t(),a("div",null,[...s[0]||(s[0]=[n(`<h1 id="deep-learning" tabindex="-1"><a class="header-anchor" href="#deep-learning"><span>Deep Learning</span></a></h1><ul><li><strong>传统 ML</strong>：重在<strong>特征工程 (Feature Engineering)</strong>。人工提取特征（如边缘、纹理、统计量），模型负责分类/回归。</li><li><strong>DL</strong>：重在<strong>表征学习 (Representation Learning)</strong>。端到端 (End-to-End) 学习，模型自动从原始数据（像素、文本）中提取高维特征。</li></ul><h2 id="neural-network" tabindex="-1"><a class="header-anchor" href="#neural-network"><span>Neural Network</span></a></h2><p>深度学习（deep learning）是机器学习的分支，是一种以人工神经网络为结构，对数据进行<strong>表征学习</strong>的算法。深度学习中的形容词“深度”是指在网络中使用多层。 早期的工作表明，线性感知器不能成为通用分类器，但具有非多项式激活函数和一个无限宽度隐藏层的网络可以成为通用分类器。</p><p>将1974年提出的标准<strong>反向传播</strong>算法应用于深度神经网络</p><p>Deep Learning 主要处理<strong>非结构化数据</strong>（图像、文本、语音）。</p><ul><li>AutoDiff <ul><li>is a tool for computing gradients of a differentiable function, b = f(a)</li><li>the key building block is a module with a forward() and backward()</li><li>sometimes define f as code in forward() by chaining existing modules together</li></ul></li><li>Computation Graphs <ul><li>are another way to define f (more conducive to slides)</li><li>so far, we saw two (deep) computation graphs <ul><li>RNN-LM</li><li>Transformer-LM</li><li>(Transformer-LM was kind of complicated)</li></ul></li></ul></li></ul><h3 id="components" tabindex="-1"><a class="header-anchor" href="#components"><span>Components</span></a></h3><ul><li><strong>神经元 (Perceptron)</strong>： <ul><li>线性变换 + 非线性激活 (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mi>x</mi><mo>+</mo><mi>b</mi><mo>→</mo><mi>σ</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Wx+b \\rightarrow \\sigma(\\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span>)。</li></ul></li><li><strong>激活函数</strong>： <ul><li>ReLU (首选),</li><li>Sigmoid (二分类输出),</li><li>Softmax (多分类输出),</li><li>Tanh</li></ul></li><li><strong>反向传播 (Backpropagation)</strong>：基于链式法则，将 Loss 对参数的梯度传回前面各层。</li></ul><h3 id="architectures" tabindex="-1"><a class="header-anchor" href="#architectures"><span>Architectures</span></a></h3><ul><li><strong>CNN (卷积神经网络)</strong>： <ul><li><strong>核心</strong>：<strong>卷积核 (Kernel)</strong> 提取局部特征，<strong>池化 (Pooling)</strong> 降低维度。</li><li><strong>特性</strong>：<strong>平移不变性</strong> (猫在图左边和右边都能识别)。</li><li><strong>场景</strong>：CV（图像分类、检测）、时间序列。</li></ul></li><li><strong>RNN (循环神经网络)</strong> / LSTM / GRU： <ul><li><strong>核心</strong>：引入<strong>状态 (State)</strong>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h_t = f(x_t, h_{t-1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，具有记忆功能。</li><li><strong>缺陷</strong>：难以并行，长序列存在梯度消失。</li><li><strong>场景</strong>：NLP、语音、时序数据。</li></ul></li><li><strong>Transformer</strong> (当前 SOTA)： <ul><li><strong>核心</strong>：<strong>Self-Attention (自注意力机制)</strong>。抛弃了循环，完全基于注意力并行计算。</li><li><strong>公式</strong>：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.6275em;vertical-align:-0.538em;"></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0895em;"><span style="top:-2.5864em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8622em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mtight" style="padding-left:0.833em;"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8222em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1778em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">Q</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9191em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.538em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>。</li><li><strong>场景</strong>：NLP (BERT, GPT), CV (ViT), 多模态。</li></ul></li></ul><h3 id="the-alchemy" tabindex="-1"><a class="header-anchor" href="#the-alchemy"><span>The &quot;Alchemy&quot;</span></a></h3><ul><li><strong>损失函数 (Loss Function)</strong>： <ul><li><strong>MSE</strong>：回归问题。</li><li><strong>Cross-Entropy (交叉熵)</strong>：分类问题。</li></ul></li><li><strong>优化器 (Optimizer)</strong>： <ul><li><strong>SGD</strong>：随机梯度下降，震荡大。</li><li><strong>Adam</strong>：自适应学习率 + 动量，目前最常用的默认选择。</li></ul></li><li><strong>归一化 (Normalization)</strong>： <ul><li><strong>Batch Norm (BN)</strong>：对一个 Batch 做归一化，用于 CNN。</li><li><strong>Layer Norm (LN)</strong>：对单样本的所有特征做归一化，用于 RNN/Transformer。</li></ul></li><li><strong>Dropout</strong>：随机让一部分神经元“下班”，防止过拟合。</li></ul><h2 id="architectures-1" tabindex="-1"><a class="header-anchor" href="#architectures-1"><span>Architectures</span></a></h2><p>参考</p><ul><li>李沐-动手学机器学习</li><li>李宏毅</li></ul><h3 id="enc-dec" tabindex="-1"><a class="header-anchor" href="#enc-dec"><span>Enc-Dec</span></a></h3><p>编码器-解码器</p><ul><li>编码器处理输入：将输入编程成中间表达形式（特征）</li><li>解码器生成输出：将中间表示解码成输出</li></ul><p><img src="https://raw.githubusercontent.com/davidliuk/images/master/image-20250610093210662.png" alt="image-20250610093210662" loading="lazy"></p><p>Eg</p><h3 id="cnn" tabindex="-1"><a class="header-anchor" href="#cnn"><span>CNN</span></a></h3><ul><li>编码器：将输入编程成中间表达形式（特征）</li><li>解码器：将中间表示解码成输出</li></ul><p><img src="https://gcore.jsdelivr.net/gh/davidliuk/images@master/image-20250610100427965.png" alt="image-20250610100427965" loading="lazy"></p><h3 id="rnn" tabindex="-1"><a class="header-anchor" href="#rnn"><span>RNN</span></a></h3><ul><li>编码器：将文本表示成向量</li><li>解码器：向量表示成输出</li></ul><p><img src="https://gcore.jsdelivr.net/gh/davidliuk/images@master/image-20250610100607564.png" alt="image-20250610100607564" loading="lazy"></p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">class</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;"> Encoder</span><span style="--shiki-light:#C18401;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;">nn</span><span style="--shiki-light:#C18401;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;">Module</span><span style="--shiki-light:#C18401;--shiki-dark:#ABB2BF;">)</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">  	def</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> __init__</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E5C07B;--shiki-dark-font-style:italic;">self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> **</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">kwargs</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">    		super</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(Encoder, </span><span style="--shiki-light:#E45649;--shiki-dark:#E5C07B;">self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">).</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">__init__</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(**kwargs)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        </span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    def</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> forward</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E5C07B;--shiki-dark-font-style:italic;">self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;"> X</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> *</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">args</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">        return</span><span style="--shiki-light:#0184BC;--shiki-dark:#ABB2BF;"> NotImplementedError</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">      </span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">class</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;"> Decoder</span><span style="--shiki-light:#C18401;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;">nn</span><span style="--shiki-light:#C18401;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;">Module</span><span style="--shiki-light:#C18401;--shiki-dark:#ABB2BF;">)</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">  	def</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> __init__</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E5C07B;--shiki-dark-font-style:italic;">self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> **</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">kwargs</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">    		super</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(Decoder, </span><span style="--shiki-light:#E45649;--shiki-dark:#E5C07B;">self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">).</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">__init__</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(**kwargs)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        </span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    def</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> init_state</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E5C07B;--shiki-dark-font-style:italic;">self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;"> enc_outputs</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> *</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">args</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">        return</span><span style="--shiki-light:#0184BC;--shiki-dark:#ABB2BF;"> NotImplementedError</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    </span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    def</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> forward</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E5C07B;--shiki-dark-font-style:italic;">self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;"> X</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> *</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">args</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">        return</span><span style="--shiki-light:#0184BC;--shiki-dark:#ABB2BF;"> NotImplementedError</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">      </span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">class</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;"> EncoderDecoder</span><span style="--shiki-light:#C18401;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;">nn</span><span style="--shiki-light:#C18401;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#C18401;--shiki-dark:#E5C07B;">Module</span><span style="--shiki-light:#C18401;--shiki-dark:#ABB2BF;">)</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">:</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">  	def</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;"> __init__</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E5C07B;--shiki-dark-font-style:italic;">self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;"> encoder</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-dark:#ABB2BF;"> decoder </span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">**</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">kwargs</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">    		super</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(Encoder, </span><span style="--shiki-light:#E45649;--shiki-dark:#E5C07B;">self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">).</span><span style="--shiki-light:#0184BC;--shiki-dark:#56B6C2;">__init__</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(**kwargs)</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E5C07B;">        self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.encoder </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> encoder</span></span>
<span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E5C07B;">        self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.decoder </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> decoder</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        </span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">    def</span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;"> forward</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E5C07B;--shiki-dark-font-style:italic;">self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;"> enc_X</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;"> dec_X</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">,</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> *</span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#D19A66;--shiki-dark-font-style:italic;">args</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">):</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        enc_outputs </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#E45649;--shiki-dark:#E5C07B;"> self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">encoder</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(enc_X, *args)</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">        dec_state </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#E45649;--shiki-dark:#E5C07B;"> self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.decoder.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">init_state</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(enc_outputs, *args)</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">        return</span><span style="--shiki-light:#E45649;--shiki-dark:#E5C07B;"> self</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">decoder</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(dec_X, dec_state)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="problem" tabindex="-1"><a class="header-anchor" href="#problem"><span>Problem</span></a></h2><p>梯度消失、梯度爆炸</p><p>resnet：残差连接</p><table><thead><tr><th><strong>特性</strong></th><th><strong>梯度消失 (Vanishing)</strong></th><th><strong>梯度爆炸 (Exploding)</strong></th></tr></thead><tbody><tr><td><strong>现象</strong></td><td>前层参数不更新，模型无法收敛，像个浅层模型</td><td>Loss 变成 NaN，Loss 剧烈震荡，权重数值巨大</td></tr><tr><td><strong>高发区</strong></td><td>深层网络 + Sigmoid/Tanh</td><td>RNN，深层网络，初始化不当</td></tr><tr><td><strong>核心解法</strong></td><td><strong>ReLU</strong>, <strong>ResNet</strong>, Batch Norm</td><td><strong>Gradient Clipping</strong>, 权重正则化</td></tr></tbody></table><p>激活函数（Activation Function）</p><p>常见的激活函数都是非线性的，因此也会给神经元引入非线性元素，使得神经网络可以逼近其他的任何非线性函数，这样可以使得神经网络应用到更多非线性模型中。</p><ul><li><strong>线性激活函数</strong>（线性方程控制输入到输出的映射，如f(x)=x等）</li><li><strong>非线性激活函数</strong>（非线性方程控制输入到输出的映射，比如 <ul><li>Sigmoid</li><li>Tanh</li><li>ReLU <ul><li>LReLU</li><li>PReLU</li><li>Swish</li></ul></li><li>Softmax 是用于多类分类问题的激活函数</li></ul></li></ul><p>attention</p><p>CNN</p><p>RNN</p><p>seq2seq</p><ul><li>LSTM架构</li></ul><p>Transformer</p><ul><li>Bert</li><li>GPT</li></ul><p>Transformer是更广义的MLP， 其中一个原因是它在处理input的时候，创造性的引入了query, key和value的概念。而CNN，RNN又属于MLP的特殊情况。GNN感觉则是CNN在图网络结构的一种应用而已。所以Transformer看着像CNN, RNN, GNN, MLP.</p><p>这个问题现在争议不大了吧，我自己遵守的一条好用的黄金法则是</p><p>有大量数据，用transformer；允许pretrain，用transformer；小数据from scratch，分类用CNN，标注和生成用RNN。</p><p>对于上述情况的中间地带，看个人调参能力吧，个人实践经验来说，哪个调的用心哪个就更好一些，喜欢无脑套默认超参的还是RNN更容易上性能，不过是真的慢。</p><hr><p>我们首先区分一个概念：</p><p>广义Transformer和狭义Transformer，</p><ul><li>狭义Transformer指Attention is all you need那个结构，包含一个encoder一个decoder；</li><li>广义Transformer指self-attention机制的各种应用。</li></ul><p>下面是我个人一些理解，感觉可能有不对或者不完整的</p><ol><li><p>狭义Transformer在大数据的情况下超过RNN一点问题没有，而且是显著的超，目前在很多大数据任务都验证了，比如WMT数据集的机器翻译，BERT，以及最近的TTS。</p></li><li><p>狭义的Transformer在跑中等数据集（一百万）或者小数据（几万或者几千）和RNN谁好谁坏不一定，可能靠调，不是那种一个默认参数就会比RNN好，我简单的跑过中等数据集的对话（Twitter）和摘要（CNN DailyMail），感觉没好多少。小数据反而更容易过拟合，按照一些paper报的结果反而RNN好</p></li><li><p>广义的Transformer又是另一个故事了，我个人理解self attention机制几乎在所有任务上好好调都有用，比如分类，阅读理解（QANet），小规模s2s（Universal Transformer）。比如QAnet和Universal Transformer大家如果有兴趣可以看一看，通过自己的Recurrent机制，能给性能带来很大的增益。Multi head attention和简单的Self attention都是很有用的结构上的trick</p></li><li><p>我忘了是ACL的哪篇文章了，做过一个实验。RNN对于50个词之前的词顺序就不敏感了，而对于100个词之前的就完全忘了，所以RNN在长序列上肯定是有问题的。不过Transformer的位置信息只靠Position Embedding，我个人觉得可能也有提升空间。</p></li></ol><hr><p>主要特点是：</p><ol><li>非顺序处理：句子是整体处理，而不是逐字处理</li><li>单个的Transformer Block主要由<strong>两部分</strong>组成： <ol><li>多头注意力机制(Multi-Head Attention)</li><li>前馈神经网络(Feed Forward)，</li></ol></li><li>Transformer Block代替了LSTM和CNN结构作为了我们的特征提取器，使得Transformer不依赖于过去的隐藏状态来捕获对先前单词的依赖性，而是整体上处理一个句子，以便允许并行计算，减少训练时间，并减少由于长期依赖性而导致的性能下降</li></ol><h3 id="gnn" tabindex="-1"><a class="header-anchor" href="#gnn"><span>GNN</span></a></h3><p>MPNN</p><p>Random-walk</p>`,58)])])}const m=i(l,[["render",e]]),o=JSON.parse(`{"path":"/ai/gm/foundations/dl/","title":"Deep Learning","lang":"en-US","frontmatter":{"description":"Deep Learning 传统 ML：重在特征工程 (Feature Engineering)。人工提取特征（如边缘、纹理、统计量），模型负责分类/回归。 DL：重在表征学习 (Representation Learning)。端到端 (End-to-End) 学习，模型自动从原始数据（像素、文本）中提取高维特征。 Neural Network 深度...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Deep Learning\\",\\"image\\":[\\"https://raw.githubusercontent.com/davidliuk/images/master/image-20250610093210662.png\\",\\"https://gcore.jsdelivr.net/gh/davidliuk/images@master/image-20250610100427965.png\\",\\"https://gcore.jsdelivr.net/gh/davidliuk/images@master/image-20250610100607564.png\\"],\\"dateModified\\":\\"2025-12-15T23:09:49.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"David Liu\\",\\"url\\":\\"https://github.com/davidliuk\\"}]}"],["meta",{"property":"og:url","content":"https://davidliuk.github.io/blog/blog/ai/gm/foundations/dl/"}],["meta",{"property":"og:site_name","content":"David's Blog"}],["meta",{"property":"og:title","content":"Deep Learning"}],["meta",{"property":"og:description","content":"Deep Learning 传统 ML：重在特征工程 (Feature Engineering)。人工提取特征（如边缘、纹理、统计量），模型负责分类/回归。 DL：重在表征学习 (Representation Learning)。端到端 (End-to-End) 学习，模型自动从原始数据（像素、文本）中提取高维特征。 Neural Network 深度..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://raw.githubusercontent.com/davidliuk/images/master/image-20250610093210662.png"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-12-15T23:09:49.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-15T23:09:49.000Z"}]]},"git":{"createdTime":1765840189000,"updatedTime":1765840189000,"contributors":[{"name":"David Liu","username":"","email":"davidliu02k@gmail.com","commits":1}]},"readingTime":{"minutes":6.29,"words":1886},"filePathRelative":"ai/gm/foundations/dl/README.md","excerpt":"\\n<ul>\\n<li><strong>传统 ML</strong>：重在<strong>特征工程 (Feature Engineering)</strong>。人工提取特征（如边缘、纹理、统计量），模型负责分类/回归。</li>\\n<li><strong>DL</strong>：重在<strong>表征学习 (Representation Learning)</strong>。端到端 (End-to-End) 学习，模型自动从原始数据（像素、文本）中提取高维特征。</li>\\n</ul>\\n<h2>Neural Network</h2>\\n<p>深度学习（deep learning）是机器学习的分支，是一种以人工神经网络为结构，对数据进行<strong>表征学习</strong>的算法。深度学习中的形容词“深度”是指在网络中使用多层。 早期的工作表明，线性感知器不能成为通用分类器，但具有非多项式激活函数和一个无限宽度隐藏层的网络可以成为通用分类器。</p>","autoDesc":true}`);export{m as comp,o as data};
