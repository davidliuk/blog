const t=JSON.parse(`{"key":"v-30b1f453","path":"/ai/nn/Untitled.html","title":"","lang":"en-US","frontmatter":{"description":"resnet 残差连接 激活函数（Activation Function） 常见的激活函数都是非线性的，因此也会给神经元引入非线性元素，使得神经网络可以逼近其他的任何非线性函数，这样可以使得神经网络应用到更多非线性模型中。 线性激活函数（线性方程控制输入到输出的映射，如f(x)=x等） 非线性激活函数（非线性方程控制输入到输出的映射，比如 Sigmoid、 Tanh、 ReLU、 LReLU PReLU Swish Softmax 是用于多类分类问题的激活函数","head":[["meta",{"property":"og:url","content":"https://davidliuk.github.io/blog/blog/ai/nn/Untitled.html"}],["meta",{"property":"og:site_name","content":"David's Blog"}],["meta",{"property":"og:description","content":"resnet 残差连接 激活函数（Activation Function） 常见的激活函数都是非线性的，因此也会给神经元引入非线性元素，使得神经网络可以逼近其他的任何非线性函数，这样可以使得神经网络应用到更多非线性模型中。 线性激活函数（线性方程控制输入到输出的映射，如f(x)=x等） 非线性激活函数（非线性方程控制输入到输出的映射，比如 Sigmoid、 Tanh、 ReLU、 LReLU PReLU Swish Softmax 是用于多类分类问题的激活函数"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2024-01-08T03:06:14.000Z"}],["meta",{"property":"article:author","content":"David Liu"}],["meta",{"property":"article:modified_time","content":"2024-01-08T03:06:14.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2024-01-08T03:06:14.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"David Liu\\",\\"url\\":\\"https://github.com/davidliuk\\"}]}"]]},"headers":[],"git":{"createdTime":1704683174000,"updatedTime":1704683174000,"contributors":[{"name":"davidliu","email":"liudawei47@jd.com","commits":1}]},"readingTime":{"minutes":0.5,"words":151},"filePathRelative":"ai/nn/Untitled.md","localizedDate":"January 8, 2024","excerpt":"<p>resnet</p>\\n<p>残差连接</p>\\n<p>激活函数（Activation Function）</p>\\n<p>常见的激活函数都是非线性的，因此也会给神经元引入非线性元素，使得神经网络可以逼近其他的任何非线性函数，这样可以使得神经网络应用到更多非线性模型中。</p>\\n<ul>\\n<li><strong>线性激活函数</strong>（线性方程控制输入到输出的映射，如f(x)=x等）</li>\\n<li><strong>非线性激活函数</strong>（非线性方程控制输入到输出的映射，比如\\n<ul>\\n<li>Sigmoid、</li>\\n<li>Tanh、</li>\\n<li>ReLU、\\n<ul>\\n<li>LReLU</li>\\n<li>PReLU</li>\\n<li>Swish</li>\\n</ul>\\n</li>\\n<li>Softmax 是用于多类分类问题的激活函数</li>\\n</ul>\\n</li>\\n</ul>","autoDesc":true}`);export{t as data};
