import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,a as o,o as l}from"./app-BtADw1TI.js";const t={};function a(r,e){return l(),n("div",null,[...e[0]||(e[0]=[o('<h1 id="generative-models" tabindex="-1"><a class="header-anchor" href="#generative-models"><span>Generative Models</span></a></h1><ul><li>GAN (Generative Adversarial Networks)</li><li>VAE (Variational Autoencoder)</li><li>Diffusion Models</li><li>ARM (Autoregressive Models)</li></ul><p>The basic goal of AI is to develop intelligent machines.</p><p>This consists of many sub-goals:</p><ul><li>• Perception <ul><li>Multimodal foundation models learn to answer questions about images (and text in images)</li><li>Diffusion models can be used as zero-shot classifiers</li></ul></li><li>• Reasoning <ul><li>LLMs are also (unexpectedly) good at certain reasoning tasks</li><li>cf. Chain-of-Though Prompting (an ex. of in-context learning)</li></ul></li><li>• Control / Motion / Manipulation <ul><li>DayDreamer learns a generative model of experiences for RL, i.e. a World Model, without simulation</li><li>Quadruped robot learns to walk in under 1 hour</li></ul></li><li>• Planning <ul><li>LLMs are already being used for grounded planning for embodied agents, c.f. LLMPlanner</li></ul></li><li>• Communication <ul><li>Communication comprises the comprehension and generation of human language.</li><li>Large language models (LLMs) excel at both</li><li>(Even though they are most often trained autoregressively, i.e. to generate a next word, given the previous ones)</li></ul></li><li>• Creativity <ul><li>Text-to-image models [Midjourney’s Discord server has 18 million members (1.7 million were online this morning)]</li><li>Text-to-music models [MusicGen capable of conditioning on text and audio sample]</li></ul></li><li>• Learning</li></ul><p>Q: What does Generative AI have to do with any of these goals?</p><p>A: It’s making in-roads into all of them.</p><hr><p>ARM: next token prediction</p><p>Text-to-image</p><ul><li>Diffusion</li><li>flow</li></ul><p>过时的：GAN</p><p>normalizing flow</p><p>基于 masked 掩码方式的离散扩散模型</p><p>next token prediction: ARM</p><p>next frequency prediction: DM</p><p>next &quot;set of token&quot; prediction: MaskGiT</p><p>VAR</p><p>损失函数分离</p><ul><li>基于对抗均衡 GAN</li><li>交叉熵：ARM</li></ul>',20)])])}const u=i(t,[["render",a]]),m=JSON.parse(`{"path":"/ai/gm/","title":"Generative Models","lang":"en-US","frontmatter":{"description":"Generative Models GAN (Generative Adversarial Networks) VAE (Variational Autoencoder) Diffusion Models ARM (Autoregressive Models) The basic goal of AI is to develop intelligent...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Generative Models\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-12-15T23:09:49.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"David Liu\\",\\"url\\":\\"https://github.com/davidliuk\\"}]}"],["meta",{"property":"og:url","content":"https://davidliuk.github.io/blog/blog/ai/gm/"}],["meta",{"property":"og:site_name","content":"David's Blog"}],["meta",{"property":"og:title","content":"Generative Models"}],["meta",{"property":"og:description","content":"Generative Models GAN (Generative Adversarial Networks) VAE (Variational Autoencoder) Diffusion Models ARM (Autoregressive Models) The basic goal of AI is to develop intelligent..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-12-15T23:09:49.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-15T23:09:49.000Z"}]]},"git":{"createdTime":1765840189000,"updatedTime":1765840189000,"contributors":[{"name":"David Liu","username":"","email":"davidliu02k@gmail.com","commits":1}]},"readingTime":{"minutes":0.9,"words":269},"filePathRelative":"ai/gm/README.md","excerpt":"\\n<ul>\\n<li>GAN (Generative Adversarial Networks)</li>\\n<li>VAE (Variational Autoencoder)</li>\\n<li>Diffusion Models</li>\\n<li>ARM (Autoregressive Models)</li>\\n</ul>\\n<p>The basic goal of AI is to develop intelligent machines.</p>\\n<p>This consists of many sub-goals:</p>\\n<ul>\\n<li>• Perception\\n<ul>\\n<li>Multimodal foundation models learn to answer questions about images (and text in images)</li>\\n<li>Diffusion models can be used as zero-shot classifiers</li>\\n</ul>\\n</li>\\n<li>• Reasoning\\n<ul>\\n<li>LLMs are also (unexpectedly) good at certain reasoning tasks</li>\\n<li>cf. Chain-of-Though Prompting (an ex. of in-context learning)</li>\\n</ul>\\n</li>\\n<li>• Control / Motion / Manipulation\\n<ul>\\n<li>DayDreamer learns a generative model of experiences for RL, i.e. a World Model, without simulation</li>\\n<li>Quadruped robot learns to walk in under 1 hour</li>\\n</ul>\\n</li>\\n<li>• Planning\\n<ul>\\n<li>LLMs are already being used for grounded planning for embodied agents, c.f. LLMPlanner</li>\\n</ul>\\n</li>\\n<li>• Communication\\n<ul>\\n<li>Communication comprises the comprehension and generation of human language.</li>\\n<li>Large language models (LLMs) excel at both</li>\\n<li>(Even though they are most often trained autoregressively, i.e. to generate a next word, given the previous ones)</li>\\n</ul>\\n</li>\\n<li>• Creativity\\n<ul>\\n<li>Text-to-image models [Midjourney’s Discord server has 18 million members (1.7 million were online this morning)]</li>\\n<li>Text-to-music models [MusicGen capable of conditioning on text and audio sample]</li>\\n</ul>\\n</li>\\n<li>• Learning</li>\\n</ul>","autoDesc":true}`);export{u as comp,m as data};
