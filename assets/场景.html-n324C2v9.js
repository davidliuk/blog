import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as l,a as i,o as p}from"./app-B6aCd_WP.js";const n={};function a(o,e){return p(),l("div",null,e[0]||(e[0]=[i("<p>双向注意力机制，可以看到上下文，做修改是非常合适的场景</p><p>加速</p><ul><li>Cache</li><li>Sampling</li></ul><blockquote><p>损失少量经典，提高大量速度</p></blockquote><p>ARM</p><ul><li>量化，也是精度损失</li></ul><p>长序列</p><p>采样策略</p><ul><li><p>自回归</p></li><li><p>半自回归（Block Diffusion）</p></li></ul><p>LLaDA</p><p>训练过程中很难scale长度</p><p>问题：scaling</p>",12)]))}const m=t(n,[["render",a],["__file","场景.html.vue"]]),u=JSON.parse(`{"path":"/ai/llm/dllm/%E5%9C%BA%E6%99%AF.html","title":"","lang":"en-US","frontmatter":{"description":"双向注意力机制，可以看到上下文，做修改是非常合适的场景 加速 Cache Sampling 损失少量经典，提高大量速度 ARM 量化，也是精度损失 长序列 采样策略 自回归 半自回归（Block Diffusion） LLaDA 训练过程中很难scale长度 问题：scaling","head":[["meta",{"property":"og:url","content":"https://davidliuk.github.io/blog/blog/ai/llm/dllm/%E5%9C%BA%E6%99%AF.html"}],["meta",{"property":"og:site_name","content":"David's Blog"}],["meta",{"property":"og:description","content":"双向注意力机制，可以看到上下文，做修改是非常合适的场景 加速 Cache Sampling 损失少量经典，提高大量速度 ARM 量化，也是精度损失 长序列 采样策略 自回归 半自回归（Block Diffusion） LLaDA 训练过程中很难scale长度 问题：scaling"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-08-17T07:52:28.000Z"}],["meta",{"property":"article:modified_time","content":"2025-08-17T07:52:28.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-08-17T07:52:28.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"David Liu\\",\\"url\\":\\"https://github.com/davidliuk\\"}]}"]]},"headers":[],"git":{"createdTime":1755417148000,"updatedTime":1755417148000,"contributors":[{"name":"dawei.liu","email":"dawei.liu@bytedance.com","commits":1}]},"readingTime":{"minutes":0.27,"words":80},"filePathRelative":"ai/llm/dllm/场景.md","localizedDate":"August 17, 2025","excerpt":"<p>双向注意力机制，可以看到上下文，做修改是非常合适的场景</p>\\n<p>加速</p>\\n<ul>\\n<li>Cache</li>\\n<li>Sampling</li>\\n</ul>\\n<blockquote>\\n<p>损失少量经典，提高大量速度</p>\\n</blockquote>\\n<p>ARM</p>\\n<ul>\\n<li>量化，也是精度损失</li>\\n</ul>\\n<p>长序列</p>\\n<p>采样策略</p>\\n<ul>\\n<li>\\n<p>自回归</p>\\n</li>\\n<li>\\n<p>半自回归（Block Diffusion）</p>\\n</li>\\n</ul>\\n<p>LLaDA</p>\\n<p>训练过程中很难scale长度</p>","autoDesc":true}`);export{m as comp,u as data};
