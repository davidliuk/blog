import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,a as l,o as t}from"./app-C_tIvXCt.js";const o={};function a(r,e){return t(),n("div",null,e[0]||(e[0]=[l('<h1 id="large-language-model" tabindex="-1"><a class="header-anchor" href="#large-language-model"><span>Large Language Model</span></a></h1><p>Pre-train 预训练范式</p><ul><li>ARM (Auto-Regressive Model)</li><li>MDM (Masked Diffusion Model)</li><li>AE (Auto-Encoding)</li></ul><p><strong>大规模数据 + Transformer + 自监督学习 + 自回归/自编码目标 + 泛化能力优先</strong></p><ol><li>预训练（Pre-training）</li><li>后训练/对齐（post-training/Alignment） <ol><li>监督/指令微调（Supervised/Instruction Fine-Tuning）</li><li>人类偏好对齐/强化学习 （Alignment） <ol><li>RLHF</li><li>DPO</li><li>PPO (Proximal Policy Optimization) <ol><li>GRPO (Group Relative Policy Optimization)</li></ol></li><li>RLAIF <ol><li>Reward Model</li></ol></li></ol></li><li>对齐与安全机制（Alignment &amp; Safety Tuning）</li></ol></li></ol>',5)]))}const m=i(o,[["render",a],["__file","index.html.vue"]]),s=JSON.parse(`{"path":"/ai/llm/","title":"Large Language Model","lang":"en-US","frontmatter":{"description":"Large Language Model Pre-train 预训练范式 ARM (Auto-Regressive Model) MDM (Masked Diffusion Model) AE (Auto-Encoding) 大规模数据 + Transformer + 自监督学习 + 自回归/自编码目标 + 泛化能力优先 预训练（Pre-trainin...","head":[["meta",{"property":"og:url","content":"https://davidliuk.github.io/blog/blog/ai/llm/"}],["meta",{"property":"og:site_name","content":"David's Blog"}],["meta",{"property":"og:title","content":"Large Language Model"}],["meta",{"property":"og:description","content":"Large Language Model Pre-train 预训练范式 ARM (Auto-Regressive Model) MDM (Masked Diffusion Model) AE (Auto-Encoding) 大规模数据 + Transformer + 自监督学习 + 自回归/自编码目标 + 泛化能力优先 预训练（Pre-trainin..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-08-17T07:52:28.000Z"}],["meta",{"property":"article:modified_time","content":"2025-08-17T07:52:28.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Large Language Model\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-08-17T07:52:28.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"David Liu\\",\\"url\\":\\"https://github.com/davidliuk\\"}]}"]]},"headers":[],"git":{"createdTime":1700646963000,"updatedTime":1755417148000,"contributors":[{"name":"David","email":"l729641074@163.com","commits":1},{"name":"dawei.liu","email":"dawei.liu@bytedance.com","commits":1}]},"readingTime":{"minutes":0.39,"words":117},"filePathRelative":"ai/llm/README.md","localizedDate":"November 22, 2023","excerpt":"\\n<p>Pre-train 预训练范式</p>\\n<ul>\\n<li>ARM (Auto-Regressive Model)</li>\\n<li>MDM (Masked Diffusion Model)</li>\\n<li>AE (Auto-Encoding)</li>\\n</ul>\\n<p><strong>大规模数据 + Transformer + 自监督学习 + 自回归/自编码目标 + 泛化能力优先</strong></p>\\n<ol>\\n<li>预训练（Pre-training）</li>\\n<li>后训练/对齐（post-training/Alignment）\\n<ol>\\n<li>监督/指令微调（Supervised/Instruction Fine-Tuning）</li>\\n<li>人类偏好对齐/强化学习 （Alignment）\\n<ol>\\n<li>RLHF</li>\\n<li>DPO</li>\\n<li>PPO (Proximal Policy Optimization)\\n<ol>\\n<li>GRPO (Group Relative Policy Optimization)</li>\\n</ol>\\n</li>\\n<li>RLAIF\\n<ol>\\n<li>Reward Model</li>\\n</ol>\\n</li>\\n</ol>\\n</li>\\n<li>对齐与安全机制（Alignment &amp; Safety Tuning）</li>\\n</ol>\\n</li>\\n</ol>","autoDesc":true}`);export{m as comp,s as data};
