import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as e,a as s,o as i}from"./app-B6aCd_WP.js";const t={};function l(r,a){return i(),e("div",null,a[0]||(a[0]=[s(`<h1 id="transformer" tabindex="-1"><a class="header-anchor" href="#transformer"><span>Transformer</span></a></h1><h2 id="步骤" tabindex="-1"><a class="header-anchor" href="#步骤"><span>步骤</span></a></h2><ol><li>Tokenization</li><li>Input Layer 理解 Token <ol><li>Embedding 语言</li><li>Positional Embedding 位置</li></ol></li><li>Transformer Block * N <ol><li>Attention 上下文</li><li>Feed Forward</li></ol></li><li>Output Layer</li></ol><h3 id="tokenization" tabindex="-1"><a class="header-anchor" href="#tokenization"><span>Tokenization</span></a></h3><p>Token list</p><p>Byte Pair Encoding (BPE)</p><p>无训练参数</p><p><a href="https://platform.openai.com/tokenizer" target="_blank" rel="noopener noreferrer">https://platform.openai.com/tokenizer</a></p><h3 id="input-layer" tabindex="-1"><a class="header-anchor" href="#input-layer"><span>Input Layer</span></a></h3><h3 id="tokenization-1" tabindex="-1"><a class="header-anchor" href="#tokenization-1"><span>Tokenization</span></a></h3><h4 id="embedding" tabindex="-1"><a class="header-anchor" href="#embedding"><span>Embedding</span></a></h4><p>把 token 变成向量：意思相近的 Token 会有接近的 Embedding</p><p>Token Embedding 是参数，训练时获得</p><blockquote><p>目前的缺点，没有考虑上下文</p></blockquote><h4 id="positional-embedding" tabindex="-1"><a class="header-anchor" href="#positional-embedding"><span>Positional Embedding</span></a></h4><p>为位置设置向量，拼接到 Embedding 前面</p><ul><li>可以由人设计</li><li>可以设成参数</li></ul><h3 id="transformer-block" tabindex="-1"><a class="header-anchor" href="#transformer-block"><span>Transformer Block</span></a></h3><h4 id="attention" tabindex="-1"><a class="header-anchor" href="#attention"><span>Attention</span></a></h4><p>Contextualized Token Embedding</p><p>考虑上下文</p><p>输入一组Embedding=&gt;输出等长Embedding</p><p>找出相关的Token</p><p>相关性：attention weight</p><p>分数=f() （有参数，需要训练得到）</p><p>输出以相关性为权的加权平均数</p><p>所有Token两两相关性</p><p>Attention Matrix</p><ul><li>Causal Attention: 只考虑前面的 Token</li></ul><p>Multi-Head Attention 关联性不止一种（一般16组）</p><h4 id="feed-forward" tabindex="-1"><a class="header-anchor" href="#feed-forward"><span>Feed Forward</span></a></h4><p>把 MHA 输出的多个向量综合考虑下，出一个向量出来</p><hr><p>最后一个Transformer Block句尾的向量通过Output Layer</p><p>Linear Transform + Softmax 给一个几率分布，知道下一个选哪个 Token 的几率</p><hr><p>研究方向</p><ul><li>加速 Attention 计算</li><li>达成无限长度的 Attention</li><li>Train short, test long</li><li>其他类神经网络架构的可能性</li></ul><h3 id="产生答案" tabindex="-1"><a class="header-anchor" href="#产生答案"><span>产生答案</span></a></h3><p>处理超长文本会是挑战：Attention 要计算两两相似度，是n^2</p><p>Cross Attention</p><h2 id="结构" tabindex="-1"><a class="header-anchor" href="#结构"><span>结构</span></a></h2><ol><li>Word Embedding</li><li>Positional encoding (PE)</li><li>Multi-Head Attention (MHA)</li><li>Feed-Forward Network (FFN) <ul><li>MLP 全连接神经网络</li><li>隐藏层12288*4维向量，先放大再提取来过滤信息 <ul><li>w1 放大4倍</li><li>w2 缩小4倍</li></ul></li></ul></li><li>Layer Normalization</li></ol><p><img src="https://gcore.jsdelivr.net/gh/davidliuk/images@master/image-20250610233936490.png" alt="image-20250610233936490" loading="lazy"></p><h2 id="word-embedding" tabindex="-1"><a class="header-anchor" href="#word-embedding"><span>Word Embedding</span></a></h2><p>词嵌入：把词变成向量</p><h2 id="self-attention" tabindex="-1"><a class="header-anchor" href="#self-attention"><span>Self-Attention</span></a></h2><p><strong>自注意力</strong>：通过Query、Key、Value三个矩阵计算序列中每个位置对其他位置的关注度。QKV 都是一个东西，所以就是自注意力机制</p><p><strong>计算过程：</strong></p><ul><li>将输入映射为Q、K、V三个矩阵</li><li>计算注意力权重：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msub><msqrt><mi>d</mi></msqrt><mi>k</mi></msub></mfrac><mo stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt d_k})V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.6775em;vertical-align:-0.588em;"></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0895em;"><span style="top:-2.5335em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9378em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal mtight" style="padding-left:0.833em;">d</span></span><span style="top:-2.8978em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1022em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3224em;"><span style="top:-2.3264em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1736em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">Q</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9191em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.588em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></li><li>允许模型并行处理序列，突破了RNN的顺序限制</li></ul><p><strong>后续改进：</strong></p><ul><li>MHA: 将注意力机制并行化，每个头关注不同的表示子空间。</li><li><strong>Multi-Query Attention (MQA)</strong>: 多个Query头共享同一个Key和Value，减少内存占用</li><li><strong>Grouped-Query Attention (GQA)</strong>: MQA和MHA的折中方案</li><li><strong>Flash Attention</strong>: 通过重新组织计算顺序，大幅降低内存使用</li><li><strong>Sliding Window Attention</strong>: 限制注意力窗口大小，提高长序列效率</li><li><strong>Sparse Attention</strong>: 只计算部分注意力连接，如Longformer、BigBird</li></ul><h2 id="positional-encoding" tabindex="-1"><a class="header-anchor" href="#positional-encoding"><span>Positional Encoding</span></a></h2><p><strong>位置编码</strong>：由于自注意力机制本身不包含位置信息，需要额外添加位置编码。</p><blockquote><p>将每个位置编号，从而每个编号对应一个向量，最终通过结合位置向量和词向量，作为输入embedding，就给每个词都引入了一定的位置信息，这样Attention就可以分辨出不同位置的词了</p></blockquote><ul><li><p>RoPE(Rotary Position Embedding): 旋转位置编码</p><ul><li>NTK-aware Scaled RoPE</li></ul></li><li><p>ALiBi(Attention with Linear Biases): 一种位置编码方法，通过线性偏置实现</p></li><li><p>Xpos</p></li></ul><h2 id="feed-forward-network" tabindex="-1"><a class="header-anchor" href="#feed-forward-network"><span>Feed-Forward Network</span></a></h2><p><strong>前馈网络</strong>：每个 Transformer 层包含一个两层的全连接网络。</p><p><strong>后续改进：</strong></p><ul><li><strong>GLU变体</strong>: 使用门控机制，如SwiGLU、GeGLU</li><li><strong>专家混合 (MoE)</strong>: 条件激活部分参数，如 Switch Transformer</li><li><strong>参数高效方法</strong>: LoRA、Adapter等减少微调参数</li></ul><h2 id="layer-normalization" tabindex="-1"><a class="header-anchor" href="#layer-normalization"><span>Layer Normalization</span></a></h2><p><strong>层归一化</strong>：标准化层输入，稳定训练过程。</p><p><strong>后续改进：</strong></p><ul><li><strong>Pre-LN</strong>: 将LayerNorm移到残差连接之前，改善训练稳定性</li><li><strong>RMSNorm</strong>: 简化LayerNorm计算</li><li><strong>DeepNorm</strong>: 专门为深层网络设计的归一化方法</li></ul><h2 id="training-tips" tabindex="-1"><a class="header-anchor" href="#training-tips"><span>Training Tips</span></a></h2><h3 id="teacher-forcing" tabindex="-1"><a class="header-anchor" href="#teacher-forcing"><span>Teacher Forcing</span></a></h3><p>using the ground truth as input when training</p><p>There is a mismatch! exposure bias</p><blockquote><p>训练的时候给她看一点错的</p></blockquote><p>Scheduled Sampling</p><blockquote><ul><li><a href="https://arxiv.org/abs/1506.03099" target="_blank" rel="noopener noreferrer">Original Scheduled Sampling</a><ul><li>for LSTM的，对于Transformer会损失并行能力</li></ul></li><li><a href="https://arxiv.org/abs/1906.07651" target="_blank" rel="noopener noreferrer">Scheduled Sampling for Transformer</a></li><li><a href="https://arxiv.org/abs/1906.04331" target="_blank" rel="noopener noreferrer">Parallel Scheduled Sampling</a></li></ul></blockquote><h3 id="copy-mechanism" tabindex="-1"><a class="header-anchor" href="#copy-mechanism"><span>Copy Mechanism</span></a></h3><blockquote><p>Pointer Network</p></blockquote><p>Copying</p><h3 id="guided-attention" tabindex="-1"><a class="header-anchor" href="#guided-attention"><span>Guided Attention</span></a></h3><p>语音识别、语音合成中经常使用</p><p>In some tasks,input and output are monotonically aligned. For example,speech recognition,TTS,etc.</p><p>Monotonic Attention</p><p>Location-aware attention</p><h3 id="beam-search" tabindex="-1"><a class="header-anchor" href="#beam-search"><span>Beam Search</span></a></h3><p>解决 Greedy Decoding 不是全局最优解</p><p>Beam Search 给一个估算的全局最优解</p><p>根据任务：</p><ul><li>确定性任务可以beam search，</li><li>创造力任务需要加入随机性不要beam search <ul><li>训练的时候加noise或drop out</li><li>decoder 中加入随机性</li></ul></li></ul><h3 id="sampling" tabindex="-1"><a class="header-anchor" href="#sampling"><span>Sampling</span></a></h3><blockquote><p>The Curious Case of Neural Text Degeneration <a href="https://arxiv.org/abs/1904.09751" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1904.09751</a></p></blockquote><h3 id="optimizing-evaluation-metrics" tabindex="-1"><a class="header-anchor" href="#optimizing-evaluation-metrics"><span>Optimizing Evaluation Metrics</span></a></h3><p>Training: minimize cross entropy</p><p>Evaluation: BLEU score (不能微分，没法sgd)</p><p>遇到无法optimize的loss function，把它当作是RL的reward，把decoder当作rl的agent，用RL硬train就可以了</p>`,90)]))}const m=n(t,[["render",l],["__file","index.html.vue"]]),c=JSON.parse(`{"path":"/ai/llm/transformer/","title":"Transformer","lang":"en-US","frontmatter":{"description":"Transformer 步骤 Tokenization Input Layer 理解 Token Embedding 语言 Positional Embedding 位置 Transformer Block * N Attention 上下文 Feed Forward Output Layer Tokenization Token list Byte ...","head":[["meta",{"property":"og:url","content":"https://davidliuk.github.io/blog/blog/ai/llm/transformer/"}],["meta",{"property":"og:site_name","content":"David's Blog"}],["meta",{"property":"og:title","content":"Transformer"}],["meta",{"property":"og:description","content":"Transformer 步骤 Tokenization Input Layer 理解 Token Embedding 语言 Positional Embedding 位置 Transformer Block * N Attention 上下文 Feed Forward Output Layer Tokenization Token list Byte ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://gcore.jsdelivr.net/gh/davidliuk/images@master/image-20250610233936490.png"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-08-17T07:52:28.000Z"}],["meta",{"property":"article:modified_time","content":"2025-08-17T07:52:28.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Transformer\\",\\"image\\":[\\"https://gcore.jsdelivr.net/gh/davidliuk/images@master/image-20250610233936490.png\\"],\\"dateModified\\":\\"2025-08-17T07:52:28.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"David Liu\\",\\"url\\":\\"https://github.com/davidliuk\\"}]}"]]},"headers":[{"level":2,"title":"步骤","slug":"步骤","link":"#步骤","children":[{"level":3,"title":"Tokenization","slug":"tokenization","link":"#tokenization","children":[]},{"level":3,"title":"Input Layer","slug":"input-layer","link":"#input-layer","children":[]},{"level":3,"title":"Tokenization","slug":"tokenization-1","link":"#tokenization-1","children":[]},{"level":3,"title":"Transformer Block","slug":"transformer-block","link":"#transformer-block","children":[]},{"level":3,"title":"产生答案","slug":"产生答案","link":"#产生答案","children":[]}]},{"level":2,"title":"结构","slug":"结构","link":"#结构","children":[]},{"level":2,"title":"Word Embedding","slug":"word-embedding","link":"#word-embedding","children":[]},{"level":2,"title":"Self-Attention","slug":"self-attention","link":"#self-attention","children":[]},{"level":2,"title":"Positional Encoding","slug":"positional-encoding","link":"#positional-encoding","children":[]},{"level":2,"title":"Feed-Forward Network","slug":"feed-forward-network","link":"#feed-forward-network","children":[]},{"level":2,"title":"Layer Normalization","slug":"layer-normalization","link":"#layer-normalization","children":[]},{"level":2,"title":"Training Tips","slug":"training-tips","link":"#training-tips","children":[{"level":3,"title":"Teacher Forcing","slug":"teacher-forcing","link":"#teacher-forcing","children":[]},{"level":3,"title":"Copy Mechanism","slug":"copy-mechanism","link":"#copy-mechanism","children":[]},{"level":3,"title":"Guided Attention","slug":"guided-attention","link":"#guided-attention","children":[]},{"level":3,"title":"Beam Search","slug":"beam-search","link":"#beam-search","children":[]},{"level":3,"title":"Sampling","slug":"sampling","link":"#sampling","children":[]},{"level":3,"title":"Optimizing Evaluation Metrics","slug":"optimizing-evaluation-metrics","link":"#optimizing-evaluation-metrics","children":[]}]}],"git":{"createdTime":1755417148000,"updatedTime":1755417148000,"contributors":[{"name":"dawei.liu","email":"dawei.liu@bytedance.com","commits":1}]},"readingTime":{"minutes":3.77,"words":1130},"filePathRelative":"ai/llm/transformer/README.md","localizedDate":"August 17, 2025","excerpt":"\\n<h2>步骤</h2>\\n<ol>\\n<li>Tokenization</li>\\n<li>Input Layer 理解 Token\\n<ol>\\n<li>Embedding 语言</li>\\n<li>Positional Embedding 位置</li>\\n</ol>\\n</li>\\n<li>Transformer Block * N\\n<ol>\\n<li>Attention 上下文</li>\\n<li>Feed Forward</li>\\n</ol>\\n</li>\\n<li>Output Layer</li>\\n</ol>\\n<h3>Tokenization</h3>\\n<p>Token list</p>\\n<p>Byte Pair Encoding (BPE)</p>","autoDesc":true}`);export{m as comp,c as data};
