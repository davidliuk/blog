import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,a as l,o as t}from"./app-BtADw1TI.js";const o={};function a(r,e){return t(),n("div",null,[...e[0]||(e[0]=[l('<h1 id="large-language-model" tabindex="-1"><a class="header-anchor" href="#large-language-model"><span>Large Language Model</span></a></h1><p>Pre-train 预训练范式</p><ul><li>ARM (Auto-Regressive Model)</li><li>MDM (Masked Diffusion Model)</li><li>AEM (Auto-Encoding Model)</li></ul><p><strong>大规模数据 + Transformer + 自监督学习 + 自回归/自编码目标 + 泛化能力优先</strong></p><ol><li>预训练（Pre-training）</li><li>后训练/对齐（Post-training/Alignment） <ol><li>监督/指令微调（Supervised/Instruction Fine-Tuning）</li><li>人类偏好对齐/强化学习 （Alignment） <ol><li>RLHF</li><li>DPO</li><li>PPO (Proximal Policy Optimization) <ol><li>GRPO (Group Relative Policy Optimization)</li></ol></li><li>RLAIF <ol><li>Reward Model</li></ol></li></ol></li><li>对齐与安全机制（Alignment &amp; Safety Tuning）</li></ol></li></ol>',5)])])}const s=i(o,[["render",a]]),m=JSON.parse(`{"path":"/ai/gm/text/","title":"Large Language Model","lang":"en-US","frontmatter":{"description":"Large Language Model Pre-train 预训练范式 ARM (Auto-Regressive Model) MDM (Masked Diffusion Model) AEM (Auto-Encoding Model) 大规模数据 + Transformer + 自监督学习 + 自回归/自编码目标 + 泛化能力优先 预训练（Pre-...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Large Language Model\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-12-15T23:09:49.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"David Liu\\",\\"url\\":\\"https://github.com/davidliuk\\"}]}"],["meta",{"property":"og:url","content":"https://davidliuk.github.io/blog/blog/ai/gm/text/"}],["meta",{"property":"og:site_name","content":"David's Blog"}],["meta",{"property":"og:title","content":"Large Language Model"}],["meta",{"property":"og:description","content":"Large Language Model Pre-train 预训练范式 ARM (Auto-Regressive Model) MDM (Masked Diffusion Model) AEM (Auto-Encoding Model) 大规模数据 + Transformer + 自监督学习 + 自回归/自编码目标 + 泛化能力优先 预训练（Pre-..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-12-15T23:09:49.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-15T23:09:49.000Z"}]]},"git":{"createdTime":1765840189000,"updatedTime":1765840189000,"contributors":[{"name":"David Liu","username":"","email":"davidliu02k@gmail.com","commits":1}]},"readingTime":{"minutes":0.39,"words":118},"filePathRelative":"ai/gm/text/README.md","excerpt":"\\n<p>Pre-train 预训练范式</p>\\n<ul>\\n<li>ARM (Auto-Regressive Model)</li>\\n<li>MDM (Masked Diffusion Model)</li>\\n<li>AEM (Auto-Encoding Model)</li>\\n</ul>\\n<p><strong>大规模数据 + Transformer + 自监督学习 + 自回归/自编码目标 + 泛化能力优先</strong></p>\\n<ol>\\n<li>预训练（Pre-training）</li>\\n<li>后训练/对齐（Post-training/Alignment）\\n<ol>\\n<li>监督/指令微调（Supervised/Instruction Fine-Tuning）</li>\\n<li>人类偏好对齐/强化学习 （Alignment）\\n<ol>\\n<li>RLHF</li>\\n<li>DPO</li>\\n<li>PPO (Proximal Policy Optimization)\\n<ol>\\n<li>GRPO (Group Relative Policy Optimization)</li>\\n</ol>\\n</li>\\n<li>RLAIF\\n<ol>\\n<li>Reward Model</li>\\n</ol>\\n</li>\\n</ol>\\n</li>\\n<li>对齐与安全机制（Alignment &amp; Safety Tuning）</li>\\n</ol>\\n</li>\\n</ol>","autoDesc":true}`);export{s as comp,m as data};
