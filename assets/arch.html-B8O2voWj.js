import{_ as l}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as i,a as n,o as t}from"./app-B6aCd_WP.js";const r={};function a(o,e){return t(),i("div",null,e[0]||(e[0]=[n('<h1 id="architecture" tabindex="-1"><a class="header-anchor" href="#architecture"><span>Architecture</span></a></h1><ul><li>Encoder-Decoder</li><li>Encoder-Only</li><li>Decoder-Only</li></ul><h3 id="涌现能力" tabindex="-1"><a class="header-anchor" href="#涌现能力"><span>涌现能力</span></a></h3><blockquote><p>上述GPT系列模型的能力拓展和增强，主要得益于模型规模和数据规模的增长。</p></blockquote><p>实验发现这些新能力随着模型规模提升凭空自然涌现出来，因此将其称为涌现能力(Emergent Abilities)，例如上下文学习、逻辑推理和常识推理等能力。</p><ul><li>上下文学习：上下文学习(In-Context Learning)指大语言模型在某些任务中无需额外的训练仅通过上下文信息中的示例或提示即可理解任务并生成输出。</li><li>常识推理：常识推理(Commonsense Reasoning)指大语言模型基于常识知识和逻辑进行理解和推断的能力。</li><li>逻辑推理：逻辑推理(Logical Reasoning)指大语言模型基于给定信息和规则进行行合乎逻辑的推断和结论的能力。</li></ul><h3 id="扩展法则" tabindex="-1"><a class="header-anchor" href="#扩展法则"><span>扩展法则</span></a></h3><p>而上述GPT系列模型的性能提升，有着一系列关于模型能力与参数/数据规模之间的定量关系作为理论支撑，即扩展法则(Scaling Law)。其中以OpenAl提出的 Kaplan-McCandlish 法则以及 DeepMind 提出的 Chinchilla 法则最为著名。</p><h2 id="encoder-decoder" tabindex="-1"><a class="header-anchor" href="#encoder-decoder"><span>Encoder-Decoder</span></a></h2><p>正常的完整 Transformer 架构</p><ul><li>T5，常用于检索器、精排器</li><li>BART</li></ul><h2 id="encoder-only" tabindex="-1"><a class="header-anchor" href="#encoder-only"><span>Encoder-Only</span></a></h2><p>双向注意力</p><p>单词填空任务</p><ul><li>BERT 表征学习时代的王者</li><li>RoBERTa</li></ul><p>架构</p><ol><li>Embedding</li><li>Transformer Block</li><li>下游</li></ol><p>目前还应用于大规模的检索和实体匹配里面</p><h2 id="decoder-only" tabindex="-1"><a class="header-anchor" href="#decoder-only"><span>Decoder-Only</span></a></h2><p>Decoder-only架构选取了Transformer中的解码器部分，即输入编码、特征解码及输出生成部分。核心特点在于省略了每个解码模块中的交叉注意力子模块。</p><p>单向注意力</p><p>下一词预测任务</p><ul><li>GPT</li><li>LLaMA</li></ul><h3 id="gpt" tabindex="-1"><a class="header-anchor" href="#gpt"><span>GPT</span></a></h3><ul><li>GPT-3 涌现出上下文学习(In-Context Learning ICL)能力，推理能力</li><li>InstructGPT 引入 RLHF (Reinforcement Learning from Human Feedback)</li><li>GPT 3.5 ChatGPT</li><li>GPT-4 多模态能力</li></ul><h3 id="llama" tabindex="-1"><a class="header-anchor" href="#llama"><span>LLaMA</span></a></h3><p>LLaMA 1</p><ul><li>Embedding: CoPE -&gt; RoPE</li><li>Attention: Pre-Norm</li><li>FFN: ReLU -&gt; SwiGLU</li></ul><p>LLaMA 2</p><ul><li>RLHF</li><li>拒绝采用</li><li>GQA (Grouped Query Attention)</li></ul><p>LLaMA 3</p><blockquote><p>AlphaGo: 0.065B</p></blockquote><p>LLaMA 衍生模型</p><ul><li>性能改进类模型 <ul><li>Alpaca</li><li>Vicuna</li></ul></li><li>垂域任务类模型 <ul><li>CodeLLaMA</li><li>FinLLaMA</li></ul></li><li>多模态任务类 <ul><li>LLaVA</li><li>MiniGPT4</li></ul></li></ul>',34)]))}const p=l(r,[["render",a],["__file","arch.html.vue"]]),s=JSON.parse(`{"path":"/ai/llm/transformer/arch.html","title":"Architecture","lang":"en-US","frontmatter":{"description":"Architecture Encoder-Decoder Encoder-Only Decoder-Only 涌现能力 上述GPT系列模型的能力拓展和增强，主要得益于模型规模和数据规模的增长。 实验发现这些新能力随着模型规模提升凭空自然涌现出来，因此将其称为涌现能力(Emergent Abilities)，例如上下文学习、逻辑推理和常识推理等能力。 上...","head":[["meta",{"property":"og:url","content":"https://davidliuk.github.io/blog/blog/ai/llm/transformer/arch.html"}],["meta",{"property":"og:site_name","content":"David's Blog"}],["meta",{"property":"og:title","content":"Architecture"}],["meta",{"property":"og:description","content":"Architecture Encoder-Decoder Encoder-Only Decoder-Only 涌现能力 上述GPT系列模型的能力拓展和增强，主要得益于模型规模和数据规模的增长。 实验发现这些新能力随着模型规模提升凭空自然涌现出来，因此将其称为涌现能力(Emergent Abilities)，例如上下文学习、逻辑推理和常识推理等能力。 上..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-08-17T07:52:28.000Z"}],["meta",{"property":"article:modified_time","content":"2025-08-17T07:52:28.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Architecture\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-08-17T07:52:28.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"David Liu\\",\\"url\\":\\"https://github.com/davidliuk\\"}]}"]]},"headers":[{"level":3,"title":"涌现能力","slug":"涌现能力","link":"#涌现能力","children":[]},{"level":3,"title":"扩展法则","slug":"扩展法则","link":"#扩展法则","children":[]},{"level":2,"title":"Encoder-Decoder","slug":"encoder-decoder","link":"#encoder-decoder","children":[]},{"level":2,"title":"Encoder-Only","slug":"encoder-only","link":"#encoder-only","children":[]},{"level":2,"title":"Decoder-Only","slug":"decoder-only","link":"#decoder-only","children":[{"level":3,"title":"GPT","slug":"gpt","link":"#gpt","children":[]},{"level":3,"title":"LLaMA","slug":"llama","link":"#llama","children":[]}]}],"git":{"createdTime":1755417148000,"updatedTime":1755417148000,"contributors":[{"name":"dawei.liu","email":"dawei.liu@bytedance.com","commits":1}]},"readingTime":{"minutes":1.86,"words":557},"filePathRelative":"ai/llm/transformer/arch.md","localizedDate":"August 17, 2025","excerpt":"\\n<ul>\\n<li>Encoder-Decoder</li>\\n<li>Encoder-Only</li>\\n<li>Decoder-Only</li>\\n</ul>\\n<h3>涌现能力</h3>\\n<blockquote>\\n<p>上述GPT系列模型的能力拓展和增强，主要得益于模型规模和数据规模的增长。</p>\\n</blockquote>\\n<p>实验发现这些新能力随着模型规模提升凭空自然涌现出来，因此将其称为涌现能力(Emergent Abilities)，例如上下文学习、逻辑推理和常识推理等能力。</p>\\n<ul>\\n<li>上下文学习：上下文学习(In-Context Learning)指大语言模型在某些任务中无需额外的训练仅通过上下文信息中的示例或提示即可理解任务并生成输出。</li>\\n<li>常识推理：常识推理(Commonsense Reasoning)指大语言模型基于常识知识和逻辑进行理解和推断的能力。</li>\\n<li>逻辑推理：逻辑推理(Logical Reasoning)指大语言模型基于给定信息和规则进行行合乎逻辑的推断和结论的能力。</li>\\n</ul>","autoDesc":true}`);export{p as comp,s as data};
