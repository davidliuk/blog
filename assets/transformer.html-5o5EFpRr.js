import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as r,a as n,o}from"./app-BtADw1TI.js";const d={};function a(i,t){return o(),r("div",null,[...t[0]||(t[0]=[n('<h1 id="transformer" tabindex="-1"><a class="header-anchor" href="#transformer"><span>Transformer</span></a></h1><ul><li>Multi-Head Attention</li><li>Self-Attention</li><li>LayerNorm+Residual</li><li>Position Encoding</li><li>FFN (Feed-Forward Network)</li></ul><table><thead><tr><th>ç»„ä»¶</th><th>ä½œç”¨</th></tr></thead><tbody><tr><td>Multi-Head Attention</td><td>å¹¶è¡Œå¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œæ•æ‰ä¸åŒè¯­ä¹‰å…³ç³»</td></tr><tr><td>Self-Attention</td><td>å¥å­å†…éƒ¨ token ä¹‹é—´äº’ç›¸å…³æ³¨</td></tr><tr><td>LayerNorm + Residual</td><td>åŠ å¿«æ”¶æ•›ã€ç¨³å®šè®­ç»ƒ</td></tr><tr><td>Position Encoding</td><td>æä¾› token çš„ä½ç½®ä¿¡æ¯ï¼ˆå› ä¸ºæ³¨æ„åŠ›æ˜¯æ— åºçš„ï¼‰</td></tr><tr><td>Feed-Forward Network</td><td>æ¯ä¸ªä½ç½®ç‹¬ç«‹çš„éçº¿æ€§å˜æ¢</td></tr></tbody></table><h3 id="ğŸ”§-decoder-only-çš„ç‰¹ç‚¹" tabindex="-1"><a class="header-anchor" href="#ğŸ”§-decoder-only-çš„ç‰¹ç‚¹"><span>ğŸ”§ Decoder-only çš„ç‰¹ç‚¹ï¼š</span></a></h3><table><thead><tr><th>ç‰¹æ€§</th><th>åŸå› </th></tr></thead><tbody><tr><td><strong>ä»…ç”¨ Masked Self-Attention</strong></td><td>é˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯ï¼ˆåªçœ‹ $x_{ï¼‰</td></tr><tr><td><strong>æ—  Encoder è¾“å…¥</strong></td><td>è¾“å…¥å°±æ˜¯è®­ç»ƒæ•°æ®æœ¬èº«ï¼Œæ— éœ€å¤–éƒ¨åºåˆ—</td></tr><tr><td><strong>å¯é€è¯ç”Ÿæˆ</strong></td><td>éå¸¸é€‚åˆ open-ended generation</td></tr></tbody></table>',5)])])}const m=e(d,[["render",a]]),c=JSON.parse(`{"path":"/ai/gm/foundations/transformer/transformer.html","title":"Transformer","lang":"en-US","frontmatter":{"description":"Transformer Multi-Head Attention Self-Attention LayerNorm+Residual Position Encoding FFN (Feed-Forward Network) ğŸ”§ Decoder-only çš„ç‰¹ç‚¹ï¼š","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Transformer\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-12-15T23:09:49.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"David Liu\\",\\"url\\":\\"https://github.com/davidliuk\\"}]}"],["meta",{"property":"og:url","content":"https://davidliuk.github.io/blog/blog/ai/gm/foundations/transformer/transformer.html"}],["meta",{"property":"og:site_name","content":"David's Blog"}],["meta",{"property":"og:title","content":"Transformer"}],["meta",{"property":"og:description","content":"Transformer Multi-Head Attention Self-Attention LayerNorm+Residual Position Encoding FFN (Feed-Forward Network) ğŸ”§ Decoder-only çš„ç‰¹ç‚¹ï¼š"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-12-15T23:09:49.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-15T23:09:49.000Z"}]]},"git":{"createdTime":1765840189000,"updatedTime":1765840189000,"contributors":[{"name":"David Liu","username":"","email":"davidliu02k@gmail.com","commits":1}]},"readingTime":{"minutes":0.5,"words":151},"filePathRelative":"ai/gm/foundations/transformer/transformer.md","excerpt":"\\n<ul>\\n<li>Multi-Head Attention</li>\\n<li>Self-Attention</li>\\n<li>LayerNorm+Residual</li>\\n<li>Position Encoding</li>\\n<li>FFN (Feed-Forward Network)</li>\\n</ul>\\n<table>\\n<thead>\\n<tr>\\n<th>ç»„ä»¶</th>\\n<th>ä½œç”¨</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>Multi-Head Attention</td>\\n<td>å¹¶è¡Œå¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œæ•æ‰ä¸åŒè¯­ä¹‰å…³ç³»</td>\\n</tr>\\n<tr>\\n<td>Self-Attention</td>\\n<td>å¥å­å†…éƒ¨ token ä¹‹é—´äº’ç›¸å…³æ³¨</td>\\n</tr>\\n<tr>\\n<td>LayerNorm + Residual</td>\\n<td>åŠ å¿«æ”¶æ•›ã€ç¨³å®šè®­ç»ƒ</td>\\n</tr>\\n<tr>\\n<td>Position Encoding</td>\\n<td>æä¾› token çš„ä½ç½®ä¿¡æ¯ï¼ˆå› ä¸ºæ³¨æ„åŠ›æ˜¯æ— åºçš„ï¼‰</td>\\n</tr>\\n<tr>\\n<td>Feed-Forward Network</td>\\n<td>æ¯ä¸ªä½ç½®ç‹¬ç«‹çš„éçº¿æ€§å˜æ¢</td>\\n</tr>\\n</tbody>\\n</table>","autoDesc":true}`);export{m as comp,c as data};
