import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as i,a as n,d as t,o as l}from"./app-BtADw1TI.js";const p={};function r(o,e){return l(),i("div",null,[...e[0]||(e[0]=[n('<h1 id="vllm" tabindex="-1"><a class="header-anchor" href="#vllm"><span>vLLM</span></a></h1><h2 id="kv-cache" tabindex="-1"><a class="header-anchor" href="#kv-cache"><span>KV Cache</span></a></h2><p>在大模型推理时，按照可生成最长序列长度分配显存。<br> 造成三种类型的浪费：</p><ol><li>预分配，但不会用到。</li><li>预分配，但尚未用到。</li><li>显存之间的间隔碎片，不足以预分配给下一个文本生成。</li></ol><p>利用率只有20%-40%</p><p>vllm就是处理碎片问题</p><h2 id="page-attention" tabindex="-1"><a class="header-anchor" href="#page-attention"><span>Page Attention</span></a></h2><p>虚拟内存和页管理技术</p><p><img src="https://gcore.jsdelivr.net/gh/davidliuk/images@master/image-20250628203009007.png" alt="image-20250628203009007" loading="lazy"></p><p>KV Cache</p><p><img src="https://gcore.jsdelivr.net/gh/davidliuk/images@master/image-20250628203235320.png" alt="image-20250628203235320" loading="lazy"></p><p>KV Block是4个token，最多浪费3个token，所以解决预分配问题</p><p>虚拟内存</p><p>逻辑KV Cache和物理内存的映射表</p><p>逻辑上连续，实际物理上不连续</p><hr><ul><li>按需分配，不提前预分配。</li><li>按B引ock分配，减少碎片大小。</li><li>虚拟内存，方便实现调用。</li></ul><p>=&gt; 利用率从20-40%到96%</p><h2 id="sharing-kv-blocks" tabindex="-1"><a class="header-anchor" href="#sharing-kv-blocks"><span>Sharing KV Blocks</span></a></h2><p>用大语言模型同一个Prompt,希望生成多个Output时。</p><p>Prompt:</p>',21),t("p",{item:""},"请把下边这句话翻译为英文：",-1),t("p",null,"引用计数",-1)])])}const s=a(p,[["render",r]]),g=JSON.parse(`{"path":"/ai/gm/efficiency/inference/vllm.html","title":"vLLM","lang":"en-US","frontmatter":{"description":"vLLM KV Cache 在大模型推理时，按照可生成最长序列长度分配显存。 造成三种类型的浪费： 预分配，但不会用到。 预分配，但尚未用到。 显存之间的间隔碎片，不足以预分配给下一个文本生成。 利用率只有20%-40% vllm就是处理碎片问题 Page Attention 虚拟内存和页管理技术 image-20250628203009007 KV ...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"vLLM\\",\\"image\\":[\\"https://gcore.jsdelivr.net/gh/davidliuk/images@master/image-20250628203009007.png\\",\\"https://gcore.jsdelivr.net/gh/davidliuk/images@master/image-20250628203235320.png\\"],\\"dateModified\\":\\"2025-12-15T23:09:49.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"David Liu\\",\\"url\\":\\"https://github.com/davidliuk\\"}]}"],["meta",{"property":"og:url","content":"https://davidliuk.github.io/blog/blog/ai/gm/efficiency/inference/vllm.html"}],["meta",{"property":"og:site_name","content":"David's Blog"}],["meta",{"property":"og:title","content":"vLLM"}],["meta",{"property":"og:description","content":"vLLM KV Cache 在大模型推理时，按照可生成最长序列长度分配显存。 造成三种类型的浪费： 预分配，但不会用到。 预分配，但尚未用到。 显存之间的间隔碎片，不足以预分配给下一个文本生成。 利用率只有20%-40% vllm就是处理碎片问题 Page Attention 虚拟内存和页管理技术 image-20250628203009007 KV ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://gcore.jsdelivr.net/gh/davidliuk/images@master/image-20250628203009007.png"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-12-15T23:09:49.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-15T23:09:49.000Z"}]]},"git":{"createdTime":1765840189000,"updatedTime":1765840189000,"contributors":[{"name":"David Liu","username":"","email":"davidliu02k@gmail.com","commits":1}]},"readingTime":{"minutes":0.82,"words":247},"filePathRelative":"ai/gm/efficiency/inference/vllm.md","excerpt":"\\n<h2>KV Cache</h2>\\n<p>在大模型推理时，按照可生成最长序列长度分配显存。<br>\\n造成三种类型的浪费：</p>\\n<ol>\\n<li>预分配，但不会用到。</li>\\n<li>预分配，但尚未用到。</li>\\n<li>显存之间的间隔碎片，不足以预分配给下一个文本生成。</li>\\n</ol>\\n<p>利用率只有20%-40%</p>\\n<p>vllm就是处理碎片问题</p>\\n<h2>Page Attention</h2>\\n<p>虚拟内存和页管理技术</p>\\n<p><img src=\\"https://gcore.jsdelivr.net/gh/davidliuk/images@master/image-20250628203009007.png\\" alt=\\"image-20250628203009007\\" loading=\\"lazy\\"></p>","autoDesc":true}`);export{s as comp,g as data};
