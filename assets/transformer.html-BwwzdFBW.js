import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as r,a as n,o}from"./app-CGXHKXsa.js";const d={};function i(a,t){return o(),r("div",null,t[0]||(t[0]=[n('<h1 id="transformer" tabindex="-1"><a class="header-anchor" href="#transformer"><span>Transformer</span></a></h1><ul><li>Multi-Head Attention</li><li>Self-Attention</li><li>LayerNorm+Residual</li><li>Position Encoding</li><li>FFN (Feed-Forward Network)</li></ul><table><thead><tr><th>组件</th><th>作用</th></tr></thead><tbody><tr><td>Multi-Head Attention</td><td>并行多个注意力头，捕捉不同语义关系</td></tr><tr><td>Self-Attention</td><td>句子内部 token 之间互相关注</td></tr><tr><td>LayerNorm + Residual</td><td>加快收敛、稳定训练</td></tr><tr><td>Position Encoding</td><td>提供 token 的位置信息（因为注意力是无序的）</td></tr><tr><td>Feed-Forward Network</td><td>每个位置独立的非线性变换</td></tr></tbody></table><h3 id="🔧-decoder-only-的特点" tabindex="-1"><a class="header-anchor" href="#🔧-decoder-only-的特点"><span>🔧 Decoder-only 的特点：</span></a></h3><table><thead><tr><th>特性</th><th>原因</th></tr></thead><tbody><tr><td><strong>仅用 Masked Self-Attention</strong></td><td>防止看到未来信息（只看 $x_{）</td></tr><tr><td><strong>无 Encoder 输入</strong></td><td>输入就是训练数据本身，无需外部序列</td></tr><tr><td><strong>可逐词生成</strong></td><td>非常适合 open-ended generation</td></tr></tbody></table>',5)]))}const m=e(d,[["render",i],["__file","transformer.html.vue"]]),c=JSON.parse(`{"path":"/ai/gm/llm/transformer.html","title":"Transformer","lang":"en-US","frontmatter":{"description":"Transformer Multi-Head Attention Self-Attention LayerNorm+Residual Position Encoding FFN (Feed-Forward Network) 🔧 Decoder-only 的特点：","head":[["meta",{"property":"og:url","content":"https://davidliuk.github.io/blog/blog/ai/gm/llm/transformer.html"}],["meta",{"property":"og:site_name","content":"David's Blog"}],["meta",{"property":"og:title","content":"Transformer"}],["meta",{"property":"og:description","content":"Transformer Multi-Head Attention Self-Attention LayerNorm+Residual Position Encoding FFN (Feed-Forward Network) 🔧 Decoder-only 的特点："}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-10-26T05:16:21.000Z"}],["meta",{"property":"article:modified_time","content":"2025-10-26T05:16:21.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Transformer\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-10-26T05:16:21.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"David Liu\\",\\"url\\":\\"https://github.com/davidliuk\\"}]}"]]},"headers":[{"level":3,"title":"🔧 Decoder-only 的特点：","slug":"🔧-decoder-only-的特点","link":"#🔧-decoder-only-的特点","children":[]}],"git":{"createdTime":1755417148000,"updatedTime":1761455781000,"contributors":[{"name":"dawei.liu","email":"dawei.liu@bytedance.com","commits":1}]},"readingTime":{"minutes":0.5,"words":151},"filePathRelative":"ai/gm/llm/transformer.md","localizedDate":"August 17, 2025","excerpt":"\\n<ul>\\n<li>Multi-Head Attention</li>\\n<li>Self-Attention</li>\\n<li>LayerNorm+Residual</li>\\n<li>Position Encoding</li>\\n<li>FFN (Feed-Forward Network)</li>\\n</ul>\\n<table>\\n<thead>\\n<tr>\\n<th>组件</th>\\n<th>作用</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>Multi-Head Attention</td>\\n<td>并行多个注意力头，捕捉不同语义关系</td>\\n</tr>\\n<tr>\\n<td>Self-Attention</td>\\n<td>句子内部 token 之间互相关注</td>\\n</tr>\\n<tr>\\n<td>LayerNorm + Residual</td>\\n<td>加快收敛、稳定训练</td>\\n</tr>\\n<tr>\\n<td>Position Encoding</td>\\n<td>提供 token 的位置信息（因为注意力是无序的）</td>\\n</tr>\\n<tr>\\n<td>Feed-Forward Network</td>\\n<td>每个位置独立的非线性变换</td>\\n</tr>\\n</tbody>\\n</table>","autoDesc":true}`);export{m as comp,c as data};
