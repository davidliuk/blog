import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as r,a as n,o}from"./app-CGXHKXsa.js";const d={};function i(a,t){return o(),r("div",null,t[0]||(t[0]=[n('<h1 id="transformer" tabindex="-1"><a class="header-anchor" href="#transformer"><span>Transformer</span></a></h1><ul><li>Multi-Head Attention</li><li>Self-Attention</li><li>LayerNorm+Residual</li><li>Position Encoding</li><li>FFN (Feed-Forward Network)</li></ul><table><thead><tr><th>ç»„ä»¶</th><th>ä½œç”¨</th></tr></thead><tbody><tr><td>Multi-Head Attention</td><td>å¹¶è¡Œå¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œæ•æ‰ä¸åŒè¯­ä¹‰å…³ç³»</td></tr><tr><td>Self-Attention</td><td>å¥å­å†…éƒ¨ token ä¹‹é—´äº’ç›¸å…³æ³¨</td></tr><tr><td>LayerNorm + Residual</td><td>åŠ å¿«æ”¶æ•›ã€ç¨³å®šè®­ç»ƒ</td></tr><tr><td>Position Encoding</td><td>æä¾› token çš„ä½ç½®ä¿¡æ¯ï¼ˆå› ä¸ºæ³¨æ„åŠ›æ˜¯æ— åºçš„ï¼‰</td></tr><tr><td>Feed-Forward Network</td><td>æ¯ä¸ªä½ç½®ç‹¬ç«‹çš„éçº¿æ€§å˜æ¢</td></tr></tbody></table><h3 id="ğŸ”§-decoder-only-çš„ç‰¹ç‚¹" tabindex="-1"><a class="header-anchor" href="#ğŸ”§-decoder-only-çš„ç‰¹ç‚¹"><span>ğŸ”§ Decoder-only çš„ç‰¹ç‚¹ï¼š</span></a></h3><table><thead><tr><th>ç‰¹æ€§</th><th>åŸå› </th></tr></thead><tbody><tr><td><strong>ä»…ç”¨ Masked Self-Attention</strong></td><td>é˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯ï¼ˆåªçœ‹ $x_{ï¼‰</td></tr><tr><td><strong>æ—  Encoder è¾“å…¥</strong></td><td>è¾“å…¥å°±æ˜¯è®­ç»ƒæ•°æ®æœ¬èº«ï¼Œæ— éœ€å¤–éƒ¨åºåˆ—</td></tr><tr><td><strong>å¯é€è¯ç”Ÿæˆ</strong></td><td>éå¸¸é€‚åˆ open-ended generation</td></tr></tbody></table>',5)]))}const m=e(d,[["render",i],["__file","transformer.html.vue"]]),c=JSON.parse(`{"path":"/ai/gm/llm/transformer.html","title":"Transformer","lang":"en-US","frontmatter":{"description":"Transformer Multi-Head Attention Self-Attention LayerNorm+Residual Position Encoding FFN (Feed-Forward Network) ğŸ”§ Decoder-only çš„ç‰¹ç‚¹ï¼š","head":[["meta",{"property":"og:url","content":"https://davidliuk.github.io/blog/blog/ai/gm/llm/transformer.html"}],["meta",{"property":"og:site_name","content":"David's Blog"}],["meta",{"property":"og:title","content":"Transformer"}],["meta",{"property":"og:description","content":"Transformer Multi-Head Attention Self-Attention LayerNorm+Residual Position Encoding FFN (Feed-Forward Network) ğŸ”§ Decoder-only çš„ç‰¹ç‚¹ï¼š"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-10-26T05:16:21.000Z"}],["meta",{"property":"article:modified_time","content":"2025-10-26T05:16:21.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Transformer\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-10-26T05:16:21.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"David Liu\\",\\"url\\":\\"https://github.com/davidliuk\\"}]}"]]},"headers":[{"level":3,"title":"ğŸ”§ Decoder-only çš„ç‰¹ç‚¹ï¼š","slug":"ğŸ”§-decoder-only-çš„ç‰¹ç‚¹","link":"#ğŸ”§-decoder-only-çš„ç‰¹ç‚¹","children":[]}],"git":{"createdTime":1755417148000,"updatedTime":1761455781000,"contributors":[{"name":"dawei.liu","email":"dawei.liu@bytedance.com","commits":1}]},"readingTime":{"minutes":0.5,"words":151},"filePathRelative":"ai/gm/llm/transformer.md","localizedDate":"August 17, 2025","excerpt":"\\n<ul>\\n<li>Multi-Head Attention</li>\\n<li>Self-Attention</li>\\n<li>LayerNorm+Residual</li>\\n<li>Position Encoding</li>\\n<li>FFN (Feed-Forward Network)</li>\\n</ul>\\n<table>\\n<thead>\\n<tr>\\n<th>ç»„ä»¶</th>\\n<th>ä½œç”¨</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>Multi-Head Attention</td>\\n<td>å¹¶è¡Œå¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œæ•æ‰ä¸åŒè¯­ä¹‰å…³ç³»</td>\\n</tr>\\n<tr>\\n<td>Self-Attention</td>\\n<td>å¥å­å†…éƒ¨ token ä¹‹é—´äº’ç›¸å…³æ³¨</td>\\n</tr>\\n<tr>\\n<td>LayerNorm + Residual</td>\\n<td>åŠ å¿«æ”¶æ•›ã€ç¨³å®šè®­ç»ƒ</td>\\n</tr>\\n<tr>\\n<td>Position Encoding</td>\\n<td>æä¾› token çš„ä½ç½®ä¿¡æ¯ï¼ˆå› ä¸ºæ³¨æ„åŠ›æ˜¯æ— åºçš„ï¼‰</td>\\n</tr>\\n<tr>\\n<td>Feed-Forward Network</td>\\n<td>æ¯ä¸ªä½ç½®ç‹¬ç«‹çš„éçº¿æ€§å˜æ¢</td>\\n</tr>\\n</tbody>\\n</table>","autoDesc":true}`);export{m as comp,c as data};
